{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "g83HiPS2mSqJ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R3gm/SD_diffusers_interactive/blob/main/Stable_diffusion_interactive_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interactive Stable Diffusion"
      ],
      "metadata": {
        "id": "kZl9FPC9sbW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Description | Link |\n",
        "| ----------- | ---- |\n",
        "| ðŸŽ‰ Repository | [![GitHub Repository](https://img.shields.io/github/stars/R3gm/SD_diffusers_interactive?style=social)](https://github.com/R3gm/SD_diffusers_interactive) |\n"
      ],
      "metadata": {
        "id": "8MRWnnDNp4Mh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Prompt weights: Depending on model and CFG you can weight up to around 1.5 or 1.6 before things start to get weird.\n",
        "- Controlnet 1.1 for SD\n",
        "- SDXL models only support txt2img\n",
        "- More functions, more bugs; less than 10 words, more laughs\n"
      ],
      "metadata": {
        "id": "e8lnApcK4vWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Google Colab notebook offers a user-friendly interface for generating AI images from text prompts using Stable Diffusion. It uses HuggingFace Diffusers and Jupyter widgets, providing a simple and lightweight alternative to web-based tools, making it accessible for all to `get started with Stable Diffusion`.\n",
        "\n",
        "Based on [redromnon's repository](https://github.com/redromnon/stable-diffusion-interactive-notebook)"
      ],
      "metadata": {
        "id": "wILzWiWRfTX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. ðŸ‘‡ Installing dependencies { display-mode: \"form\" }\n",
        "\n",
        "!pip install -q omegaconf==2.3.0 torch git+https://github.com/huggingface/diffusers.git git+https://github.com/damian0815/compel.git invisible_watermark  transformers accelerate scipy safetensors==0.3.3 xformers safetensors mediapy ipywidgets==7.7.1 controlnet_aux==0.0.6 mediapipe==0.10.1 pytorch-lightning asdff\n",
        "\n",
        "!apt install git-lfs\n",
        "!git lfs install\n",
        "!apt -y install -qq aria2"
      ],
      "metadata": {
        "id": "A04WkRDwGpLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. ðŸ‘‡ Download Model: Please provide a link for the Civitai API, Google Drive, or Hugging Face. { form-width: \"20%\", display-mode: \"form\" }\n",
        "import os\n",
        "%cd /content\n",
        "\n",
        "def download_things(directory, url, hf_token=\"\"):\n",
        "    url = url.strip()\n",
        "\n",
        "    if \"drive.google.com\" in url:\n",
        "        original_dir = os.getcwd()\n",
        "        os.chdir(directory)\n",
        "        !gdown --fuzzy {url}\n",
        "        os.chdir(original_dir)\n",
        "    elif \"huggingface.co\" in url:\n",
        "        if \"/blob/\" in url:\n",
        "            url = url.replace(\"/blob/\", \"/resolve/\")\n",
        "        user_header = f'\"Authorization: Bearer {hf_token}\"'\n",
        "        !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 {url} -d {directory}  -o {url.split('/')[-1]}\n",
        "    else:\n",
        "        !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {directory} {url}\n",
        "\n",
        "def get_model_list(directory_path):\n",
        "    model_list = []\n",
        "    valid_extensions = {'.ckpt' , '.pt', '.pth', '.safetensors', '.bin'}\n",
        "\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if os.path.splitext(filename)[1] in valid_extensions:\n",
        "            name_without_extension = os.path.splitext(filename)[0]\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            model_list.append((name_without_extension, file_path))\n",
        "            print('\\033[34mFILE: ' + name_without_extension + file_path + '\\033[0m')\n",
        "    return model_list\n",
        "\n",
        "def process_string(input_string):\n",
        "    parts = input_string.split('/')\n",
        "\n",
        "    if len(parts) == 2:\n",
        "        first_element = parts[1]\n",
        "        complete_string = input_string\n",
        "        result = (first_element, complete_string)\n",
        "        return result\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "directory_models = 'models'\n",
        "os.makedirs(directory_models, exist_ok=True)\n",
        "directory_loras = 'loras'\n",
        "os.makedirs(directory_loras, exist_ok=True)\n",
        "directory_vaes = 'vaes'\n",
        "os.makedirs(directory_vaes, exist_ok=True)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown - **Download a Model**\n",
        "download_model = \"https://civitai.com/api/download/models/125771\" # @param {type:\"string\"}\n",
        "#@markdown - For SDXL models, only diffuser format models are supported, and you only need the repository name.\n",
        "load_diffusers_format_model = 'gsdf/CounterfeitXL' # @param {type:\"string\"}\n",
        "#@markdown - **Download a VAE**\n",
        "download_vae = \"https://huggingface.co/fp16-guy/anything_kl-f8-anime2_vae-ft-mse-840000-ema-pruned_blessed_clearvae_fp16_cleaned/resolve/main/anything_fp16.safetensors\" # @param {type:\"string\"}\n",
        "#@markdown - **Download a LoRA**\n",
        "download_lora = \"https://civitai.com/api/download/models/97655\" # @param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **HF TOKEN** - If you need to download your private model from Hugging Face, input your token here.\n",
        "hf_token = \"\"  # @param {type:\"string\"}\n",
        "#@markdown\n",
        "#@markdown ---\n",
        "\n",
        "download_things(directory_models, download_model, hf_token)\n",
        "download_things(directory_vaes, download_vae, hf_token)\n",
        "download_things(directory_loras, download_lora, hf_token)\n",
        "\n",
        "\n",
        "# TI more combatible in safetensor format; maybe convert to safetensor can help\n",
        "directory_embeds = 'embedings'\n",
        "os.makedirs(directory_embeds, exist_ok=True)\n",
        "download_embeds = [\n",
        "    'https://huggingface.co/datasets/Nerfgun3/bad_prompt/resolve/main/bad_prompt.pt',\n",
        "    'https://huggingface.co/datasets/Nerfgun3/bad_prompt/blob/main/bad_prompt_version2.pt',\n",
        "    'https://huggingface.co/sayakpaul/EasyNegative-test/blob/main/EasyNegative.safetensors',\n",
        "    ]\n",
        "for url_embed in download_embeds:\n",
        "    download_things(directory_embeds, url_embed, hf_token)\n",
        "embed_list = get_model_list(directory_embeds)\n",
        "\n",
        "model_list = get_model_list(directory_models)\n",
        "if load_diffusers_format_model.strip() != \"\" and load_diffusers_format_model.count('/') == 1:\n",
        "    model_list.append(process_string(load_diffusers_format_model))\n",
        "lora_model_list = get_model_list(directory_loras)\n",
        "lora_model_list.insert(0, (\"None\",None))\n",
        "vae_model_list = get_model_list(directory_vaes)\n",
        "vae_model_list.insert(0, (\"None\",\"None\"))\n",
        "\n",
        "\n",
        "\n",
        "print('\\033[33mðŸ Download finished.\\033[0m')\n",
        "\n",
        "### SECOND PART ###\n",
        "import gc\n",
        "import time\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from diffusers import (\n",
        "    ControlNetModel,\n",
        "    DiffusionPipeline,\n",
        "    StableDiffusionControlNetPipeline,\n",
        "    StableDiffusionControlNetInpaintPipeline,\n",
        "    StableDiffusionPipeline,\n",
        "    AutoencoderKL\n",
        ")\n",
        "import torch\n",
        "import random\n",
        "from controlnet_aux import (CannyDetector, ContentShuffleDetector, HEDdetector,\n",
        "                            LineartAnimeDetector, LineartDetector,\n",
        "                            MidasDetector, MLSDdetector, NormalBaeDetector,\n",
        "                            OpenposeDetector, PidiNetDetector)\n",
        "from transformers import pipeline\n",
        "from controlnet_aux.util import HWC3, ade_palette\n",
        "from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\n",
        "import cv2\n",
        "from diffusers import (\n",
        "    DPMSolverMultistepScheduler,\n",
        "    DPMSolverSinglestepScheduler,\n",
        "    KDPM2DiscreteScheduler,\n",
        "    KDPM2AncestralDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    HeunDiscreteScheduler,\n",
        "    LMSDiscreteScheduler,\n",
        "    DDIMScheduler,\n",
        "    DEISMultistepScheduler,\n",
        "    UniPCMultistepScheduler,\n",
        ")\n",
        "\n",
        "# =====================================\n",
        "# Utils preprocessor\n",
        "# =====================================\n",
        "def resize_image(input_image, resolution, interpolation=None):\n",
        "    H, W, C = input_image.shape\n",
        "    H = float(H)\n",
        "    W = float(W)\n",
        "    k = float(resolution) / max(H, W)\n",
        "    H *= k\n",
        "    W *= k\n",
        "    H = int(np.round(H / 64.0)) * 64\n",
        "    W = int(np.round(W / 64.0)) * 64\n",
        "    if interpolation is None:\n",
        "        interpolation = cv2.INTER_LANCZOS4 if k > 1 else cv2.INTER_AREA\n",
        "    img = cv2.resize(input_image, (W, H), interpolation=interpolation)\n",
        "    return img\n",
        "\n",
        "class DepthEstimator:\n",
        "    def __init__(self):\n",
        "        self.model = pipeline('depth-estimation')\n",
        "\n",
        "    def __call__(self, image: np.ndarray, **kwargs) -> PIL.Image.Image:\n",
        "        detect_resolution = kwargs.pop('detect_resolution', 512)\n",
        "        image_resolution = kwargs.pop('image_resolution', 512)\n",
        "        image = np.array(image)\n",
        "        image = HWC3(image)\n",
        "        image = resize_image(image, resolution=detect_resolution)\n",
        "        image = PIL.Image.fromarray(image)\n",
        "        image = self.model(image)\n",
        "        image = image['depth']\n",
        "        image = np.array(image)\n",
        "        image = HWC3(image)\n",
        "        image = resize_image(image, resolution=image_resolution)\n",
        "        return PIL.Image.fromarray(image)\n",
        "\n",
        "class ImageSegmentor:\n",
        "    def __init__(self):\n",
        "        self.image_processor = AutoImageProcessor.from_pretrained(\n",
        "            'openmmlab/upernet-convnext-small')\n",
        "        self.image_segmentor = UperNetForSemanticSegmentation.from_pretrained(\n",
        "            'openmmlab/upernet-convnext-small')\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def __call__(self, image: np.ndarray, **kwargs) -> PIL.Image.Image:\n",
        "        detect_resolution = kwargs.pop('detect_resolution', 512)\n",
        "        image_resolution = kwargs.pop('image_resolution', 512)\n",
        "        image = HWC3(image)\n",
        "        image = resize_image(image, resolution=detect_resolution)\n",
        "        image = PIL.Image.fromarray(image)\n",
        "\n",
        "        pixel_values = self.image_processor(image,\n",
        "                                            return_tensors='pt').pixel_values\n",
        "        outputs = self.image_segmentor(pixel_values)\n",
        "        seg = self.image_processor.post_process_semantic_segmentation(\n",
        "            outputs, target_sizes=[image.size[::-1]])[0]\n",
        "        color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8)\n",
        "        for label, color in enumerate(ade_palette()):\n",
        "            color_seg[seg == label, :] = color\n",
        "        color_seg = color_seg.astype(np.uint8)\n",
        "\n",
        "        color_seg = resize_image(color_seg,\n",
        "                                 resolution=image_resolution,\n",
        "                                 interpolation=cv2.INTER_NEAREST)\n",
        "        return PIL.Image.fromarray(color_seg)\n",
        "\n",
        "class Preprocessor:\n",
        "    MODEL_ID = 'lllyasviel/Annotators'\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.name = ''\n",
        "\n",
        "    def load(self, name: str) -> None:\n",
        "        if name == self.name:\n",
        "            return\n",
        "        if name == 'HED':\n",
        "            self.model = HEDdetector.from_pretrained(self.MODEL_ID)\n",
        "        elif name == 'Midas':\n",
        "            self.model = MidasDetector.from_pretrained(self.MODEL_ID)\n",
        "        elif name == 'MLSD':\n",
        "            self.model = MLSDdetector.from_pretrained(self.MODEL_ID)\n",
        "        elif name == 'Openpose':\n",
        "            self.model = OpenposeDetector.from_pretrained(self.MODEL_ID)\n",
        "        elif name == 'PidiNet':\n",
        "            self.model = PidiNetDetector.from_pretrained(self.MODEL_ID)\n",
        "        elif name == 'NormalBae':\n",
        "            self.model = NormalBaeDetector.from_pretrained(self.MODEL_ID)\n",
        "        elif name == 'Lineart':\n",
        "            self.model = LineartDetector.from_pretrained(self.MODEL_ID)\n",
        "        elif name == 'LineartAnime':\n",
        "            self.model = LineartAnimeDetector.from_pretrained(self.MODEL_ID)\n",
        "        elif name == 'Canny':\n",
        "            self.model = CannyDetector()\n",
        "        elif name == 'ContentShuffle':\n",
        "            self.model = ContentShuffleDetector()\n",
        "        elif name == 'DPT':\n",
        "            self.model = DepthEstimator()\n",
        "        elif name == 'UPerNet':\n",
        "            self.model = ImageSegmentor()\n",
        "        else:\n",
        "            raise ValueError\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        self.name = name\n",
        "\n",
        "    def __call__(self, image: PIL.Image.Image, **kwargs) -> PIL.Image.Image:\n",
        "        if self.name == 'Canny':\n",
        "            if 'detect_resolution' in kwargs:\n",
        "                detect_resolution = kwargs.pop('detect_resolution')\n",
        "                image = np.array(image)\n",
        "                image = HWC3(image)\n",
        "                image = resize_image(image, resolution=detect_resolution)\n",
        "            image = self.model(image, **kwargs)\n",
        "            return PIL.Image.fromarray(image)\n",
        "        elif self.name == 'Midas':\n",
        "            detect_resolution = kwargs.pop('detect_resolution', 512)\n",
        "            image_resolution = kwargs.pop('image_resolution', 512)\n",
        "            image = np.array(image)\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=detect_resolution)\n",
        "            image = self.model(image, **kwargs)\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            return PIL.Image.fromarray(image)\n",
        "        else:\n",
        "            return self.model(image, **kwargs)\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# Base Model\n",
        "# =====================================\n",
        "\n",
        "CONTROLNET_MODEL_IDS = {\n",
        "    'Openpose': 'lllyasviel/control_v11p_sd15_openpose',\n",
        "    'Canny': 'lllyasviel/control_v11p_sd15_canny',\n",
        "    'MLSD': 'lllyasviel/control_v11p_sd15_mlsd',\n",
        "    'scribble': 'lllyasviel/control_v11p_sd15_scribble',\n",
        "    'softedge': 'lllyasviel/control_v11p_sd15_softedge',\n",
        "    'segmentation': 'lllyasviel/control_v11p_sd15_seg',\n",
        "    'depth': 'lllyasviel/control_v11f1p_sd15_depth',\n",
        "    'NormalBae': 'lllyasviel/control_v11p_sd15_normalbae',\n",
        "    'lineart': 'lllyasviel/control_v11p_sd15_lineart',\n",
        "    'lineart_anime': 'lllyasviel/control_v11p_sd15s2_lineart_anime',\n",
        "    'shuffle': 'lllyasviel/control_v11e_sd15_shuffle',\n",
        "    'ip2p': 'lllyasviel/control_v11e_sd15_ip2p',\n",
        "    'Inpaint': 'lllyasviel/control_v11p_sd15_inpaint',\n",
        "    'txt2img': 'NotControlnet',\n",
        "}\n",
        "\n",
        "def download_all_controlnet_weights() -> None:\n",
        "    for model_id in CONTROLNET_MODEL_IDS.values():\n",
        "        ControlNetModel.from_pretrained(model_id)\n",
        "\n",
        "class Model:\n",
        "    def __init__(self,\n",
        "                 base_model_id: str = 'runwayml/stable-diffusion-v1-5',\n",
        "                 task_name: str = 'Canny', vae_model=None, type_model_precision = torch.float16):\n",
        "        self.device = torch.device(\n",
        "            'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.base_model_id = ''\n",
        "        self.task_name = ''\n",
        "        self.vae_model = None\n",
        "        self.type_model_precision = type_model_precision if torch.cuda.is_available() else torch.float32 # For SD 1.5\n",
        "\n",
        "        self.pipe = self.load_pipe(base_model_id, task_name, vae_model, type_model_precision)\n",
        "        self.preprocessor = Preprocessor()\n",
        "\n",
        "    def load_pipe(self, base_model_id: str, task_name, vae_model=None, type_model_precision = torch.float16, reload=False) -> DiffusionPipeline:\n",
        "        if base_model_id == self.base_model_id and task_name == self.task_name and hasattr(\n",
        "                self, 'pipe') and self.vae_model==vae_model and self.pipe is not None and reload==False and type_model_precision == self.type_model_precision:\n",
        "            print('Previous loaded')\n",
        "            return self.pipe\n",
        "        if base_model_id == self.base_model_id and task_name == self.task_name and hasattr(\n",
        "                self, 'pipe') and self.vae_model==vae_model and self.pipe is not None and reload==False and self.device == \"cpu\":\n",
        "            print('Pipe in CPU')\n",
        "            return self.pipe\n",
        "\n",
        "\n",
        "        self.type_model_precision = type_model_precision if torch.cuda.is_available() else torch.float32\n",
        "        self.pipe = None\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        model_id = CONTROLNET_MODEL_IDS[task_name]\n",
        "\n",
        "        if task_name == 'txt2img':\n",
        "            if os.path.exists(base_model_id):\n",
        "                if self.type_model_precision == torch.float32:\n",
        "                    print(\"Working with full precision\")\n",
        "                pipe = StableDiffusionPipeline.from_single_file(\n",
        "                  base_model_id,\n",
        "                  vae = None if vae_model == 'None' else AutoencoderKL.from_single_file(vae_model), # , torch_dtype=self.type_model_precision\n",
        "                  torch_dtype=self.type_model_precision,\n",
        "                )\n",
        "                pipe.safety_checker = None\n",
        "            else:\n",
        "                print(\"Default VAE: madebyollin/sdxl-vae-fp16-fix\")\n",
        "                pipe = DiffusionPipeline.from_pretrained(\n",
        "                    base_model_id,\n",
        "                    vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16),\n",
        "                    torch_dtype=torch.float16,\n",
        "                    use_safetensors=True,\n",
        "                    variant=\"fp16\",\n",
        "                    )\n",
        "                pipe.safety_checker = None\n",
        "            print('Loaded txt2img pipeline')\n",
        "        elif task_name == 'Inpaint':\n",
        "            if self.type_model_precision == torch.float32:\n",
        "                print(\"Working with full precision\")\n",
        "            controlnet = ControlNetModel.from_pretrained(model_id,\n",
        "                                                        torch_dtype=self.type_model_precision)\n",
        "            if os.path.exists(base_model_id):\n",
        "                pipe = StableDiffusionControlNetInpaintPipeline.from_single_file(\n",
        "                    base_model_id,\n",
        "                    vae = None if vae_model == 'None' else AutoencoderKL.from_single_file(vae_model),\n",
        "                    safety_checker=None,\n",
        "                    controlnet=controlnet,\n",
        "                    torch_dtype=self.type_model_precision)\n",
        "            print('Loaded ControlNet Inpaint pipeline')\n",
        "        else:\n",
        "            if self.type_model_precision == torch.float32:\n",
        "                print(\"Working with full precision\")\n",
        "            controlnet = ControlNetModel.from_pretrained(model_id,\n",
        "                                                        torch_dtype=self.type_model_precision) # for all\n",
        "            if os.path.exists(base_model_id):\n",
        "                pipe = StableDiffusionControlNetPipeline.from_single_file(\n",
        "                    base_model_id,\n",
        "                    vae = None if vae_model == 'None' else AutoencoderKL.from_single_file(vae_model),\n",
        "                    safety_checker=None,\n",
        "                    controlnet=controlnet,\n",
        "                    torch_dtype=self.type_model_precision)\n",
        "            else:\n",
        "                raise ZeroDivisionError(\"Not implemented\")\n",
        "            #     pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "            #         base_model_id,\n",
        "            #         vae = AutoencoderKL.from_pretrained(base_model_id, subfolder='vae') if vae_model == 'None' else AutoencoderKL.from_single_file(vae_model),\n",
        "            #         safety_checker=None,\n",
        "            #         controlnet=controlnet,\n",
        "            #         torch_dtype=torch.float16)\n",
        "            # print('Loaded ControlNet pipeline')\n",
        "\n",
        "            pipe.scheduler = UniPCMultistepScheduler.from_config(\n",
        "                pipe.scheduler.config)\n",
        "\n",
        "        if self.device.type == 'cuda':\n",
        "            pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "        pipe.to(self.device)\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        self.pipe = pipe\n",
        "        self.base_model_id = base_model_id\n",
        "        self.task_name = task_name\n",
        "        self.vae_model = vae_model\n",
        "\n",
        "        self.lora_memory = [None, None, None, None, None] # no need __init__\n",
        "        self.lora_scale_memory = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
        "        self.FreeU = False\n",
        "        return pipe\n",
        "\n",
        "    def set_base_model(self, base_model_id: str) -> str:\n",
        "        if not base_model_id or base_model_id == self.base_model_id:\n",
        "            return self.base_model_id\n",
        "        del self.pipe\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        try:\n",
        "            self.pipe = self.load_pipe(base_model_id, self.task_name, self.vae_model)\n",
        "        except Exception:\n",
        "            self.pipe = self.load_pipe(self.base_model_id, self.task_name, self.vae_model)\n",
        "        return self.base_model_id\n",
        "\n",
        "    def load_controlnet_weight(self, task_name: str) -> None:\n",
        "        if task_name == self.task_name:\n",
        "            return\n",
        "        if self.pipe is not None and hasattr(self.pipe, 'controlnet'):\n",
        "            del self.pipe.controlnet\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        model_id = CONTROLNET_MODEL_IDS[task_name]\n",
        "        controlnet = ControlNetModel.from_pretrained(model_id,\n",
        "                                                     torch_dtype=self.type_model_precision)\n",
        "        controlnet.to(self.device)\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        self.pipe.controlnet = controlnet\n",
        "        self.task_name = task_name\n",
        "\n",
        "    def get_prompt(self, prompt: str, additional_prompt: str) -> str:\n",
        "        if not prompt:\n",
        "            prompt = additional_prompt\n",
        "        else:\n",
        "            prompt = f'{prompt}, {additional_prompt}'\n",
        "        return prompt\n",
        "\n",
        "    @torch.autocast('cuda')\n",
        "    def run_pipe(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        negative_prompt: str,\n",
        "        prompt_embeds,\n",
        "        negative_prompt_embeds,\n",
        "        control_image: PIL.Image.Image,\n",
        "        num_images: int,\n",
        "        num_steps: int,\n",
        "        guidance_scale: float,\n",
        "        clip_skip: int,\n",
        "        generator,\n",
        "        controlnet_conditioning_scale,\n",
        "        control_guidance_start,\n",
        "        control_guidance_end,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        # Return PIL images\n",
        "        #generator = torch.Generator().manual_seed(seed)\n",
        "        return self.pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "            guidance_scale=guidance_scale,\n",
        "            clip_skip = clip_skip,\n",
        "            num_images_per_prompt=num_images,\n",
        "            num_inference_steps=num_steps,\n",
        "            generator=generator,\n",
        "            controlnet_conditioning_scale = controlnet_conditioning_scale,\n",
        "            control_guidance_start = control_guidance_start,\n",
        "            control_guidance_end = control_guidance_end,\n",
        "            image=control_image\n",
        "            ).images\n",
        "\n",
        "    @torch.autocast('cuda')\n",
        "    def run_pipe_SD(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        negative_prompt: str,\n",
        "        prompt_embeds,\n",
        "        negative_prompt_embeds,\n",
        "        num_images: int,\n",
        "        num_steps: int,\n",
        "        guidance_scale: float,\n",
        "        clip_skip: int,\n",
        "        height : int,\n",
        "        width : int,\n",
        "        generator,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        # Return PIL images\n",
        "        #generator = torch.Generator().manual_seed(seed)\n",
        "        return self.pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "            guidance_scale=guidance_scale,\n",
        "            clip_skip = clip_skip,\n",
        "            num_images_per_prompt=num_images,\n",
        "            num_inference_steps=num_steps,\n",
        "            generator=generator,\n",
        "            height = height,\n",
        "            width = width,\n",
        "            ).images\n",
        "\n",
        "\n",
        "    # @torch.autocast('cuda')\n",
        "    # def run_pipe_SDXL(\n",
        "    #     self,\n",
        "    #     prompt: str,\n",
        "    #     negative_prompt: str,\n",
        "    #     prompt_embeds,\n",
        "    #     negative_prompt_embeds,\n",
        "    #     num_images: int,\n",
        "    #     num_steps: int,\n",
        "    #     guidance_scale: float,\n",
        "    #     clip_skip: int,\n",
        "    #     height : int,\n",
        "    #     width : int,\n",
        "    #     generator,\n",
        "    #     seddd,\n",
        "    #     conditioning,\n",
        "    #     pooled,\n",
        "    # ) -> list[PIL.Image.Image]:\n",
        "    #     # Return PIL images\n",
        "    #     #generator = torch.Generator(\"cuda\").manual_seed(seddd) # generator = torch.Generator(\"cuda\").manual_seed(seed),\n",
        "    #     return self.pipe(\n",
        "    #         prompt = None,\n",
        "    #         negative_prompt = None,\n",
        "    #         prompt_embeds=conditioning[0:1],\n",
        "    #         pooled_prompt_embeds=pooled[0:1],\n",
        "    #         negative_prompt_embeds=conditioning[1:2],\n",
        "    #         negative_pooled_prompt_embeds=pooled[1:2],\n",
        "    #         height = height,\n",
        "    #         width = width,\n",
        "    #         num_inference_steps = num_steps,\n",
        "    #         guidance_scale = guidance_scale,\n",
        "    #         clip_skip = clip_skip,\n",
        "    #         num_images_per_prompt = num_images,\n",
        "    #         generator = generator,\n",
        "    #         ).images\n",
        "\n",
        "    @torch.autocast('cuda')\n",
        "    def run_pipe_inpaint(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        negative_prompt: str,\n",
        "        prompt_embeds,\n",
        "        negative_prompt_embeds,\n",
        "        control_image: PIL.Image.Image,\n",
        "        num_images: int,\n",
        "        num_steps: int,\n",
        "        guidance_scale: float,\n",
        "        clip_skip: int,\n",
        "        strength: float,\n",
        "        init_image,\n",
        "        control_mask,\n",
        "        controlnet_conditioning_scale,\n",
        "        control_guidance_start,\n",
        "        control_guidance_end,\n",
        "        generator,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        # Return PIL images\n",
        "        #generator = torch.Generator().manual_seed(seed)\n",
        "        return self.pipe(\n",
        "            prompt= None,\n",
        "            negative_prompt= None,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "            eta=1.0,\n",
        "            strength = strength,\n",
        "            image = init_image, # original image\n",
        "            mask_image = control_mask, # mask, values of 0 to 255\n",
        "            control_image = control_image, # tensor control image\n",
        "            num_images_per_prompt  = num_images,\n",
        "            num_inference_steps = num_steps,\n",
        "            guidance_scale = guidance_scale,\n",
        "            clip_skip = clip_skip,\n",
        "            generator = generator,\n",
        "            controlnet_conditioning_scale = controlnet_conditioning_scale,\n",
        "            control_guidance_start = control_guidance_start,\n",
        "            control_guidance_end = control_guidance_end,\n",
        "            ).images\n",
        "\n",
        "    ### self.x_process return image_preprocessor###\n",
        "    @torch.inference_mode()\n",
        "    def process_canny(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        low_threshold: int,\n",
        "        high_threshold: int,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        self.preprocessor.load('Canny')\n",
        "        control_image = self.preprocessor(\n",
        "            image=image,\n",
        "            low_threshold=low_threshold,\n",
        "            high_threshold=high_threshold,\n",
        "            image_resolution=image_resolution,\n",
        "            detect_resolution=preprocess_resolution,\n",
        "        )\n",
        "\n",
        "        self.load_controlnet_weight('Canny')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_mlsd(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        value_threshold: float,\n",
        "        distance_threshold: float,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        self.preprocessor.load('MLSD')\n",
        "        control_image = self.preprocessor(\n",
        "            image=image,\n",
        "            image_resolution=image_resolution,\n",
        "            detect_resolution=preprocess_resolution,\n",
        "            thr_v=value_threshold,\n",
        "            thr_d=distance_threshold,\n",
        "        )\n",
        "        self.load_controlnet_weight('MLSD')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_scribble(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        preprocessor_name: str,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        if preprocessor_name == 'None':\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            control_image = PIL.Image.fromarray(image)\n",
        "        elif preprocessor_name == 'HED':\n",
        "            self.preprocessor.load(preprocessor_name)\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "                scribble=False,\n",
        "            )\n",
        "        elif preprocessor_name == 'PidiNet':\n",
        "            self.preprocessor.load(preprocessor_name)\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "                safe=False,\n",
        "            )\n",
        "        self.load_controlnet_weight('scribble')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_scribble_interactive(\n",
        "        self,\n",
        "        image_and_mask: dict[str, np.ndarray],\n",
        "        image_resolution: int,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image_and_mask is None:\n",
        "            raise ValueError\n",
        "\n",
        "        image = image_and_mask['mask']\n",
        "        image = HWC3(image)\n",
        "        image = resize_image(image, resolution=image_resolution)\n",
        "        control_image = PIL.Image.fromarray(image)\n",
        "\n",
        "        self.load_controlnet_weight('scribble')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_softedge(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        preprocessor_name: str,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        if preprocessor_name == 'None':\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            control_image = PIL.Image.fromarray(image)\n",
        "        elif preprocessor_name in ['HED', 'HED safe']:\n",
        "            safe = 'safe' in preprocessor_name\n",
        "            self.preprocessor.load('HED')\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "                scribble=safe,\n",
        "            )\n",
        "        elif preprocessor_name in ['PidiNet', 'PidiNet safe']:\n",
        "            safe = 'safe' in preprocessor_name\n",
        "            self.preprocessor.load('PidiNet')\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "                safe=safe,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError\n",
        "        self.load_controlnet_weight('softedge')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_openpose(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        preprocessor_name: str,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        if preprocessor_name == 'None':\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            control_image = PIL.Image.fromarray(image)\n",
        "        else:\n",
        "            self.preprocessor.load('Openpose')\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "                hand_and_face=True,\n",
        "            )\n",
        "        self.load_controlnet_weight('Openpose')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_segmentation(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        preprocessor_name: str,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        if preprocessor_name == 'None':\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            control_image = PIL.Image.fromarray(image)\n",
        "        else:\n",
        "            self.preprocessor.load(preprocessor_name)\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "            )\n",
        "        self.load_controlnet_weight('segmentation')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_depth(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        preprocessor_name: str,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        if preprocessor_name == 'None':\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            control_image = PIL.Image.fromarray(image)\n",
        "        else:\n",
        "            self.preprocessor.load(preprocessor_name)\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "            )\n",
        "        self.load_controlnet_weight('depth')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_normal(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        preprocessor_name: str,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        if preprocessor_name == 'None':\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            control_image = PIL.Image.fromarray(image)\n",
        "        else:\n",
        "            self.preprocessor.load('NormalBae')\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "            )\n",
        "        self.load_controlnet_weight('NormalBae')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_lineart(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        preprocessor_name: str,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        if preprocessor_name in ['None', 'None (anime)']:\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            control_image = PIL.Image.fromarray(image)\n",
        "        elif preprocessor_name in ['Lineart', 'Lineart coarse']:\n",
        "            coarse = 'coarse' in preprocessor_name\n",
        "            self.preprocessor.load('Lineart')\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "                coarse=coarse,\n",
        "            )\n",
        "        elif preprocessor_name == 'Lineart (anime)':\n",
        "            self.preprocessor.load('LineartAnime')\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "            )\n",
        "        if 'anime' in preprocessor_name:\n",
        "            self.load_controlnet_weight('lineart_anime')\n",
        "        else:\n",
        "            self.load_controlnet_weight('lineart')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_shuffle(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocessor_name: str,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        if preprocessor_name == 'None':\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            control_image = PIL.Image.fromarray(image)\n",
        "        else:\n",
        "            self.preprocessor.load(preprocessor_name)\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "            )\n",
        "        self.load_controlnet_weight('shuffle')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_ip2p(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        image = HWC3(image)\n",
        "        image = resize_image(image, resolution=image_resolution)\n",
        "        control_image = PIL.Image.fromarray(image)\n",
        "        self.load_controlnet_weight('ip2p')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_inpaint(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        image_mask: str,###\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        image = HWC3(image)\n",
        "        image = resize_image(image, resolution=image_resolution)\n",
        "        init_image = PIL.Image.fromarray(image)\n",
        "\n",
        "        image_mask = HWC3(image_mask)\n",
        "        image_mask = resize_image(image_mask, resolution=image_resolution)\n",
        "        control_mask = PIL.Image.fromarray(image_mask)\n",
        "\n",
        "        control_image = make_inpaint_condition(init_image, control_mask)\n",
        "\n",
        "        self.load_controlnet_weight('Inpaint')\n",
        "\n",
        "        return init_image, control_mask, control_image\n",
        "\n",
        "    def get_scheduler(self, name):\n",
        "      #Get scheduler\n",
        "      match name:\n",
        "\n",
        "        case \"DPM++ 2M\":\n",
        "          return DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config)\n",
        "\n",
        "        case \"DPM++ 2M Karras\":\n",
        "          return DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config, use_karras_sigmas=True)\n",
        "\n",
        "        case \"DPM++ 2M SDE\":\n",
        "          return DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config, algorithm_type=\"sde-dpmsolver++\")\n",
        "\n",
        "        case \"DPM++ 2M SDE Karras\":\n",
        "          return DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config, use_karras_sigmas=True, algorithm_type=\"sde-dpmsolver++\")\n",
        "\n",
        "        case \"DPM++ SDE\":\n",
        "          return DPMSolverSinglestepScheduler.from_config(self.pipe.scheduler.config, )\n",
        "\n",
        "        case \"DPM++ SDE Karras\":\n",
        "          return DPMSolverSinglestepScheduler.from_config(self.pipe.scheduler.config, use_karras_sigmas=True)\n",
        "\n",
        "        case \"DPM2\":\n",
        "          return KDPM2DiscreteScheduler.from_config(self.pipe.scheduler.config, )\n",
        "\n",
        "        case \"DPM2 Karras\":\n",
        "          return KDPM2DiscreteScheduler.from_config(self.pipe.scheduler.config, use_karras_sigmas=True)\n",
        "\n",
        "        case \"Euler\":\n",
        "          return EulerDiscreteScheduler.from_config(self.pipe.scheduler.config, )\n",
        "\n",
        "        case \"Euler a\":\n",
        "          return EulerAncestralDiscreteScheduler.from_config(self.pipe.scheduler.config, )\n",
        "\n",
        "        case \"Heun\":\n",
        "          return HeunDiscreteScheduler.from_config(self.pipe.scheduler.config, )\n",
        "\n",
        "        case \"LMS\":\n",
        "          return LMSDiscreteScheduler.from_config(self.pipe.scheduler.config, )\n",
        "\n",
        "        case \"LMS Karras\":\n",
        "          return LMSDiscreteScheduler.from_config(self.pipe.scheduler.config, use_karras_sigmas=True)\n",
        "\n",
        "        case \"DDIMScheduler\":\n",
        "          return DDIMScheduler.from_config(self.pipe.scheduler.config)\n",
        "\n",
        "        case \"DEISMultistepScheduler\":\n",
        "          return DEISMultistepScheduler.from_config(self.pipe.scheduler.config)\n",
        "\n",
        "        case \"UniPCMultistepScheduler\":\n",
        "          return UniPCMultistepScheduler.from_config(self.pipe.scheduler.config)\n",
        "\n",
        "    def process_lora(self, select_lora, lora_weights_scale, unload=False):\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        if not unload:\n",
        "            if select_lora != None:\n",
        "                  try:\n",
        "                      self.pipe = lora_mix_load(self.pipe, select_lora, lora_weights_scale, device=device, dtype=self.type_model_precision)\n",
        "                      print(select_lora)\n",
        "                  except:\n",
        "                      print(f\"ERROR: LoRA not compatible: {select_lora}\")\n",
        "            return self.pipe\n",
        "        else:\n",
        "            # Unload numerically unstable but fast\n",
        "            if select_lora != None:\n",
        "                try:\n",
        "                    self.pipe = lora_mix_load(self.pipe, select_lora, -lora_weights_scale, device=device, dtype=self.type_model_precision)\n",
        "                    #print(select_lora, 'unload')\n",
        "                except:\n",
        "                    pass\n",
        "            return self.pipe\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt = None,\n",
        "        negative_prompt = None,\n",
        "        # prompt_embeds = None,\n",
        "        # negative_prompt_embeds = None,\n",
        "        img_height = None,\n",
        "        img_width = None,\n",
        "        num_images = None,\n",
        "        num_steps = None,\n",
        "        guidance_scale = None,\n",
        "        clip_skip = True,\n",
        "        seed = None,\n",
        "        image = None, # path, np.array, or PIL image\n",
        "        preprocessor_name = None,\n",
        "        preprocess_resolution = None,\n",
        "        image_resolution = None,\n",
        "        additional_prompt = \"\",\n",
        "        image_mask = None,\n",
        "        strength = None,\n",
        "        low_threshold=None, # Canny\n",
        "        high_threshold=None, # Canny\n",
        "        value_threshold=None, # MLSD\n",
        "        distance_threshold=None, # MLSD\n",
        "        lora_A = None,\n",
        "        lora_scale_A = 1.0,\n",
        "        lora_B = None,\n",
        "        lora_scale_B = 1.0,\n",
        "        lora_C = None,\n",
        "        lora_scale_C = 1.0,\n",
        "        lora_D = None,\n",
        "        lora_scale_D = 1.0,\n",
        "        lora_E = None,\n",
        "        lora_scale_E = 1.0,\n",
        "        active_textual_inversion = False,\n",
        "        textual_inversion = [], # List of tuples [(activation_token, path_embedding),...]\n",
        "        convert_weights_prompt = False,\n",
        "        sampler = None,\n",
        "        xformers_memory_efficient_attention = True,\n",
        "        gui_active = False,\n",
        "        loop_generation = 1,\n",
        "        controlnet_conditioning_scale = 1.0,\n",
        "        control_guidance_start = 0.0,\n",
        "        control_guidance_end  = 0.0,\n",
        "        generator_in_cpu = False, # Initial noise not in CPU\n",
        "        FreeU = False,\n",
        "    ):\n",
        "        if self.task_name != \"txt2img\" and image == None:\n",
        "            raise ValueError\n",
        "        if img_height % 8 != 0 or img_width % 8 != 0:\n",
        "            raise ValueError(\"Height and width must be divisible by 8\")\n",
        "        if control_guidance_start >= control_guidance_end:\n",
        "            raise ValueError(\"Control guidance start (ControlNet Start Threshold) cannot be larger or equal to control guidance end (ControlNet Stop Threshold)\")\n",
        "\n",
        "\n",
        "\n",
        "        if self.pipe == None:\n",
        "            self.load_pipe(self.base_model_id, task_name = self.task_name, vae_model = self.vae_model)\n",
        "\n",
        "        #self.pipe.to(self.device)\n",
        "        # self.pipe.unfuse_lora()\n",
        "        # self.pipe.unload_lora_weights()\n",
        "        # self.pipe = self.process_lora(lora_A, lora_scale_A)\n",
        "        # self.pipe = self.process_lora(lora_B, lora_scale_B)\n",
        "        # self.pipe = self.process_lora(lora_C, lora_scale_C)\n",
        "        # self.pipe = self.process_lora(lora_D, lora_scale_D)\n",
        "        # self.pipe = self.process_lora(lora_E, lora_scale_E)\n",
        "\n",
        "        if xformers_memory_efficient_attention and torch.cuda.is_available():\n",
        "            self.pipe.disable_xformers_memory_efficient_attention()\n",
        "        self.pipe.to(self.device)\n",
        "\n",
        "        # in call\n",
        "        if self.lora_memory == [lora_A, lora_B, lora_C, lora_D, lora_E] and self.lora_scale_memory == [lora_scale_A, lora_scale_B, lora_scale_C, lora_scale_D, lora_scale_E]:\n",
        "                for single_lora in self.lora_memory:\n",
        "                    if single_lora != None:\n",
        "                        print(f\"LoRA in memory:{single_lora}\")\n",
        "                pass\n",
        "\n",
        "        else:\n",
        "            #print(\"_un, re and load_\")\n",
        "            self.pipe = self.process_lora(self.lora_memory[0], self.lora_scale_memory[0], unload=True)\n",
        "            self.pipe = self.process_lora(self.lora_memory[1], self.lora_scale_memory[1], unload=True)\n",
        "            self.pipe = self.process_lora(self.lora_memory[2], self.lora_scale_memory[2], unload=True)\n",
        "            self.pipe = self.process_lora(self.lora_memory[3], self.lora_scale_memory[3], unload=True)\n",
        "            self.pipe = self.process_lora(self.lora_memory[4], self.lora_scale_memory[4], unload=True)\n",
        "\n",
        "            self.pipe = self.process_lora(lora_A, lora_scale_A)\n",
        "            self.pipe = self.process_lora(lora_B, lora_scale_B)\n",
        "            self.pipe = self.process_lora(lora_C, lora_scale_C)\n",
        "            self.pipe = self.process_lora(lora_D, lora_scale_D)\n",
        "            self.pipe = self.process_lora(lora_E, lora_scale_E)\n",
        "\n",
        "        self.lora_memory = [lora_A, lora_B, lora_C, lora_D, lora_E]\n",
        "        self.lora_scale_memory = [lora_scale_A, lora_scale_B, lora_scale_C, lora_scale_D, lora_scale_E]\n",
        "\n",
        "        # FreeU for txt2img\n",
        "        if self.task_name == 'txt2img':\n",
        "            if FreeU:\n",
        "                print(\"FreeU active\")\n",
        "                if os.path.exists(self.base_model_id):\n",
        "                    #sd\n",
        "                    self.pipe.enable_freeu(s1=0.9, s2=0.2, b1=1.2, b2=1.4)\n",
        "                else:\n",
        "                    #sdxl\n",
        "                    self.pipe.enable_freeu(s1=0.6, s2=0.4, b1=1.1, b2=1.2)\n",
        "                self.FreeU = True\n",
        "            elif self.FreeU:\n",
        "                self.pipe.disable_freeu()\n",
        "                self.FreeU = False\n",
        "\n",
        "        # Prompt Optimizations for 1.5\n",
        "        if os.path.exists(self.base_model_id):\n",
        "            if  active_textual_inversion:\n",
        "              # Textual Inversion\n",
        "              for name, directory_name in textual_inversion:\n",
        "                  try:\n",
        "                          #self.pipe.text_encoder.resize_token_embeddings(len(self.pipe.tokenizer),pad_to_multiple_of=128)\n",
        "                          #self.pipe.load_textual_inversion(\"./bad_prompt.pt\", token=\"baddd\")\n",
        "                          self.pipe.load_textual_inversion(directory_name, token=name)\n",
        "                  except ValueError:\n",
        "                      # previous loaded ti\n",
        "                      pass\n",
        "                  except:\n",
        "                      print(f\"Can't apply {name}\")\n",
        "\n",
        "            #Clip skip\n",
        "            if clip_skip:\n",
        "                #clip_skip_diffusers = None #clip_skip - 1 # future update\n",
        "                compel = Compel(\n",
        "                    tokenizer=self.pipe.tokenizer,\n",
        "                    text_encoder=self.pipe.text_encoder,\n",
        "                    truncate_long_prompts=False,\n",
        "                    returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NORMALIZED\n",
        "                )\n",
        "            else:\n",
        "                #clip_skip_diffusers = None # clip_skip = None # future update\n",
        "                compel = Compel(\n",
        "                    tokenizer=self.pipe.tokenizer,\n",
        "                    text_encoder=self.pipe.text_encoder,\n",
        "                    truncate_long_prompts=False\n",
        "                )\n",
        "\n",
        "            # Prompt weights for textual inversion\n",
        "            prompt_ti = self.pipe.maybe_convert_prompt(prompt, self.pipe.tokenizer)\n",
        "            negative_prompt_ti = self.pipe.maybe_convert_prompt(negative_prompt, self.pipe.tokenizer)\n",
        "\n",
        "            # prompt syntax style a1...\n",
        "            if convert_weights_prompt:\n",
        "                prompt_ti = prompt_weight_conversor(prompt_ti)\n",
        "                negative_prompt_ti = prompt_weight_conversor(negative_prompt_ti)\n",
        "\n",
        "            # prompt embed chunks style a1...\n",
        "            prompt_emb = merge_embeds(tokenize_line(prompt_ti, self.pipe.tokenizer), compel)\n",
        "            negative_prompt_emb = merge_embeds(tokenize_line(negative_prompt_ti, self.pipe.tokenizer), compel)\n",
        "\n",
        "            # fix error shape\n",
        "            if prompt_emb.shape != negative_prompt_emb.shape:\n",
        "                prompt_emb, negative_prompt_emb = compel.pad_conditioning_tensors_to_same_length([prompt_emb, negative_prompt_emb])\n",
        "\n",
        "            compel = None\n",
        "            del compel\n",
        "\n",
        "        # Prompt Optimizations for SDXL\n",
        "        else:\n",
        "            if  active_textual_inversion:\n",
        "              # Textual Inversion\n",
        "              # for name, directory_name in textual_inversion:\n",
        "              #     try:\n",
        "              #             #self.pipe.text_encoder.resize_token_embeddings(len(self.pipe.tokenizer),pad_to_multiple_of=128)\n",
        "              #             #self.pipe.load_textual_inversion(\"./bad_prompt.pt\", token=\"baddd\")\n",
        "              #             self.pipe.load_textual_inversion(directory_name, token=name)\n",
        "              #     except ValueError:\n",
        "              #         # previous loaded ti\n",
        "              #         pass\n",
        "              #     except:\n",
        "              #         print(f\"Can't apply {name}\")\n",
        "              print(\"SDXL textual inversion not available\")\n",
        "\n",
        "            #Clip skip\n",
        "            if clip_skip:\n",
        "                #clip_skip_diffusers = None #clip_skip - 1 # future update\n",
        "                compel = Compel(\n",
        "                    tokenizer=[self.pipe.tokenizer, self.pipe.tokenizer_2] ,\n",
        "                    text_encoder=[self.pipe.text_encoder, self.pipe.text_encoder_2],\n",
        "                    returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
        "                    requires_pooled=[False, True],\n",
        "                    truncate_long_prompts=False)\n",
        "            else:\n",
        "                #clip_skip_diffusers = None # clip_skip = None # future update\n",
        "                compel = Compel(\n",
        "                    tokenizer=[self.pipe.tokenizer, self.pipe.tokenizer_2] ,\n",
        "                    text_encoder=[self.pipe.text_encoder, self.pipe.text_encoder_2],\n",
        "                    requires_pooled=[False, True],\n",
        "                    truncate_long_prompts=False)\n",
        "\n",
        "            # Prompt weights for textual inversion\n",
        "            # prompt_ti = self.pipe.maybe_convert_prompt(prompt, self.pipe.tokenizer)\n",
        "            # negative_prompt_ti = self.pipe.maybe_convert_prompt(negative_prompt, self.pipe.tokenizer)\n",
        "\n",
        "            # prompt syntax style a1...\n",
        "            if convert_weights_prompt:\n",
        "                prompt_ti = prompt_weight_conversor(prompt)\n",
        "                negative_prompt_ti = prompt_weight_conversor(negative_prompt)\n",
        "            else:\n",
        "                prompt_ti = prompt\n",
        "                negative_prompt_ti = negative_prompt\n",
        "\n",
        "            # prompt embed chunks style a1...\n",
        "            # prompt_emb = merge_embeds(tokenize_line(prompt_ti, self.pipe.tokenizer), compel)\n",
        "            # negative_prompt_emb = merge_embeds(tokenize_line(negative_prompt_ti, self.pipe.tokenizer), compel)\n",
        "\n",
        "            # fix error shape\n",
        "            # if prompt_emb.shape != negative_prompt_emb.shape:\n",
        "            #     prompt_emb, negative_prompt_emb = compel.pad_conditioning_tensors_to_same_length([prompt_emb, negative_prompt_emb])\n",
        "\n",
        "            conditioning, pooled = compel([prompt_ti, negative_prompt_ti])\n",
        "            prompt_emb = None\n",
        "            negative_prompt_emb = None\n",
        "\n",
        "            compel = None\n",
        "            del compel\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            if xformers_memory_efficient_attention:\n",
        "                self.pipe.enable_xformers_memory_efficient_attention()\n",
        "            else:\n",
        "                self.pipe.disable_xformers_memory_efficient_attention()\n",
        "\n",
        "        try:\n",
        "            self.pipe.scheduler = self.get_scheduler(sampler)\n",
        "        except:\n",
        "            print(\"Error in sampler, please try again\")\n",
        "            self.pipe = None\n",
        "            self.base_model_id = None\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            return\n",
        "\n",
        "        self.pipe.safety_checker = None\n",
        "\n",
        "        # Get Control image\n",
        "        if self.task_name != 'txt2img':\n",
        "            if isinstance(image, str):\n",
        "                # If the input is a string (file path), open it as an image\n",
        "                image_pil = Image.open(image)\n",
        "                numpy_array = np.array(image_pil, dtype=np.uint8)\n",
        "            elif isinstance(image, Image.Image):\n",
        "                # If the input is already a PIL Image, convert it to a NumPy array\n",
        "                numpy_array = np.array(image, dtype=np.uint8)\n",
        "            elif isinstance(image, np.ndarray):\n",
        "                # If the input is a NumPy array, np.uint8\n",
        "                numpy_array = image.astype(np.uint8)\n",
        "            else:\n",
        "                self.pipe = None\n",
        "                self.base_model_id = None\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "                if gui_active:\n",
        "                    print(\"To use this function, you have to upload an image in the cell below first ðŸ‘‡\")\n",
        "                    return\n",
        "                else:\n",
        "                    raise ValueError(\"Unsupported image type or not control image found\")\n",
        "\n",
        "            # Extract the RGB channels\n",
        "            try:\n",
        "                array_rgb = numpy_array[:, :, :3]\n",
        "            except:\n",
        "                print(\"Unsupported image type\")\n",
        "                self.pipe = None\n",
        "                self.base_model_id = None\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "                raise ValueError(\"Unsupported image type\") # return\n",
        "\n",
        "        # Get params preprocess\n",
        "        preprocess_params_config = {}\n",
        "        if self.task_name != 'txt2img' and self.task_name != 'Inpaint':\n",
        "            preprocess_params_config[\"image\"] = array_rgb\n",
        "            preprocess_params_config[\"image_resolution\"] = image_resolution\n",
        "            #preprocess_params_config[\"additional_prompt\"] = additional_prompt # \"\"\n",
        "\n",
        "            if self.task_name != \"ip2p\":\n",
        "                if  self.task_name != \"shuffle\":\n",
        "                    preprocess_params_config[\"preprocess_resolution\"] = preprocess_resolution\n",
        "                if self.task_name != \"MLSD\" and self.task_name != \"Canny\":\n",
        "                    preprocess_params_config[\"preprocessor_name\"] = preprocessor_name\n",
        "\n",
        "        # RUN Preprocess\n",
        "        if self.task_name == 'Inpaint':\n",
        "\n",
        "            # Get mask for Inpaint\n",
        "            if gui_active or os.path.exists(image_mask):\n",
        "                # Read image mask from gui\n",
        "                mask_control_img = Image.open(image_mask)\n",
        "                numpy_array_mask = np.array(mask_control_img, dtype=np.uint8)\n",
        "                array_rgb_mask = numpy_array_mask[:, :, :3]\n",
        "            elif not gui_active:\n",
        "                # Convert control image for draw\n",
        "                name_without_extension = os.path.splitext(image.split('/')[-1])[0]\n",
        "                image64 = base64.b64encode(open(image, 'rb').read())\n",
        "                image64 = image64.decode('utf-8')\n",
        "                img = np.array(plt.imread(f'{image}')[:,:,:3])\n",
        "\n",
        "                # Create mask interactive\n",
        "                draw(image64, filename=f\"./{name_without_extension}_draw.png\", w=img.shape[1], h=img.shape[0], line_width=0.04*img.shape[1])\n",
        "\n",
        "                # Create mask and save\n",
        "                with_mask = np.array(plt.imread(f\"./{name_without_extension}_draw.png\")[:,:,:3])\n",
        "                mask = (with_mask[:,:,0]==1)*(with_mask[:,:,1]==0)*(with_mask[:,:,2]==0)\n",
        "                plt.imsave(f\"./{name_without_extension}_mask.png\",mask, cmap='gray')\n",
        "                mask_control = f\"./{name_without_extension}_mask.png\"\n",
        "                print(f'Mask saved: {mask_control}')\n",
        "\n",
        "                # Read image mask\n",
        "                mask_control_img = Image.open(mask_control)\n",
        "                numpy_array_mask = np.array(mask_control_img, dtype=np.uint8)\n",
        "                array_rgb_mask = numpy_array_mask[:, :, :3]\n",
        "            else:\n",
        "                raise ValueError(\"No images found\")\n",
        "\n",
        "            init_image, control_mask, control_image = self.process_inpaint(\n",
        "                image=array_rgb,\n",
        "                image_resolution=image_resolution,\n",
        "                preprocess_resolution=preprocess_resolution, # Not used\n",
        "                image_mask=array_rgb_mask,\n",
        "            )\n",
        "\n",
        "        elif self.task_name == 'Openpose':\n",
        "            print('Openpose')\n",
        "            control_image = self.process_openpose(**preprocess_params_config)\n",
        "\n",
        "        elif self.task_name == 'Canny':\n",
        "            print('Canny')\n",
        "            control_image = self.process_canny(\n",
        "                **preprocess_params_config,\n",
        "                low_threshold=low_threshold,\n",
        "                high_threshold=high_threshold,\n",
        "            )\n",
        "\n",
        "        elif self.task_name == 'MLSD':\n",
        "            print('MLSD')\n",
        "            control_image = self.process_mlsd(\n",
        "                **preprocess_params_config,\n",
        "                value_threshold=value_threshold,\n",
        "                distance_threshold=distance_threshold,\n",
        "            )\n",
        "\n",
        "        elif self.task_name == 'scribble':\n",
        "            print('Scribble')\n",
        "            control_image = self.process_scribble(**preprocess_params_config)\n",
        "\n",
        "        elif self.task_name == 'softedge':\n",
        "            print('Softedge')\n",
        "            control_image = self.process_softedge(**preprocess_params_config)\n",
        "\n",
        "        elif self.task_name == 'segmentation':\n",
        "            print('Segmentation')\n",
        "            control_image = self.process_segmentation(**preprocess_params_config)\n",
        "\n",
        "        elif self.task_name == 'depth':\n",
        "            print('Depth')\n",
        "            control_image = self.process_depth(**preprocess_params_config)\n",
        "\n",
        "        elif self.task_name == 'NormalBae':\n",
        "            print('NormalBae')\n",
        "            control_image = self.process_normal(**preprocess_params_config)\n",
        "\n",
        "        elif 'lineart' in self.task_name:\n",
        "            print('Lineart')\n",
        "            control_image = self.process_lineart(**preprocess_params_config)\n",
        "\n",
        "        elif self.task_name == 'shuffle':\n",
        "            print('Shuffle')\n",
        "            control_image = self.process_shuffle(**preprocess_params_config)\n",
        "\n",
        "        elif self.task_name == 'ip2p':\n",
        "            print('Ip2p')\n",
        "            control_image = self.process_ip2p(**preprocess_params_config)\n",
        "\n",
        "\n",
        "        # Get params for TASK\n",
        "        pipe_params_config = {\n",
        "            \"prompt\": None, #prompt, #self.get_prompt(prompt, additional_prompt),\n",
        "            \"negative_prompt\": None, #negative_prompt,\n",
        "            \"prompt_embeds\": prompt_emb,\n",
        "            \"negative_prompt_embeds\": negative_prompt_emb,\n",
        "            \"num_images\": num_images,\n",
        "            \"num_steps\": num_steps,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"clip_skip\": None, #clip_skip, because we use clip skip of compel\n",
        "        }\n",
        "\n",
        "        if not os.path.exists(self.base_model_id):\n",
        "            # pipe_params_config[\"prompt_embeds\"]                 = conditioning[0:1]\n",
        "            # pipe_params_config[\"pooled_prompt_embeds\"]          = pooled[0:1],\n",
        "            # pipe_params_config[\"negative_prompt_embeds\"]        = conditioning[1:2]\n",
        "            # pipe_params_config[\"negative_pooled_prompt_embeds\"] = pooled[1:2],\n",
        "            # pipe_params_config[\"conditioning\"]                  = conditioning,\n",
        "            # pipe_params_config[\"pooled\"]                        = pooled,\n",
        "            # pipe_params_config[\"height\"]                        = img_height\n",
        "            # pipe_params_config[\"width\"]                         = img_width\n",
        "            pass\n",
        "        elif self.task_name == 'txt2img':\n",
        "            pipe_params_config[\"height\"]                        = img_height\n",
        "            pipe_params_config[\"width\"]                         = img_width\n",
        "        elif self.task_name == \"Inpaint\":\n",
        "            pipe_params_config[\"strength\"]                      = strength\n",
        "            pipe_params_config[\"init_image\"]                    = init_image\n",
        "            pipe_params_config[\"control_mask\"]                  = control_mask\n",
        "            pipe_params_config[\"control_image\"]                 = control_image\n",
        "            pipe_params_config[\"controlnet_conditioning_scale\"] = controlnet_conditioning_scale\n",
        "            pipe_params_config[\"control_guidance_start\"]        = control_guidance_start\n",
        "            pipe_params_config[\"control_guidance_end\"]          = control_guidance_end\n",
        "            print(f\"Image resolution: {str(init_image.size)}\")\n",
        "        elif self.task_name != 'txt2img' and self.task_name != 'Inpaint':\n",
        "            pipe_params_config[\"control_image\"]                 = control_image\n",
        "            pipe_params_config[\"controlnet_conditioning_scale\"] = controlnet_conditioning_scale\n",
        "            pipe_params_config[\"control_guidance_start\"]        = control_guidance_start\n",
        "            pipe_params_config[\"control_guidance_end\"]          = control_guidance_end\n",
        "            print(f\"Image resolution: {str(control_image.size)}\")\n",
        "\n",
        "        ### RUN PIPE ###\n",
        "        for i in range (loop_generation):\n",
        "\n",
        "            calculate_seed = random.randint(0, 2147483647) if seed == -1 else seed\n",
        "            if generator_in_cpu:\n",
        "                pipe_params_config[\"generator\"] = torch.Generator().manual_seed(calculate_seed)\n",
        "            else:\n",
        "                try:\n",
        "                    pipe_params_config[\"generator\"] = torch.Generator(\"cuda\").manual_seed(calculate_seed)\n",
        "                except:\n",
        "                    print(\"Generator in CPU\")\n",
        "                    pipe_params_config[\"generator\"] = torch.Generator().manual_seed(calculate_seed)\n",
        "\n",
        "            if not os.path.exists(self.base_model_id):\n",
        "                #images = self.run_pipe_SDXL(**pipe_params_config)\n",
        "                images = self.pipe(\n",
        "                    prompt = None,\n",
        "                    negative_prompt = None,\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    height = img_height,\n",
        "                    width = img_width,\n",
        "                    num_inference_steps = num_steps,\n",
        "                    guidance_scale = guidance_scale,\n",
        "                    clip_skip = None,\n",
        "                    num_images_per_prompt = num_images,\n",
        "                    generator = pipe_params_config[\"generator\"],\n",
        "                ).images\n",
        "            elif self.task_name == 'txt2img':\n",
        "                images = self.run_pipe_SD(**pipe_params_config)\n",
        "            elif self.task_name == \"Inpaint\":\n",
        "                images = self.run_pipe_inpaint(**pipe_params_config)\n",
        "            elif self.task_name != 'txt2img' and self.task_name != 'Inpaint':\n",
        "                results = self.run_pipe(**pipe_params_config) ## pipe ControlNet add condition_weights\n",
        "                images =  [control_image] + results\n",
        "                del results\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            if loop_generation > 1:\n",
        "\n",
        "                mediapy.show_images(images)\n",
        "                #print(image_list)\n",
        "                #del images\n",
        "                time.sleep(1)\n",
        "\n",
        "            # save image\n",
        "            image_list = []\n",
        "            metadata = [\n",
        "                    prompt,\n",
        "                    negative_prompt,\n",
        "                    self.base_model_id,\n",
        "                    self.vae_model,\n",
        "                    num_steps,\n",
        "                    guidance_scale,\n",
        "                    sampler,\n",
        "                    calculate_seed\n",
        "            ]\n",
        "\n",
        "            for image_ in images:\n",
        "                image_path = save_pil_image_with_metadata(image_, \"./images\", metadata)\n",
        "                image_list.append(image_path)\n",
        "\n",
        "            if loop_generation > 1:\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "                print(image_list)\n",
        "            print(f\"Seed:\\n{calculate_seed}\")\n",
        "\n",
        "\n",
        "\n",
        "        # if select_lora1.value != \"None\":\n",
        "        #     model.pipe.unfuse_lora()\n",
        "        #     model.pipe.unload_lora_weights()\n",
        "        # if select_lora2.value != \"None\" or select_lora3.value != \"None\":\n",
        "        #     print('BETA: reload weights for lora')\n",
        "        #     model.load_pipe(select_model.value, task_name=options_controlnet.value, vae_model = vae_model_dropdown.value, reload=True)\n",
        "        # self.pipe.to(self.device)\n",
        "        # self.pipe = self.process_lora(lora_A, lora_scale_A, unload=True)\n",
        "        # self.pipe = self.process_lora(lora_B, lora_scale_B, unload=True)\n",
        "        # self.pipe = self.process_lora(lora_C, lora_scale_C, unload=True)\n",
        "        # self.pipe = self.process_lora(lora_D, lora_scale_D, unload=True)\n",
        "        # self.pipe = self.process_lora(lora_E, lora_scale_E, unload=True)\n",
        "        # model.pipe.unfuse_lora()\n",
        "        # model.pipe.unload_lora_weights()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return images, image_list\n",
        "\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# Prompt weights\n",
        "# =====================================\n",
        "from compel import Compel\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "import re\n",
        "from compel import EmbeddingsProvider, ReturnedEmbeddingsType\n",
        "\n",
        "def concat_tensor(t):\n",
        "    t_list = torch.split(t, 1, dim=0)\n",
        "    t = torch.cat(t_list, dim=1)\n",
        "    return t\n",
        "\n",
        "def merge_embeds(prompt_chanks, compel):\n",
        "    num_chanks = len(prompt_chanks)\n",
        "    power_prompt = 1/(num_chanks*(num_chanks+1)//2)\n",
        "    prompt_embs = compel(prompt_chanks)\n",
        "    t_list = list(torch.split(prompt_embs, 1, dim=0))\n",
        "    for i in range(num_chanks):\n",
        "        t_list[-(i+1)] = t_list[-(i+1)] * ((i+1)*power_prompt)\n",
        "    prompt_emb = torch.stack(t_list, dim=0).sum(dim=0)\n",
        "    return prompt_emb\n",
        "\n",
        "def detokenize(chunk, actual_prompt):\n",
        "    chunk[-1] = chunk[-1].replace('</w>', '')\n",
        "    chanked_prompt = ''.join(chunk).strip()\n",
        "    while '</w>' in chanked_prompt:\n",
        "        if actual_prompt[chanked_prompt.find('</w>')] == ' ':\n",
        "            chanked_prompt = chanked_prompt.replace('</w>', ' ', 1)\n",
        "        else:\n",
        "            chanked_prompt = chanked_prompt.replace('</w>', '', 1)\n",
        "    actual_prompt = actual_prompt.replace(chanked_prompt,'')\n",
        "    return chanked_prompt.strip(), actual_prompt.strip()\n",
        "\n",
        "def tokenize_line(line, tokenizer): # split into chunks\n",
        "    actual_prompt = line.lower().strip()\n",
        "    if actual_prompt == \"\":\n",
        "      actual_prompt = 'worst quality'\n",
        "    actual_tokens = tokenizer.tokenize(actual_prompt)\n",
        "    max_tokens = tokenizer.model_max_length - 2\n",
        "    comma_token = tokenizer.tokenize(',')[0]\n",
        "\n",
        "    chunks = []\n",
        "    chunk = []\n",
        "    for item in actual_tokens:\n",
        "        chunk.append(item)\n",
        "        if len(chunk) == max_tokens:\n",
        "            if chunk[-1] != comma_token:\n",
        "                for i in range(max_tokens-1, -1, -1):\n",
        "                    if chunk[i] == comma_token:\n",
        "                        actual_chunk, actual_prompt = detokenize(chunk[:i+1], actual_prompt)\n",
        "                        chunks.append(actual_chunk)\n",
        "                        chunk = chunk[i+1:]\n",
        "                        break\n",
        "                else:\n",
        "                    actual_chunk, actual_prompt = detokenize(chunk, actual_prompt)\n",
        "                    chunks.append(actual_chunk)\n",
        "                    chunk = []\n",
        "            else:\n",
        "                actual_chunk, actual_prompt = detokenize(chunk, actual_prompt)\n",
        "                chunks.append(actual_chunk)\n",
        "                chunk = []\n",
        "    if chunk:\n",
        "        actual_chunk, _ = detokenize(chunk, actual_prompt)\n",
        "        chunks.append(actual_chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def prompt_weight_conversor(input_string):\n",
        "    # Convert prompt weights from a1... to comel\n",
        "\n",
        "    # Find and replace instances of the colon format with the desired format\n",
        "    converted_string = re.sub(r'\\(([^:]+):([\\d.]+)\\)', r'(\\1)\\2', input_string)\n",
        "\n",
        "    # Find and replace square brackets with round brackets and assign weight\n",
        "    converted_string = re.sub(r'\\[([^:\\]]+)\\]', r'(\\1)0.909090909', converted_string)\n",
        "\n",
        "    # Handle the general case of [x:number] and convert it to (x)0.9\n",
        "    converted_string = re.sub(r'\\[([^:]+):[\\d.]+\\]', r'(\\1)0.9', converted_string)\n",
        "\n",
        "    # Add a '+' sign after the closing parenthesis if no weight is specified\n",
        "    modified_string = re.sub(r'\\(([^)]+)\\)(?![\\d.])', r'(\\1)+', converted_string)\n",
        "\n",
        "    # double (())\n",
        "    #modified_string = re.sub(r'\\(\\(([^)]+)\\)\\+\\)', r'(\\1)++', modified_string)\n",
        "\n",
        "    # triple ((()))\n",
        "    #modified_string = re.sub(r'\\(\\(([^)]+)\\)\\+\\+\\)', r'(\\1)+++', modified_string)\n",
        "\n",
        "    #print(modified_string)\n",
        "    return modified_string\n",
        "\n",
        "# =====================================\n",
        "# IMAGE: METADATA AND SAVE\n",
        "# =====================================\n",
        "import os\n",
        "from PIL import Image\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "\n",
        "def save_pil_image_with_metadata(image, folder_path, metadata_list):\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    existing_files = os.listdir(folder_path)\n",
        "\n",
        "    # Determine the next available image name\n",
        "    image_name = f\"image{str(len(existing_files) + 1).zfill(3)}.png\"\n",
        "    image_path = os.path.join(folder_path, image_name)\n",
        "\n",
        "    try:\n",
        "        # metadata\n",
        "        metadata = PngInfo()\n",
        "        metadata.add_text(\"Prompt\", str(metadata_list[0]))\n",
        "        metadata.add_text(\"Negative prompt\", str(metadata_list[1]))\n",
        "        metadata.add_text(\"Model\", str(metadata_list[2]))\n",
        "        metadata.add_text(\"VAE\", str(metadata_list[3]))\n",
        "        metadata.add_text(\"Steps\", str(metadata_list[4]))\n",
        "        metadata.add_text(\"CFG\", str(metadata_list[5]))\n",
        "        metadata.add_text(\"Scheduler\", str(metadata_list[6]))\n",
        "        metadata.add_text(\"Seed\", str(metadata_list[7]))\n",
        "\n",
        "        image.save(image_path, pnginfo=metadata)\n",
        "    except:\n",
        "        print('Saving image without metadata')\n",
        "        image.save(image_path)\n",
        "\n",
        "    return image_path\n",
        "\n",
        "# =====================================\n",
        "# LoRA Loaders\n",
        "# =====================================\n",
        "import torch\n",
        "from safetensors.torch import load_file\n",
        "from collections import defaultdict\n",
        "def load_lora_weights(pipeline, checkpoint_path, multiplier, device, dtype):\n",
        "    LORA_PREFIX_UNET = \"lora_unet\"\n",
        "    LORA_PREFIX_TEXT_ENCODER = \"lora_te\"\n",
        "    # load LoRA weight from .safetensors\n",
        "    if isinstance(checkpoint_path, str):\n",
        "\n",
        "        state_dict = load_file(checkpoint_path, device=device)\n",
        "\n",
        "        updates = defaultdict(dict)\n",
        "        for key, value in state_dict.items():\n",
        "            # it is suggested to print out the key, it usually will be something like below\n",
        "            # \"lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\"\n",
        "\n",
        "            layer, elem = key.split('.', 1)\n",
        "            updates[layer][elem] = value\n",
        "\n",
        "        # directly update weight in diffusers model\n",
        "        for layer, elems in updates.items():\n",
        "\n",
        "            if \"text\" in layer:\n",
        "                layer_infos = layer.split(LORA_PREFIX_TEXT_ENCODER + \"_\")[-1].split(\"_\")\n",
        "                curr_layer = pipeline.text_encoder\n",
        "            else:\n",
        "                layer_infos = layer.split(LORA_PREFIX_UNET + \"_\")[-1].split(\"_\")\n",
        "                curr_layer = pipeline.unet\n",
        "\n",
        "            # find the target layer\n",
        "            temp_name = layer_infos.pop(0)\n",
        "            while len(layer_infos) > -1:\n",
        "                try:\n",
        "                    curr_layer = curr_layer.__getattr__(temp_name)\n",
        "                    if len(layer_infos) > 0:\n",
        "                        temp_name = layer_infos.pop(0)\n",
        "                    elif len(layer_infos) == 0:\n",
        "                        break\n",
        "                except Exception:\n",
        "                    if len(temp_name) > 0:\n",
        "                        temp_name += \"_\" + layer_infos.pop(0)\n",
        "                    else:\n",
        "                        temp_name = layer_infos.pop(0)\n",
        "\n",
        "            # get elements for this layer\n",
        "            weight_up = elems['lora_up.weight'].to(dtype)\n",
        "            weight_down = elems['lora_down.weight'].to(dtype)\n",
        "            alpha = elems['alpha']\n",
        "            if alpha:\n",
        "                alpha = alpha.item() / weight_up.shape[1]\n",
        "            else:\n",
        "                alpha = 1.0\n",
        "\n",
        "            # update weight\n",
        "            if len(weight_up.shape) == 4:\n",
        "                curr_layer.weight.data += multiplier * alpha * torch.mm(weight_up.squeeze(3).squeeze(2), weight_down.squeeze(3).squeeze(2)).unsqueeze(2).unsqueeze(3)\n",
        "            else:\n",
        "                curr_layer.weight.data += multiplier * alpha * torch.mm(weight_up, weight_down)\n",
        "    else:\n",
        "        for ckptpath in checkpoint_path:\n",
        "            state_dict = load_file(ckptpath, device=device)\n",
        "\n",
        "            updates = defaultdict(dict)\n",
        "            for key, value in state_dict.items():\n",
        "                # it is suggested to print out the key, it usually will be something like below\n",
        "                # \"lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\"\n",
        "\n",
        "                layer, elem = key.split('.', 1)\n",
        "                updates[layer][elem] = value\n",
        "\n",
        "            # directly update weight in diffusers model\n",
        "            for layer, elems in updates.items():\n",
        "\n",
        "                if \"text\" in layer:\n",
        "                    layer_infos = layer.split(LORA_PREFIX_TEXT_ENCODER + \"_\")[-1].split(\"_\")\n",
        "                    curr_layer = pipeline.text_encoder\n",
        "                else:\n",
        "                    layer_infos = layer.split(LORA_PREFIX_UNET + \"_\")[-1].split(\"_\")\n",
        "                    curr_layer = pipeline.unet\n",
        "\n",
        "                # find the target layer\n",
        "                temp_name = layer_infos.pop(0)\n",
        "                while len(layer_infos) > -1:\n",
        "                    try:\n",
        "                        curr_layer = curr_layer.__getattr__(temp_name)\n",
        "                        if len(layer_infos) > 0:\n",
        "                            temp_name = layer_infos.pop(0)\n",
        "                        elif len(layer_infos) == 0:\n",
        "                            break\n",
        "                    except Exception:\n",
        "                        if len(temp_name) > 0:\n",
        "                            temp_name += \"_\" + layer_infos.pop(0)\n",
        "                        else:\n",
        "                            temp_name = layer_infos.pop(0)\n",
        "\n",
        "                # get elements for this layer\n",
        "                weight_up = elems['lora_up.weight'].to(dtype)\n",
        "                weight_down = elems['lora_down.weight'].to(dtype)\n",
        "                alpha = elems['alpha']\n",
        "                if alpha:\n",
        "                    alpha = alpha.item() / weight_up.shape[1]\n",
        "                else:\n",
        "                    alpha = 1.0\n",
        "\n",
        "                # update weight\n",
        "                if len(weight_up.shape) == 4:\n",
        "                    curr_layer.weight.data += multiplier * alpha * torch.mm(weight_up.squeeze(3).squeeze(2), weight_down.squeeze(3).squeeze(2)).unsqueeze(2).unsqueeze(3)\n",
        "                else:\n",
        "                    curr_layer.weight.data += multiplier * alpha * torch.mm(weight_up, weight_down)\n",
        "    return pipeline\n",
        "\n",
        "def lora_mix_load(pipe, lora_path, alpha_scale=1.0, device='cuda', dtype=torch.float16):\n",
        "    try:\n",
        "        pipe=load_lora_weights(pipe, [lora_path], alpha_scale, device=device, dtype=torch.float16)\n",
        "    except:\n",
        "        pipe.load_lora_weights(lora_path)\n",
        "        pipe.fuse_lora(lora_scale=alpha_scale)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# Inpainting canvas\n",
        "# =====================================\n",
        "canvas_html = \"\"\"\n",
        "<style>\n",
        ".button {\n",
        "  background-color: #4CAF50;\n",
        "  border: none;\n",
        "  color: white;\n",
        "  padding: 15px 32px;\n",
        "  text-align: center;\n",
        "  text-decoration: none;\n",
        "  display: inline-block;\n",
        "  font-size: 16px;\n",
        "  margin: 4px 2px;\n",
        "  cursor: pointer;\n",
        "}\n",
        "</style>\n",
        "<canvas1 width=%d height=%d>\n",
        "</canvas1>\n",
        "<canvas width=%d height=%d>\n",
        "</canvas>\n",
        "\n",
        "<button>Finish</button>\n",
        "<script>\n",
        "var canvas = document.querySelector('canvas')\n",
        "var ctx = canvas.getContext('2d')\n",
        "\n",
        "var canvas1 = document.querySelector('canvas1')\n",
        "var ctx1 = canvas.getContext('2d')\n",
        "\n",
        "\n",
        "ctx.strokeStyle = 'red';\n",
        "\n",
        "var img = new Image();\n",
        "img.src = \"data:image/%s;charset=utf-8;base64,%s\";\n",
        "console.log(img)\n",
        "img.onload = function() {\n",
        "  ctx1.drawImage(img, 0, 0);\n",
        "};\n",
        "img.crossOrigin = 'Anonymous';\n",
        "\n",
        "ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "\n",
        "ctx.lineWidth = %d\n",
        "var button = document.querySelector('button')\n",
        "var mouse = {x: 0, y: 0}\n",
        "\n",
        "canvas.addEventListener('mousemove', function(e) {\n",
        "  mouse.x = e.pageX - this.offsetLeft\n",
        "  mouse.y = e.pageY - this.offsetTop\n",
        "})\n",
        "canvas.onmousedown = ()=>{\n",
        "  ctx.beginPath()\n",
        "  ctx.moveTo(mouse.x, mouse.y)\n",
        "  canvas.addEventListener('mousemove', onPaint)\n",
        "}\n",
        "canvas.onmouseup = ()=>{\n",
        "  canvas.removeEventListener('mousemove', onPaint)\n",
        "}\n",
        "var onPaint = ()=>{\n",
        "  ctx.lineTo(mouse.x, mouse.y)\n",
        "  ctx.stroke()\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "  button.onclick = ()=>{\n",
        "    resolve(canvas.toDataURL('image/png'))\n",
        "  }\n",
        "})\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "import base64, os\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from shutil import copyfile\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def draw(imgm, filename='drawing.png', w=400, h=200, line_width=1):\n",
        "  display(HTML(canvas_html % (w, h, w,h, filename.split('.')[-1], imgm, line_width)))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "\n",
        "# the control image of init_image and mask_image\n",
        "def make_inpaint_condition(image, image_mask):\n",
        "    image = np.array(image.convert(\"RGB\")).astype(np.float32) / 255.0\n",
        "    image_mask = np.array(image_mask.convert(\"L\")).astype(np.float32) / 255.0\n",
        "\n",
        "    assert image.shape[0:1] == image_mask.shape[0:1], \"image and image_mask must have the same image size\"\n",
        "    image[image_mask > 0.5] = -1.0  # set as masked pixel\n",
        "    image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return image\n",
        "\n",
        "# =====================================\n",
        "# Adetailer\n",
        "# =====================================\n",
        "from functools import partial\n",
        "from diffusers import DPMSolverMultistepScheduler, DPMSolverSinglestepScheduler\n",
        "from asdff import AdCnPipeline, AdPipeline, yolo_detector\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "def ad_model_process(\n",
        "    face_detector_ad,\n",
        "    person_detector_ad,\n",
        "    hand_detector_ad,\n",
        "    model_repo_id,\n",
        "    common,\n",
        "    inpaint,\n",
        "    image_list_task,\n",
        "    ):\n",
        "\n",
        "    pipe = AdPipeline.from_pretrained(model_repo_id, torch_dtype=torch.float16)\n",
        "\n",
        "    try:\n",
        "        pipe.scheduler = DPMSolverSinglestepScheduler.from_config(pipe.scheduler.config, use_karras_sigmas=True)\n",
        "    except:\n",
        "        pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "    pipe.safety_checker = None\n",
        "    pipe.to(\"cuda\")\n",
        "\n",
        "    image_list_ad = []\n",
        "\n",
        "    for path in image_list_task:\n",
        "        if os.path.exists(path):\n",
        "            # Open the image using PIL and convert it to PIL.Image.Image\n",
        "            with Image.open(path) as img:\n",
        "                images_ad = img.convert(\"RGB\")\n",
        "\n",
        "        detectors = []\n",
        "        if face_detector_ad:\n",
        "            face_model_path = hf_hub_download(\"Bingsu/adetailer\", \"face_yolov8n.pt\")\n",
        "            face_detector = partial(yolo_detector, model_path=face_model_path)\n",
        "            detectors.append(face_detector)\n",
        "        if person_detector_ad:\n",
        "            person_model_path = hf_hub_download(\"Bingsu/adetailer\", \"person_yolov8s-seg.pt\")\n",
        "            person_detector = partial(yolo_detector, model_path=person_model_path)\n",
        "            detectors.append(person_detector)\n",
        "        if hand_detector_ad:\n",
        "            hand_model_path = hf_hub_download(\"Bingsu/adetailer\", \"hand_yolov8n.pt\")\n",
        "            hand_detector = partial(yolo_detector, model_path=hand_model_path)\n",
        "            detectors.append(hand_detector)\n",
        "\n",
        "        result_ad = pipe(images=[images_ad], common=common, inpaint_only=inpaint, detectors=detectors)\n",
        "\n",
        "        try:\n",
        "            mediapy.show_images([result_ad[0][0], result_ad[1][0]])\n",
        "        except:\n",
        "            del pipe\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            return image_list_task\n",
        "\n",
        "        image_path = save_pil_image_with_metadata(result_ad[0][0], f'{os.getcwd()}/images', metadata_list=None)\n",
        "        image_list_ad.append(image_path)\n",
        "\n",
        "    del pipe\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return image_list_ad\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Experimental upscaler\n",
        "# =====================================\n",
        "# BASE UPSCALER\n",
        "# =====================================\n",
        "devicee = 'cuda' # run in CPU is very slow\n",
        "\n",
        "from abc import abstractmethod\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "LANCZOS = (Image.Resampling.LANCZOS if hasattr(Image, 'Resampling') else Image.LANCZOS)\n",
        "NEAREST = (Image.Resampling.NEAREST if hasattr(Image, 'Resampling') else Image.NEAREST)\n",
        "\n",
        "\n",
        "\n",
        "class Upscaler:\n",
        "    name = None\n",
        "    model_path = None\n",
        "    model_name = None\n",
        "    model_url = None\n",
        "    enable = True\n",
        "    filter = None\n",
        "    model = None\n",
        "    user_path = None\n",
        "    scalers: []\n",
        "    tile = True\n",
        "\n",
        "    def __init__(self, create_dirs=False):\n",
        "        self.mod_pad_h = None\n",
        "        self.tile_size = 100\n",
        "        self.tile_pad = 10\n",
        "        self.device = devicee # â­\n",
        "        self.img = None\n",
        "        self.output = None\n",
        "        self.scale = 4.0\n",
        "        self.half = True\n",
        "        self.pre_pad = 0\n",
        "        self.mod_scale = None\n",
        "        self.model_download_path = None\n",
        "\n",
        "        if self.model_path is None and self.name:\n",
        "            self.model_path = os.path.join('/content/Real-ESRGAN/weights/RealESRGAN_x2plus.pth', self.name)\n",
        "        if self.model_path and create_dirs:\n",
        "            os.makedirs(self.model_path, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            import cv2  # noqa: F401\n",
        "            self.can_tile = True\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def do_upscale(self, img: PIL.Image, selected_model: str):\n",
        "        return img\n",
        "\n",
        "    def upscale(self, img: PIL.Image, scale, selected_model: str = None):\n",
        "        self.scale = scale\n",
        "        dest_w = int((img.width * scale) // 8 * 8)\n",
        "        dest_h = int((img.height * scale) // 8 * 8)\n",
        "\n",
        "        for _ in range(3):\n",
        "            shape = (img.width, img.height)\n",
        "\n",
        "            img = self.do_upscale(img, selected_model)\n",
        "\n",
        "            if shape == (img.width, img.height):\n",
        "                break\n",
        "\n",
        "            if img.width >= dest_w and img.height >= dest_h:\n",
        "                break\n",
        "\n",
        "        if img.width != dest_w or img.height != dest_h:\n",
        "            img = img.resize((int(dest_w), int(dest_h)), resample=LANCZOS)\n",
        "\n",
        "        return img\n",
        "\n",
        "    @abstractmethod\n",
        "    def load_model(self, path: str):\n",
        "        pass\n",
        "\n",
        "    def find_models(self, ext_filter=None) -> list:\n",
        "        return load_models(model_path=self.model_path, model_url=self.model_url, command_path=self.user_path, ext_filter=ext_filter)\n",
        "\n",
        "    def update_status(self, prompt):\n",
        "        print(f\"\\nextras: {prompt}\", file=shared.progress_print_out)\n",
        "\n",
        "\n",
        "class UpscalerData:\n",
        "    name = None\n",
        "    data_path = None\n",
        "    scale: int = 4\n",
        "    scaler: Upscaler = None\n",
        "    model: None\n",
        "\n",
        "    def __init__(self, name: str, path: str, upscaler: Upscaler = None, scale: int = 4, model=None):\n",
        "        self.name = name\n",
        "        self.data_path = path\n",
        "        self.local_data_path = path\n",
        "        self.scaler = upscaler\n",
        "        self.scale = scale\n",
        "        self.model = model\n",
        "\n",
        "\n",
        "class UpscalerNone(Upscaler):\n",
        "    name = \"None\"\n",
        "    scalers = []\n",
        "\n",
        "    def load_model(self, path):\n",
        "        pass\n",
        "\n",
        "    def do_upscale(self, img, selected_model=None):\n",
        "        return img\n",
        "\n",
        "    def __init__(self, dirname=None):\n",
        "        super().__init__(False)\n",
        "        self.scalers = [UpscalerData(\"None\", None, self)]\n",
        "\n",
        "\n",
        "class UpscalerLanczos(Upscaler):\n",
        "    scalers = []\n",
        "\n",
        "    def do_upscale(self, img, selected_model=None):\n",
        "        return img.resize((int(img.width * self.scale), int(img.height * self.scale)), resample=LANCZOS)\n",
        "\n",
        "    def load_model(self, _):\n",
        "        pass\n",
        "\n",
        "    def __init__(self, dirname=None):\n",
        "        super().__init__(False)\n",
        "        self.name = \"Lanczos\"\n",
        "        self.scalers = [UpscalerData(\"Lanczos\", None, self)]\n",
        "\n",
        "\n",
        "class UpscalerNearest(Upscaler):\n",
        "    scalers = []\n",
        "\n",
        "    def do_upscale(self, img, selected_model=None):\n",
        "        return img.resize((int(img.width * self.scale), int(img.height * self.scale)), resample=NEAREST)\n",
        "\n",
        "    def load_model(self, _):\n",
        "        pass\n",
        "\n",
        "    def __init__(self, dirname=None):\n",
        "        super().__init__(False)\n",
        "        self.name = \"Nearest\"\n",
        "        self.scalers = [UpscalerData(\"Nearest\", None, self)]\n",
        "\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# UpscalerESRGAN\n",
        "# =====================================\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "def mod2normal(state_dict):\n",
        "    # this code is copied from https://github.com/victorca25/iNNfer\n",
        "    if 'conv_first.weight' in state_dict:\n",
        "        crt_net = {}\n",
        "        items = list(state_dict)\n",
        "\n",
        "        crt_net['model.0.weight'] = state_dict['conv_first.weight']\n",
        "        crt_net['model.0.bias'] = state_dict['conv_first.bias']\n",
        "\n",
        "        for k in items.copy():\n",
        "            if 'RDB' in k:\n",
        "                ori_k = k.replace('RRDB_trunk.', 'model.1.sub.')\n",
        "                if '.weight' in k:\n",
        "                    ori_k = ori_k.replace('.weight', '.0.weight')\n",
        "                elif '.bias' in k:\n",
        "                    ori_k = ori_k.replace('.bias', '.0.bias')\n",
        "                crt_net[ori_k] = state_dict[k]\n",
        "                items.remove(k)\n",
        "\n",
        "        crt_net['model.1.sub.23.weight'] = state_dict['trunk_conv.weight']\n",
        "        crt_net['model.1.sub.23.bias'] = state_dict['trunk_conv.bias']\n",
        "        crt_net['model.3.weight'] = state_dict['upconv1.weight']\n",
        "        crt_net['model.3.bias'] = state_dict['upconv1.bias']\n",
        "        crt_net['model.6.weight'] = state_dict['upconv2.weight']\n",
        "        crt_net['model.6.bias'] = state_dict['upconv2.bias']\n",
        "        crt_net['model.8.weight'] = state_dict['HRconv.weight']\n",
        "        crt_net['model.8.bias'] = state_dict['HRconv.bias']\n",
        "        crt_net['model.10.weight'] = state_dict['conv_last.weight']\n",
        "        crt_net['model.10.bias'] = state_dict['conv_last.bias']\n",
        "        state_dict = crt_net\n",
        "    return state_dict\n",
        "\n",
        "\n",
        "def resrgan2normal(state_dict, nb=23):\n",
        "    # this code is copied from https://github.com/victorca25/iNNfer\n",
        "    if \"conv_first.weight\" in state_dict and \"body.0.rdb1.conv1.weight\" in state_dict:\n",
        "        re8x = 0\n",
        "        crt_net = {}\n",
        "        items = list(state_dict)\n",
        "\n",
        "        crt_net['model.0.weight'] = state_dict['conv_first.weight']\n",
        "        crt_net['model.0.bias'] = state_dict['conv_first.bias']\n",
        "\n",
        "        for k in items.copy():\n",
        "            if \"rdb\" in k:\n",
        "                ori_k = k.replace('body.', 'model.1.sub.')\n",
        "                ori_k = ori_k.replace('.rdb', '.RDB')\n",
        "                if '.weight' in k:\n",
        "                    ori_k = ori_k.replace('.weight', '.0.weight')\n",
        "                elif '.bias' in k:\n",
        "                    ori_k = ori_k.replace('.bias', '.0.bias')\n",
        "                crt_net[ori_k] = state_dict[k]\n",
        "                items.remove(k)\n",
        "\n",
        "        crt_net[f'model.1.sub.{nb}.weight'] = state_dict['conv_body.weight']\n",
        "        crt_net[f'model.1.sub.{nb}.bias'] = state_dict['conv_body.bias']\n",
        "        crt_net['model.3.weight'] = state_dict['conv_up1.weight']\n",
        "        crt_net['model.3.bias'] = state_dict['conv_up1.bias']\n",
        "        crt_net['model.6.weight'] = state_dict['conv_up2.weight']\n",
        "        crt_net['model.6.bias'] = state_dict['conv_up2.bias']\n",
        "\n",
        "        if 'conv_up3.weight' in state_dict:\n",
        "            # modification supporting: https://github.com/ai-forever/Real-ESRGAN/blob/main/RealESRGAN/rrdbnet_arch.py\n",
        "            re8x = 3\n",
        "            crt_net['model.9.weight'] = state_dict['conv_up3.weight']\n",
        "            crt_net['model.9.bias'] = state_dict['conv_up3.bias']\n",
        "\n",
        "        crt_net[f'model.{8+re8x}.weight'] = state_dict['conv_hr.weight']\n",
        "        crt_net[f'model.{8+re8x}.bias'] = state_dict['conv_hr.bias']\n",
        "        crt_net[f'model.{10+re8x}.weight'] = state_dict['conv_last.weight']\n",
        "        crt_net[f'model.{10+re8x}.bias'] = state_dict['conv_last.bias']\n",
        "\n",
        "        state_dict = crt_net\n",
        "    return state_dict\n",
        "\n",
        "\n",
        "def infer_params(state_dict):\n",
        "    # this code is copied from https://github.com/victorca25/iNNfer\n",
        "    scale2x = 0\n",
        "    scalemin = 6\n",
        "    n_uplayer = 0\n",
        "    plus = False\n",
        "\n",
        "    for block in list(state_dict):\n",
        "        parts = block.split(\".\")\n",
        "        n_parts = len(parts)\n",
        "        if n_parts == 5 and parts[2] == \"sub\":\n",
        "            nb = int(parts[3])\n",
        "        elif n_parts == 3:\n",
        "            part_num = int(parts[1])\n",
        "            if (part_num > scalemin\n",
        "                and parts[0] == \"model\"\n",
        "                and parts[2] == \"weight\"):\n",
        "                scale2x += 1\n",
        "            if part_num > n_uplayer:\n",
        "                n_uplayer = part_num\n",
        "                out_nc = state_dict[block].shape[0]\n",
        "        if not plus and \"conv1x1\" in block:\n",
        "            plus = True\n",
        "\n",
        "    nf = state_dict[\"model.0.weight\"].shape[0]\n",
        "    in_nc = state_dict[\"model.0.weight\"].shape[1]\n",
        "    out_nc = out_nc\n",
        "    scale = 2 ** scale2x\n",
        "\n",
        "    return in_nc, out_nc, nf, nb, plus, scale\n",
        "\n",
        "\n",
        "class UpscalerESRGAN(Upscaler):\n",
        "    def __init__(self, dirname):\n",
        "        self.name = \"ESRGAN\"\n",
        "        self.model_url = \"https://github.com/cszn/KAIR/releases/download/v1.0/ESRGAN.pth\"\n",
        "        self.model_name = \"ESRGAN_4x\"\n",
        "        self.scalers = []\n",
        "        self.user_path = dirname\n",
        "        super().__init__()\n",
        "        model_paths = self.find_models(ext_filter=[\".pt\", \".pth\"])\n",
        "        scalers = []\n",
        "        if len(model_paths) == 0:\n",
        "            scaler_data = UpscalerData(self.model_name, self.model_url, self, 4)\n",
        "            scalers.append(scaler_data)\n",
        "        for file in model_paths:\n",
        "            if file.startswith(\"http\"):\n",
        "                name = self.model_name\n",
        "            else:\n",
        "                name = modelloader.friendly_name(file)\n",
        "\n",
        "            scaler_data = UpscalerData(name, file, self, 4)\n",
        "            self.scalers.append(scaler_data)\n",
        "\n",
        "    def do_upscale(self, img, selected_model):\n",
        "        try:\n",
        "            model = self.load_model(selected_model)\n",
        "        except Exception as e:\n",
        "            print(f\"Unable to load ESRGAN model {selected_model}: {e}\", file=sys.stderr)\n",
        "            return img\n",
        "        model.to(devicee) # â­\n",
        "        img = esrgan_upscale(model, img)\n",
        "        return img\n",
        "\n",
        "    def load_model(self, path: str):\n",
        "        if path.startswith(\"http\"):\n",
        "            # TODO: this doesn't use `path` at all?\n",
        "            filename = modelloader.load_file_from_url(\n",
        "                url=self.model_url,\n",
        "                model_dir=self.model_download_path,\n",
        "                file_name=f\"{self.model_name}.pth\",\n",
        "            )\n",
        "        else:\n",
        "            filename = path\n",
        "\n",
        "        state_dict = torch.load(filename, map_location='cpu' if 'pâ­s' == 'mps' else None)\n",
        "\n",
        "        if \"params_ema\" in state_dict:\n",
        "            state_dict = state_dict[\"params_ema\"]\n",
        "        elif \"params\" in state_dict:\n",
        "            state_dict = state_dict[\"params\"]\n",
        "            num_conv = 16 if \"realesr-animevideov3\" in filename else 32\n",
        "            model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=num_conv, upscale=4, act_type='prelu')\n",
        "            model.load_state_dict(state_dict)\n",
        "            model.eval()\n",
        "            return model\n",
        "\n",
        "        if \"body.0.rdb1.conv1.weight\" in state_dict and \"conv_first.weight\" in state_dict:\n",
        "            nb = 6 if \"RealESRGAN_x4plus_anime_6B\" in filename else 23\n",
        "            state_dict = resrgan2normal(state_dict, nb)\n",
        "        elif \"conv_first.weight\" in state_dict:\n",
        "            state_dict = mod2normal(state_dict)\n",
        "        elif \"model.0.weight\" not in state_dict:\n",
        "            raise Exception(\"The file is not a recognized ESRGAN model.\")\n",
        "\n",
        "        in_nc, out_nc, nf, nb, plus, mscale = infer_params(state_dict)\n",
        "\n",
        "        model = RRDBNet(in_nc=in_nc, out_nc=out_nc, nf=nf, nb=nb, upscale=mscale, plus=plus)\n",
        "        model.load_state_dict(state_dict)\n",
        "        model.eval()\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "def upscale_without_tiling(model, img):\n",
        "    img = np.array(img)\n",
        "    img = img[:, :, ::-1]\n",
        "    img = np.ascontiguousarray(np.transpose(img, (2, 0, 1))) / 255\n",
        "    img = torch.from_numpy(img).float()\n",
        "    img = img.unsqueeze(0).to(devicee) # â­\n",
        "    with torch.no_grad():\n",
        "        output = model(img)\n",
        "    output = output.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
        "    output = 255. * np.moveaxis(output, 0, 2)\n",
        "    output = output.astype(np.uint8)\n",
        "    output = output[:, :, ::-1]\n",
        "    return Image.fromarray(output, 'RGB')\n",
        "\n",
        "\n",
        "def esrgan_upscale(model, img):\n",
        "    if 'active default' == 0: # tilling â­\n",
        "        return upscale_without_tiling(model, img)\n",
        "    ESRGAN_tile = 100 #â­\n",
        "    ESRGAN_tile_overlap = 10 # â­\n",
        "\n",
        "    grid = split_grid(img, ESRGAN_tile, ESRGAN_tile, ESRGAN_tile_overlap)\n",
        "    newtiles = []\n",
        "    scale_factor = 4\n",
        "\n",
        "    for y, h, row in grid.tiles:\n",
        "        newrow = []\n",
        "        for tiledata in row:\n",
        "            x, w, tile = tiledata\n",
        "\n",
        "            output = upscale_without_tiling(model, tile)\n",
        "            scale_factor = output.width // tile.width\n",
        "\n",
        "            newrow.append([x * scale_factor, w * scale_factor, output])\n",
        "        newtiles.append([y * scale_factor, h * scale_factor, newrow])\n",
        "\n",
        "    newgrid = Grid(newtiles, grid.tile_w * scale_factor, grid.tile_h * scale_factor, grid.image_w * scale_factor, grid.image_h * scale_factor, grid.overlap * scale_factor)\n",
        "    output = combine_grid(newgrid)\n",
        "    return output\n",
        "# =====================================\n",
        "# Upscaler Utils\n",
        "# =====================================\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "Grid = namedtuple(\"Grid\", [\"tiles\", \"tile_w\", \"tile_h\", \"image_w\", \"image_h\", \"overlap\"])\n",
        "\n",
        "\n",
        "def split_grid(image, tile_w=512, tile_h=512, overlap=64):\n",
        "    w = image.width\n",
        "    h = image.height\n",
        "\n",
        "    non_overlap_width = tile_w - overlap\n",
        "    non_overlap_height = tile_h - overlap\n",
        "\n",
        "    cols = math.ceil((w - overlap) / non_overlap_width)\n",
        "    rows = math.ceil((h - overlap) / non_overlap_height)\n",
        "\n",
        "    dx = (w - tile_w) / (cols - 1) if cols > 1 else 0\n",
        "    dy = (h - tile_h) / (rows - 1) if rows > 1 else 0\n",
        "\n",
        "    grid = Grid([], tile_w, tile_h, w, h, overlap)\n",
        "    for row in range(rows):\n",
        "        row_images = []\n",
        "\n",
        "        y = int(row * dy)\n",
        "\n",
        "        if y + tile_h >= h:\n",
        "            y = h - tile_h\n",
        "\n",
        "        for col in range(cols):\n",
        "            x = int(col * dx)\n",
        "\n",
        "            if x + tile_w >= w:\n",
        "                x = w - tile_w\n",
        "\n",
        "            tile = image.crop((x, y, x + tile_w, y + tile_h))\n",
        "\n",
        "            row_images.append([x, tile_w, tile])\n",
        "\n",
        "        grid.tiles.append([y, tile_h, row_images])\n",
        "\n",
        "    return grid\n",
        "\n",
        "\n",
        "def combine_grid(grid):\n",
        "    def make_mask_image(r):\n",
        "        r = r * 255 / grid.overlap\n",
        "        r = r.astype(np.uint8)\n",
        "        return Image.fromarray(r, 'L')\n",
        "\n",
        "    mask_w = make_mask_image(np.arange(grid.overlap, dtype=np.float32).reshape((1, grid.overlap)).repeat(grid.tile_h, axis=0))\n",
        "    mask_h = make_mask_image(np.arange(grid.overlap, dtype=np.float32).reshape((grid.overlap, 1)).repeat(grid.image_w, axis=1))\n",
        "\n",
        "    combined_image = Image.new(\"RGB\", (grid.image_w, grid.image_h))\n",
        "    for y, h, row in grid.tiles:\n",
        "        combined_row = Image.new(\"RGB\", (grid.image_w, h))\n",
        "        for x, w, tile in row:\n",
        "            if x == 0:\n",
        "                combined_row.paste(tile, (0, 0))\n",
        "                continue\n",
        "\n",
        "            combined_row.paste(tile.crop((0, 0, grid.overlap, h)), (x, 0), mask=mask_w)\n",
        "            combined_row.paste(tile.crop((grid.overlap, 0, w, h)), (x + grid.overlap, 0))\n",
        "\n",
        "        if y == 0:\n",
        "            combined_image.paste(combined_row, (0, 0))\n",
        "            continue\n",
        "\n",
        "        combined_image.paste(combined_row.crop((0, 0, combined_row.width, grid.overlap)), (0, y), mask=mask_h)\n",
        "        combined_image.paste(combined_row.crop((0, grid.overlap, combined_row.width, h)), (0, y + grid.overlap))\n",
        "\n",
        "    return combined_image\n",
        "# =====================================\n",
        "# Upscaler loaders\n",
        "# =====================================\n",
        "import os\n",
        "import shutil\n",
        "import importlib\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "\n",
        "def load_file_from_url(\n",
        "    url: str,\n",
        "    *,\n",
        "    model_dir: str,\n",
        "    progress: bool = True,\n",
        "    file_name: str | None = None,\n",
        ") -> str:\n",
        "    \"\"\"Download a file from `url` into `model_dir`, using the file present if possible.\n",
        "\n",
        "    Returns the path to the downloaded file.\n",
        "    \"\"\"\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    if not file_name:\n",
        "        parts = urlparse(url)\n",
        "        file_name = os.path.basename(parts.path)\n",
        "    cached_file = os.path.abspath(os.path.join(model_dir, file_name))\n",
        "    if not os.path.exists(cached_file):\n",
        "        print(f'Downloading: \"{url}\" to {cached_file}\\n')\n",
        "        from torch.hub import download_url_to_file\n",
        "        download_url_to_file(url, cached_file, progress=progress)\n",
        "    return cached_file\n",
        "\n",
        "\n",
        "def load_models(model_path: str, model_url: str = None, command_path: str = None, ext_filter=None, download_name=None, ext_blacklist=None) -> list:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    output = []\n",
        "\n",
        "    try:\n",
        "        places = []\n",
        "\n",
        "        if command_path is not None and command_path != model_path:\n",
        "            pretrained_path = os.path.join(command_path, 'experiments/pretrained_models')\n",
        "            if os.path.exists(pretrained_path):\n",
        "                print(f\"Appending path: {pretrained_path}\")\n",
        "                places.append(pretrained_path)\n",
        "            elif os.path.exists(command_path):\n",
        "                places.append(command_path)\n",
        "\n",
        "        places.append(model_path)\n",
        "\n",
        "        for place in places:\n",
        "            for full_path in shared.walk_files(place, allowed_extensions=ext_filter):\n",
        "                if os.path.islink(full_path) and not os.path.exists(full_path):\n",
        "                    print(f\"Skipping broken symlink: {full_path}\")\n",
        "                    continue\n",
        "                if ext_blacklist is not None and any(full_path.endswith(x) for x in ext_blacklist):\n",
        "                    continue\n",
        "                if full_path not in output:\n",
        "                    output.append(full_path)\n",
        "\n",
        "        if model_url is not None and len(output) == 0:\n",
        "            if download_name is not None:\n",
        "                output.append(load_file_from_url(model_url, model_dir=places[0], file_name=download_name))\n",
        "            else:\n",
        "                output.append(model_url)\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def friendly_name(file: str):\n",
        "    if file.startswith(\"http\"):\n",
        "        file = urlparse(file).path\n",
        "\n",
        "    file = os.path.basename(file)\n",
        "    model_name, extension = os.path.splitext(file)\n",
        "    return model_name\n",
        "\n",
        "# =====================================\n",
        "# UpscalerESRGAN ARCH\n",
        "# =====================================\n",
        "\n",
        "# this file is adapted from https://github.com/victorca25/iNNfer\n",
        "\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "####################\n",
        "# RRDBNet Generator\n",
        "####################\n",
        "\n",
        "class RRDBNet(nn.Module):\n",
        "    def __init__(self, in_nc, out_nc, nf, nb, nr=3, gc=32, upscale=4, norm_type=None,\n",
        "            act_type='leakyrelu', mode='CNA', upsample_mode='upconv', convtype='Conv2D',\n",
        "            finalact=None, gaussian_noise=False, plus=False):\n",
        "        super(RRDBNet, self).__init__()\n",
        "        n_upscale = int(math.log(upscale, 2))\n",
        "        if upscale == 3:\n",
        "            n_upscale = 1\n",
        "\n",
        "        self.resrgan_scale = 0\n",
        "        if in_nc % 16 == 0:\n",
        "            self.resrgan_scale = 1\n",
        "        elif in_nc != 4 and in_nc % 4 == 0:\n",
        "            self.resrgan_scale = 2\n",
        "\n",
        "        fea_conv = conv_block(in_nc, nf, kernel_size=3, norm_type=None, act_type=None, convtype=convtype)\n",
        "        rb_blocks = [RRDB(nf, nr, kernel_size=3, gc=32, stride=1, bias=1, pad_type='zero',\n",
        "            norm_type=norm_type, act_type=act_type, mode='CNA', convtype=convtype,\n",
        "            gaussian_noise=gaussian_noise, plus=plus) for _ in range(nb)]\n",
        "        LR_conv = conv_block(nf, nf, kernel_size=3, norm_type=norm_type, act_type=None, mode=mode, convtype=convtype)\n",
        "\n",
        "        if upsample_mode == 'upconv':\n",
        "            upsample_block = upconv_block\n",
        "        elif upsample_mode == 'pixelshuffle':\n",
        "            upsample_block = pixelshuffle_block\n",
        "        else:\n",
        "            raise NotImplementedError(f'upsample mode [{upsample_mode}] is not found')\n",
        "        if upscale == 3:\n",
        "            upsampler = upsample_block(nf, nf, 3, act_type=act_type, convtype=convtype)\n",
        "        else:\n",
        "            upsampler = [upsample_block(nf, nf, act_type=act_type, convtype=convtype) for _ in range(n_upscale)]\n",
        "        HR_conv0 = conv_block(nf, nf, kernel_size=3, norm_type=None, act_type=act_type, convtype=convtype)\n",
        "        HR_conv1 = conv_block(nf, out_nc, kernel_size=3, norm_type=None, act_type=None, convtype=convtype)\n",
        "\n",
        "        outact = act(finalact) if finalact else None\n",
        "\n",
        "        self.model = sequential(fea_conv, ShortcutBlock(sequential(*rb_blocks, LR_conv)),\n",
        "            *upsampler, HR_conv0, HR_conv1, outact)\n",
        "\n",
        "    def forward(self, x, outm=None):\n",
        "        if self.resrgan_scale == 1:\n",
        "            feat = pixel_unshuffle(x, scale=4)\n",
        "        elif self.resrgan_scale == 2:\n",
        "            feat = pixel_unshuffle(x, scale=2)\n",
        "        else:\n",
        "            feat = x\n",
        "\n",
        "        return self.model(feat)\n",
        "\n",
        "\n",
        "class RRDB(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual in Residual Dense Block\n",
        "    (ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nf, nr=3, kernel_size=3, gc=32, stride=1, bias=1, pad_type='zero',\n",
        "            norm_type=None, act_type='leakyrelu', mode='CNA', convtype='Conv2D',\n",
        "            spectral_norm=False, gaussian_noise=False, plus=False):\n",
        "        super(RRDB, self).__init__()\n",
        "        # This is for backwards compatibility with existing models\n",
        "        if nr == 3:\n",
        "            self.RDB1 = ResidualDenseBlock_5C(nf, kernel_size, gc, stride, bias, pad_type,\n",
        "                    norm_type, act_type, mode, convtype, spectral_norm=spectral_norm,\n",
        "                    gaussian_noise=gaussian_noise, plus=plus)\n",
        "            self.RDB2 = ResidualDenseBlock_5C(nf, kernel_size, gc, stride, bias, pad_type,\n",
        "                    norm_type, act_type, mode, convtype, spectral_norm=spectral_norm,\n",
        "                    gaussian_noise=gaussian_noise, plus=plus)\n",
        "            self.RDB3 = ResidualDenseBlock_5C(nf, kernel_size, gc, stride, bias, pad_type,\n",
        "                    norm_type, act_type, mode, convtype, spectral_norm=spectral_norm,\n",
        "                    gaussian_noise=gaussian_noise, plus=plus)\n",
        "        else:\n",
        "            RDB_list = [ResidualDenseBlock_5C(nf, kernel_size, gc, stride, bias, pad_type,\n",
        "                                              norm_type, act_type, mode, convtype, spectral_norm=spectral_norm,\n",
        "                                              gaussian_noise=gaussian_noise, plus=plus) for _ in range(nr)]\n",
        "            self.RDBs = nn.Sequential(*RDB_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if hasattr(self, 'RDB1'):\n",
        "            out = self.RDB1(x)\n",
        "            out = self.RDB2(out)\n",
        "            out = self.RDB3(out)\n",
        "        else:\n",
        "            out = self.RDBs(x)\n",
        "        return out * 0.2 + x\n",
        "\n",
        "\n",
        "class ResidualDenseBlock_5C(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual Dense Block\n",
        "    The core module of paper: (Residual Dense Network for Image Super-Resolution, CVPR 18)\n",
        "    Modified options that can be used:\n",
        "        - \"Partial Convolution based Padding\" arXiv:1811.11718\n",
        "        - \"Spectral normalization\" arXiv:1802.05957\n",
        "        - \"ICASSP 2020 - ESRGAN+ : Further Improving ESRGAN\" N. C.\n",
        "            {Rakotonirina} and A. {Rasoanaivo}\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nf=64, kernel_size=3, gc=32, stride=1, bias=1, pad_type='zero',\n",
        "            norm_type=None, act_type='leakyrelu', mode='CNA', convtype='Conv2D',\n",
        "            spectral_norm=False, gaussian_noise=False, plus=False):\n",
        "        super(ResidualDenseBlock_5C, self).__init__()\n",
        "\n",
        "        self.noise = GaussianNoise() if gaussian_noise else None\n",
        "        self.conv1x1 = conv1x1(nf, gc) if plus else None\n",
        "\n",
        "        self.conv1 = conv_block(nf, gc, kernel_size, stride, bias=bias, pad_type=pad_type,\n",
        "            norm_type=norm_type, act_type=act_type, mode=mode, convtype=convtype,\n",
        "            spectral_norm=spectral_norm)\n",
        "        self.conv2 = conv_block(nf+gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type,\n",
        "            norm_type=norm_type, act_type=act_type, mode=mode, convtype=convtype,\n",
        "            spectral_norm=spectral_norm)\n",
        "        self.conv3 = conv_block(nf+2*gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type,\n",
        "            norm_type=norm_type, act_type=act_type, mode=mode, convtype=convtype,\n",
        "            spectral_norm=spectral_norm)\n",
        "        self.conv4 = conv_block(nf+3*gc, gc, kernel_size, stride, bias=bias, pad_type=pad_type,\n",
        "            norm_type=norm_type, act_type=act_type, mode=mode, convtype=convtype,\n",
        "            spectral_norm=spectral_norm)\n",
        "        if mode == 'CNA':\n",
        "            last_act = None\n",
        "        else:\n",
        "            last_act = act_type\n",
        "        self.conv5 = conv_block(nf+4*gc, nf, 3, stride, bias=bias, pad_type=pad_type,\n",
        "            norm_type=norm_type, act_type=last_act, mode=mode, convtype=convtype,\n",
        "            spectral_norm=spectral_norm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.conv2(torch.cat((x, x1), 1))\n",
        "        if self.conv1x1:\n",
        "            x2 = x2 + self.conv1x1(x)\n",
        "        x3 = self.conv3(torch.cat((x, x1, x2), 1))\n",
        "        x4 = self.conv4(torch.cat((x, x1, x2, x3), 1))\n",
        "        if self.conv1x1:\n",
        "            x4 = x4 + x2\n",
        "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
        "        if self.noise:\n",
        "            return self.noise(x5.mul(0.2) + x)\n",
        "        else:\n",
        "            return x5 * 0.2 + x\n",
        "\n",
        "\n",
        "####################\n",
        "# ESRGANplus\n",
        "####################\n",
        "\n",
        "class GaussianNoise(nn.Module):\n",
        "    def __init__(self, sigma=0.1, is_relative_detach=False):\n",
        "        super().__init__()\n",
        "        self.sigma = sigma\n",
        "        self.is_relative_detach = is_relative_detach\n",
        "        self.noise = torch.tensor(0, dtype=torch.float)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training and self.sigma != 0:\n",
        "            self.noise = self.noise.to(x.device)\n",
        "            scale = self.sigma * x.detach() if self.is_relative_detach else self.sigma * x\n",
        "            sampled_noise = self.noise.repeat(*x.size()).normal_() * scale\n",
        "            x = x + sampled_noise\n",
        "        return x\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "####################\n",
        "# SRVGGNetCompact\n",
        "####################\n",
        "\n",
        "class SRVGGNetCompact(nn.Module):\n",
        "    \"\"\"A compact VGG-style network structure for super-resolution.\n",
        "    This class is copied from https://github.com/xinntao/Real-ESRGAN\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=16, upscale=4, act_type='prelu'):\n",
        "        super(SRVGGNetCompact, self).__init__()\n",
        "        self.num_in_ch = num_in_ch\n",
        "        self.num_out_ch = num_out_ch\n",
        "        self.num_feat = num_feat\n",
        "        self.num_conv = num_conv\n",
        "        self.upscale = upscale\n",
        "        self.act_type = act_type\n",
        "\n",
        "        self.body = nn.ModuleList()\n",
        "        # the first conv\n",
        "        self.body.append(nn.Conv2d(num_in_ch, num_feat, 3, 1, 1))\n",
        "        # the first activation\n",
        "        if act_type == 'relu':\n",
        "            activation = nn.ReLU(inplace=True)\n",
        "        elif act_type == 'prelu':\n",
        "            activation = nn.PReLU(num_parameters=num_feat)\n",
        "        elif act_type == 'leakyrelu':\n",
        "            activation = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "        self.body.append(activation)\n",
        "\n",
        "        # the body structure\n",
        "        for _ in range(num_conv):\n",
        "            self.body.append(nn.Conv2d(num_feat, num_feat, 3, 1, 1))\n",
        "            # activation\n",
        "            if act_type == 'relu':\n",
        "                activation = nn.ReLU(inplace=True)\n",
        "            elif act_type == 'prelu':\n",
        "                activation = nn.PReLU(num_parameters=num_feat)\n",
        "            elif act_type == 'leakyrelu':\n",
        "                activation = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "            self.body.append(activation)\n",
        "\n",
        "        # the last conv\n",
        "        self.body.append(nn.Conv2d(num_feat, num_out_ch * upscale * upscale, 3, 1, 1))\n",
        "        # upsample\n",
        "        self.upsampler = nn.PixelShuffle(upscale)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for i in range(0, len(self.body)):\n",
        "            out = self.body[i](out)\n",
        "\n",
        "        out = self.upsampler(out)\n",
        "        # add the nearest upsampled image, so that the network learns the residual\n",
        "        base = F.interpolate(x, scale_factor=self.upscale, mode='nearest')\n",
        "        out += base\n",
        "        return out\n",
        "\n",
        "\n",
        "####################\n",
        "# Upsampler\n",
        "####################\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    r\"\"\"Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.\n",
        "    The input data is assumed to be of the form\n",
        "    `minibatch x channels x [optional depth] x [optional height] x width`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n",
        "        super(Upsample, self).__init__()\n",
        "        if isinstance(scale_factor, tuple):\n",
        "            self.scale_factor = tuple(float(factor) for factor in scale_factor)\n",
        "        else:\n",
        "            self.scale_factor = float(scale_factor) if scale_factor else None\n",
        "        self.mode = mode\n",
        "        self.size = size\n",
        "        self.align_corners = align_corners\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.functional.interpolate(x, size=self.size, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        if self.scale_factor is not None:\n",
        "            info = f'scale_factor={self.scale_factor}'\n",
        "        else:\n",
        "            info = f'size={self.size}'\n",
        "        info += f', mode={self.mode}'\n",
        "        return info\n",
        "\n",
        "\n",
        "def pixel_unshuffle(x, scale):\n",
        "    \"\"\" Pixel unshuffle.\n",
        "    Args:\n",
        "        x (Tensor): Input feature with shape (b, c, hh, hw).\n",
        "        scale (int): Downsample ratio.\n",
        "    Returns:\n",
        "        Tensor: the pixel unshuffled feature.\n",
        "    \"\"\"\n",
        "    b, c, hh, hw = x.size()\n",
        "    out_channel = c * (scale**2)\n",
        "    assert hh % scale == 0 and hw % scale == 0\n",
        "    h = hh // scale\n",
        "    w = hw // scale\n",
        "    x_view = x.view(b, c, h, scale, w, scale)\n",
        "    return x_view.permute(0, 1, 3, 5, 2, 4).reshape(b, out_channel, h, w)\n",
        "\n",
        "\n",
        "def pixelshuffle_block(in_nc, out_nc, upscale_factor=2, kernel_size=3, stride=1, bias=True,\n",
        "                        pad_type='zero', norm_type=None, act_type='relu', convtype='Conv2D'):\n",
        "    \"\"\"\n",
        "    Pixel shuffle layer\n",
        "    (Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional\n",
        "    Neural Network, CVPR17)\n",
        "    \"\"\"\n",
        "    conv = conv_block(in_nc, out_nc * (upscale_factor ** 2), kernel_size, stride, bias=bias,\n",
        "                        pad_type=pad_type, norm_type=None, act_type=None, convtype=convtype)\n",
        "    pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
        "\n",
        "    n = norm(norm_type, out_nc) if norm_type else None\n",
        "    a = act(act_type) if act_type else None\n",
        "    return sequential(conv, pixel_shuffle, n, a)\n",
        "\n",
        "\n",
        "def upconv_block(in_nc, out_nc, upscale_factor=2, kernel_size=3, stride=1, bias=True,\n",
        "                pad_type='zero', norm_type=None, act_type='relu', mode='nearest', convtype='Conv2D'):\n",
        "    \"\"\" Upconv layer \"\"\"\n",
        "    upscale_factor = (1, upscale_factor, upscale_factor) if convtype == 'Conv3D' else upscale_factor\n",
        "    upsample = Upsample(scale_factor=upscale_factor, mode=mode)\n",
        "    conv = conv_block(in_nc, out_nc, kernel_size, stride, bias=bias,\n",
        "                        pad_type=pad_type, norm_type=norm_type, act_type=act_type, convtype=convtype)\n",
        "    return sequential(upsample, conv)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####################\n",
        "# Basic blocks\n",
        "####################\n",
        "\n",
        "\n",
        "def make_layer(basic_block, num_basic_block, **kwarg):\n",
        "    \"\"\"Make layers by stacking the same blocks.\n",
        "    Args:\n",
        "        basic_block (nn.module): nn.module class for basic block. (block)\n",
        "        num_basic_block (int): number of blocks. (n_layers)\n",
        "    Returns:\n",
        "        nn.Sequential: Stacked blocks in nn.Sequential.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    for _ in range(num_basic_block):\n",
        "        layers.append(basic_block(**kwarg))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def act(act_type, inplace=True, neg_slope=0.2, n_prelu=1, beta=1.0):\n",
        "    \"\"\" activation helper \"\"\"\n",
        "    act_type = act_type.lower()\n",
        "    if act_type == 'relu':\n",
        "        layer = nn.ReLU(inplace)\n",
        "    elif act_type in ('leakyrelu', 'lrelu'):\n",
        "        layer = nn.LeakyReLU(neg_slope, inplace)\n",
        "    elif act_type == 'prelu':\n",
        "        layer = nn.PReLU(num_parameters=n_prelu, init=neg_slope)\n",
        "    elif act_type == 'tanh':  # [-1, 1] range output\n",
        "        layer = nn.Tanh()\n",
        "    elif act_type == 'sigmoid':  # [0, 1] range output\n",
        "        layer = nn.Sigmoid()\n",
        "    else:\n",
        "        raise NotImplementedError(f'activation layer [{act_type}] is not found')\n",
        "    return layer\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self, *kwargs):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x, *kwargs):\n",
        "        return x\n",
        "\n",
        "\n",
        "def norm(norm_type, nc):\n",
        "    \"\"\" Return a normalization layer \"\"\"\n",
        "    norm_type = norm_type.lower()\n",
        "    if norm_type == 'batch':\n",
        "        layer = nn.BatchNorm2d(nc, affine=True)\n",
        "    elif norm_type == 'instance':\n",
        "        layer = nn.InstanceNorm2d(nc, affine=False)\n",
        "    elif norm_type == 'none':\n",
        "        def norm_layer(x): return Identity()\n",
        "    else:\n",
        "        raise NotImplementedError(f'normalization layer [{norm_type}] is not found')\n",
        "    return layer\n",
        "\n",
        "\n",
        "def pad(pad_type, padding):\n",
        "    \"\"\" padding layer helper \"\"\"\n",
        "    pad_type = pad_type.lower()\n",
        "    if padding == 0:\n",
        "        return None\n",
        "    if pad_type == 'reflect':\n",
        "        layer = nn.ReflectionPad2d(padding)\n",
        "    elif pad_type == 'replicate':\n",
        "        layer = nn.ReplicationPad2d(padding)\n",
        "    elif pad_type == 'zero':\n",
        "        layer = nn.ZeroPad2d(padding)\n",
        "    else:\n",
        "        raise NotImplementedError(f'padding layer [{pad_type}] is not implemented')\n",
        "    return layer\n",
        "\n",
        "\n",
        "def get_valid_padding(kernel_size, dilation):\n",
        "    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n",
        "    padding = (kernel_size - 1) // 2\n",
        "    return padding\n",
        "\n",
        "\n",
        "class ShortcutBlock(nn.Module):\n",
        "    \"\"\" Elementwise sum the output of a submodule to its input \"\"\"\n",
        "    def __init__(self, submodule):\n",
        "        super(ShortcutBlock, self).__init__()\n",
        "        self.sub = submodule\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x + self.sub(x)\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'Identity + \\n|' + self.sub.__repr__().replace('\\n', '\\n|')\n",
        "\n",
        "\n",
        "def sequential(*args):\n",
        "    \"\"\" Flatten Sequential. It unwraps nn.Sequential. \"\"\"\n",
        "    if len(args) == 1:\n",
        "        if isinstance(args[0], OrderedDict):\n",
        "            raise NotImplementedError('sequential does not support OrderedDict input.')\n",
        "        return args[0]  # No sequential is needed.\n",
        "    modules = []\n",
        "    for module in args:\n",
        "        if isinstance(module, nn.Sequential):\n",
        "            for submodule in module.children():\n",
        "                modules.append(submodule)\n",
        "        elif isinstance(module, nn.Module):\n",
        "            modules.append(module)\n",
        "    return nn.Sequential(*modules)\n",
        "\n",
        "\n",
        "def conv_block(in_nc, out_nc, kernel_size, stride=1, dilation=1, groups=1, bias=True,\n",
        "               pad_type='zero', norm_type=None, act_type='relu', mode='CNA', convtype='Conv2D',\n",
        "               spectral_norm=False):\n",
        "    \"\"\" Conv layer with padding, normalization, activation \"\"\"\n",
        "    assert mode in ['CNA', 'NAC', 'CNAC'], f'Wrong conv mode [{mode}]'\n",
        "    padding = get_valid_padding(kernel_size, dilation)\n",
        "    p = pad(pad_type, padding) if pad_type and pad_type != 'zero' else None\n",
        "    padding = padding if pad_type == 'zero' else 0\n",
        "\n",
        "    if convtype=='PartialConv2D':\n",
        "        from torchvision.ops import PartialConv2d  # this is definitely not going to work, but PartialConv2d doesn't work anyway and this shuts up static analyzer\n",
        "        c = PartialConv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "               dilation=dilation, bias=bias, groups=groups)\n",
        "    elif convtype=='DeformConv2D':\n",
        "        from torchvision.ops import DeformConv2d  # not tested\n",
        "        c = DeformConv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "               dilation=dilation, bias=bias, groups=groups)\n",
        "    elif convtype=='Conv3D':\n",
        "        c = nn.Conv3d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                dilation=dilation, bias=bias, groups=groups)\n",
        "    else:\n",
        "        c = nn.Conv2d(in_nc, out_nc, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                dilation=dilation, bias=bias, groups=groups)\n",
        "\n",
        "    if spectral_norm:\n",
        "        c = nn.utils.spectral_norm(c)\n",
        "\n",
        "    a = act(act_type) if act_type else None\n",
        "    if 'CNA' in mode:\n",
        "        n = norm(norm_type, out_nc) if norm_type else None\n",
        "        return sequential(p, c, n, a)\n",
        "    elif mode == 'NAC':\n",
        "        if norm_type is None and act_type is not None:\n",
        "            a = act(act_type, inplace=False)\n",
        "        n = norm(norm_type, in_nc) if norm_type else None\n",
        "        return sequential(n, a, p, c)"
      ],
      "metadata": {
        "id": "da0BxuMj2gBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3. ðŸ‘‡ Generating Images { form-width: \"20%\", display-mode: \"form\" }\n",
        "#@markdown ---\n",
        "#@markdown - **Prompt** - Tell the model what you want to see.\n",
        "#@markdown - **Negative Prompt** - Tell the model what you don't want to see.\n",
        "#@markdown - **Steps** - How long the model should work on the image.\n",
        "#@markdown - **CFG** - Controls how much the image generation process follows the text prompt.  Guidance scale ranging from 0 to 20. Lower values allow the AI to be more creative and less strict at following the prompt. Default is 7.5\n",
        "#@markdown - **Sampler** - Progressively reduce image noise through denoising steps.\n",
        "#@markdown - **Seed** -  A number that helps the model start generating the image. Set `-1` for using random seed values.\n",
        "#@markdown - **Clip Skip** - It allows to control the level of detail and accuracy in the generated images by skipping certain layers of the CLIP model during the image generation process.\n",
        "#@markdown - **Prompt weights** -  Helps the model focus on different parts of the prompt. Prompt weights can be used to emphasize or de-emphasize certain aspects of the image, such as the object, the scene, or the style. Currently, the [Compel syntax](https://github.com/damian0815/compel/blob/main/doc/syntax.md) is being used. You can also activate the `Convert Prompt weights` to automatically convert syntax from `(word:1.1)` to `(word)1.1` or `(word)` to `(word)+` to make them compatible with Compel weights. Compel scale more with its values so that fewer weights are needed for good results\n",
        "#@markdown - **FreeU** - Is a method that substantially improves diffusion model sample quality at no costs.\n",
        "#@markdown - **ControlNet** - Enhances text-to-image diffusion models by allowing various spatial contexts to serve as additional conditioning, enabling the generation of more controlled and context-aware images.\n",
        "#@markdown ---\n",
        "%cd /content\n",
        "import ipywidgets as widgets, mediapy, random\n",
        "from diffusers.models.attention_processor import AttnProcessor2_0\n",
        "from PIL import Image\n",
        "import IPython.display\n",
        "import time\n",
        "from IPython.utils import capture\n",
        "import logging\n",
        "from ipywidgets import Button, Layout, jslink, IntText, IntSlider, BoundedFloatText\n",
        "logging.getLogger(\"diffusers\").setLevel(logging.ERROR)\n",
        "\n",
        "#from IPython.display import display\n",
        "from ipywidgets import TwoByTwoLayout, Layout, Button, Box, FloatText, Textarea, Dropdown, Label, IntSlider, interactive, HBox, VBox, BoundedIntText, BoundedFloatText\n",
        "\n",
        "#PARAMETER WIDGETS\n",
        "width = \"225px\"\n",
        "auto_layout = widgets.Layout(height='auto', width='auto')\n",
        "lora_layout = {'width':'165px'}\n",
        "lora_scale_layout = {'width':'50px'}\n",
        "style = {'description_width': 'initial'} # show full text\n",
        "\n",
        "# =====================================\n",
        "# Left\n",
        "# =====================================\n",
        "\n",
        "num_images = widgets.BoundedIntText(\n",
        "    value = 1,\n",
        "    min = 1,\n",
        "    description=\"Images:\",\n",
        "    layout=widgets.Layout(width=width)\n",
        ")\n",
        "\n",
        "steps = widgets.BoundedIntText(\n",
        "    value = 30,\n",
        "    min = 1,\n",
        "    max = 100,\n",
        "    description=\"Steps:\",\n",
        "    layout=widgets.Layout(width=width)\n",
        ")\n",
        "\n",
        "CFG = widgets.BoundedFloatText(\n",
        "    value = 7.5,\n",
        "    min = 0,\n",
        "    step=0.5,\n",
        "    description=\"CFG:\",\n",
        "    layout=widgets.Layout(width=width)\n",
        ")\n",
        "\n",
        "select_sampler = widgets.Dropdown(\n",
        "    options=[\n",
        "        \"DPM++ 2M\",\n",
        "        \"DPM++ 2M Karras\",\n",
        "        \"DPM++ 2M SDE\",\n",
        "        \"DPM++ 2M SDE Karras\",\n",
        "        \"DPM++ SDE\",\n",
        "        \"DPM++ SDE Karras\",\n",
        "        \"DPM2\",\n",
        "        \"DPM2 Karras\",\n",
        "        \"Euler\",\n",
        "        \"Euler a\",\n",
        "        \"Heun\",\n",
        "        \"LMS\",\n",
        "        \"LMS Karras\",\n",
        "        \"DDIMScheduler\",\n",
        "        \"DEISMultistepScheduler\",\n",
        "        \"UniPCMultistepScheduler\",\n",
        "    ],\n",
        "    description=\"Sampler:\",\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "img_height = widgets.BoundedIntText(\n",
        "    min=64,\n",
        "    max=4096,\n",
        "    step=8,\n",
        "    value=512,\n",
        "    description=\"Height:\",\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "img_width = widgets.BoundedIntText(\n",
        "    min=64,\n",
        "    max=4096,\n",
        "    step=8,\n",
        "    value=512,\n",
        "    description=\"Width:\",\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "random_seed = widgets.IntText(\n",
        "    value=-1,\n",
        "    description=\"Seed:\",\n",
        "    layout=widgets.Layout(width=width),\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "#lora1\n",
        "select_lora1 = widgets.Dropdown(\n",
        "    options=lora_model_list,\n",
        "    description=\"Lora1:\",\n",
        "    layout=lora_layout\n",
        ")\n",
        "\n",
        "lora_weights_scale1 = widgets.FloatText(\n",
        "    min=-2.0,\n",
        "    max=2.0,\n",
        "    step=0.01,\n",
        "    value=1,\n",
        "    #description=\"Lora scale1:\",\n",
        "    layout=lora_scale_layout\n",
        ")\n",
        "#lora2\n",
        "select_lora2 = widgets.Dropdown(\n",
        "    options=lora_model_list,\n",
        "    description=\"Lora2:\",\n",
        "    layout=lora_layout\n",
        ")\n",
        "\n",
        "lora_weights_scale2 = widgets.FloatText(\n",
        "    min=-2.0,\n",
        "    max=2.0,\n",
        "    step=0.01,\n",
        "    value=1,\n",
        "    #description=\"Lora scale2:\",\n",
        "    layout=lora_scale_layout\n",
        ")\n",
        "#lora3\n",
        "select_lora3 = widgets.Dropdown(\n",
        "    options=lora_model_list,\n",
        "    description=\"Lora3:\",\n",
        "    layout=lora_layout\n",
        ")\n",
        "\n",
        "lora_weights_scale3 = widgets.FloatText(\n",
        "    min=-2.0,\n",
        "    max=2.0,\n",
        "    step=0.01,\n",
        "    value=1,\n",
        "    #description=\"Lora scale3:\",\n",
        "    layout=lora_scale_layout\n",
        ")\n",
        "#lora4\n",
        "select_lora4 = widgets.Dropdown(\n",
        "    options=lora_model_list,\n",
        "    description=\"Lora4:\",\n",
        "    layout=lora_layout\n",
        ")\n",
        "\n",
        "lora_weights_scale4 = widgets.FloatText(\n",
        "    min=-2.0,\n",
        "    max=2.0,\n",
        "    step=0.01,\n",
        "    value=1,\n",
        "    #description=\"Lora scale4:\",\n",
        "    layout=lora_scale_layout\n",
        ")\n",
        "#lora5\n",
        "select_lora5 = widgets.Dropdown(\n",
        "    options=lora_model_list,\n",
        "    description=\"Lora5:\",\n",
        "    layout=lora_layout\n",
        ")\n",
        "\n",
        "lora_weights_scale5 = widgets.FloatText(\n",
        "    min=-2.0,\n",
        "    max=2.0,\n",
        "    step=0.01,\n",
        "    value=1,\n",
        "    #description=\"Lora scale5:\",\n",
        "    layout=lora_scale_layout\n",
        ")\n",
        "\n",
        "select_clip_skip = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Layer 2 Clip Skip',\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "freeu_check = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='FreeU for txt2img',\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "# =====================================\n",
        "# Center\n",
        "# =====================================\n",
        "\n",
        "display_imgs = widgets.Output() # layout={'border': '1px solid black'}\n",
        "\n",
        "select_model = widgets.Dropdown(\n",
        "    options=model_list,\n",
        "    description=\"Model:\",\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "vae_model_dropdown = widgets.Dropdown(\n",
        "    options=vae_model_list,\n",
        "    description=\"VAE:\",\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "prompt = widgets.Textarea(\n",
        "    value=\"\",\n",
        "    placeholder=\"Enter prompt\",\n",
        "    rows=5,\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "neg_prompt = widgets.Textarea(\n",
        "    value=\"\",\n",
        "    placeholder=\"Enter negative prompt\",\n",
        "    rows=5,\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "active_ti = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Active Textual Inversion in prompt (Experimental)',\n",
        "    #style = style,\n",
        "    layout=auto_layout,\n",
        ")\n",
        "# alternative prompt weights\n",
        "weights_prompt = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Convert Prompt weights',\n",
        "    #style = style,\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "generate = widgets.Button(\n",
        "    description=\"Generate\",\n",
        "    disabled=False,\n",
        "    button_style=\"primary\",\n",
        "    layout=Layout(height='auto', width='auto'),\n",
        ")\n",
        "\n",
        "### GENERATE ###\n",
        "\n",
        "def generate_img(i):\n",
        "  global model\n",
        "  #Clear output\n",
        "  display_imgs.clear_output()\n",
        "  generate.disabled = True\n",
        "\n",
        "  with display_imgs:\n",
        "\n",
        "    print(\"Running...\")\n",
        "\n",
        "\n",
        "    # First load\n",
        "    try:\n",
        "        model\n",
        "    except:\n",
        "        model = Model(base_model_id=select_model.value, task_name=select_task.value, vae_model = vae_model_dropdown.value, type_model_precision = model_precision.value)\n",
        "\n",
        "    model.load_pipe(select_model.value, task_name=select_task.value, vae_model = vae_model_dropdown.value, type_model_precision = model_precision.value)\n",
        "\n",
        "    display_imgs.clear_output()\n",
        "\n",
        "    try:\n",
        "      preprocessor_name_found = int_inputs[select_task.value][0].value\n",
        "    except:\n",
        "      preprocessor_name_found = None\n",
        "\n",
        "    global destination_path_cn_img, mask_control, image_list\n",
        "\n",
        "    image_control_base = None\n",
        "    if select_task.value != \"txt2img\":\n",
        "        try:\n",
        "            image_control_base = destination_path_cn_img\n",
        "        except:\n",
        "            print(\"No control image found\")\n",
        "            generate.disabled = False\n",
        "            return\n",
        "\n",
        "    mask_control_base = None\n",
        "    if select_task.value == \"Inpaint\":\n",
        "        if os.path.exists(int_inputs['Inpaint'][1].value):\n",
        "            mask_control_base = int_inputs['Inpaint'][1].value\n",
        "        else:\n",
        "            try:\n",
        "                mask_control_base = mask_control\n",
        "            except:\n",
        "                print(\"No mask image found\")\n",
        "                generate.disabled = False\n",
        "                return\n",
        "\n",
        "    pipe_params = {\n",
        "    \"prompt\": prompt.value,\n",
        "    \"negative_prompt\": neg_prompt.value,\n",
        "    \"img_height\": img_height.value,\n",
        "    \"img_width\": img_width.value,\n",
        "    \"num_images\": num_images.value,\n",
        "    \"num_steps\": steps.value,\n",
        "    \"guidance_scale\": CFG.value,\n",
        "    \"clip_skip\": select_clip_skip.value,\n",
        "    \"seed\": random_seed.value,\n",
        "    \"image\": image_control_base,\n",
        "    \"preprocessor_name\": preprocessor_name_found,\n",
        "    \"preprocess_resolution\": preprocess_resolution_global.value,\n",
        "    \"image_resolution\": image_resolution_global.value,\n",
        "    \"additional_prompt\": \"\",\n",
        "    \"image_mask\": mask_control_base, # only for Inpaint\n",
        "    \"strength\": int_inputs['Inpaint'][0].value, # only for Inpaint\n",
        "    \"low_threshold\": int_inputs['Canny'][0].value,\n",
        "    \"high_threshold\": int_inputs['Canny'][1].value,\n",
        "    \"value_threshold\": int_inputs['MLSD'][0].value,\n",
        "    \"distance_threshold\": int_inputs['MLSD'][1].value,\n",
        "    \"lora_A\": select_lora1.value,\n",
        "    \"lora_scale_A\": lora_weights_scale1.value,\n",
        "    \"lora_B\": select_lora2.value,\n",
        "    \"lora_scale_B\": lora_weights_scale2.value,\n",
        "    \"lora_C\": select_lora3.value,\n",
        "    \"lora_scale_C\": lora_weights_scale3.value,\n",
        "    \"lora_D\": select_lora4.value,\n",
        "    \"lora_scale_D\": lora_weights_scale4.value,\n",
        "    \"lora_E\": select_lora5.value,\n",
        "    \"lora_scale_E\": lora_weights_scale5.value,\n",
        "    \"active_textual_inversion\": active_ti.value,\n",
        "    \"textual_inversion\": embed_list,\n",
        "    \"convert_weights_prompt\": weights_prompt.value,\n",
        "    \"sampler\": select_sampler.value,\n",
        "    \"xformers_memory_efficient_attention\": xformers_memory_efficient_attention.value,\n",
        "    \"gui_active\": True,\n",
        "    \"loop_generation\": loop_generator.value,\n",
        "    \"controlnet_conditioning_scale\" : controlnet_output_scaling_in_unet.value,\n",
        "    \"control_guidance_start\" : controlnet_start_threshold.value,\n",
        "    \"control_guidance_end\" : controlnet_stop_threshold.value,\n",
        "    \"generator_in_cpu\" : init_generator_in_cpu.value,\n",
        "    \"FreeU\" : freeu_check.value,\n",
        "    }\n",
        "\n",
        "    images, image_list = model(**pipe_params)\n",
        "\n",
        "    if loop_generator.value == 1:\n",
        "        mediapy.show_images(images)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "  generate.disabled = False\n",
        "  return\n",
        "\n",
        "generate.on_click(generate_img)\n",
        "\n",
        "show_textual_inversion = widgets.Button(\n",
        "    description=\"List available textual inversions\",\n",
        "    disabled=False,\n",
        "    button_style=\"info\",\n",
        "    layout=widgets.Layout(height='auto', width='auto'),\n",
        ")\n",
        "\n",
        "def elemets_textual_inversion(value):\n",
        "  with display_imgs:\n",
        "    print('The embeddings currently supported. Write in the prompt the word for use')\n",
        "    for name, directory_name in embed_list:\n",
        "        print(name)\n",
        "    return\n",
        "\n",
        "show_textual_inversion.on_click(elemets_textual_inversion)\n",
        "\n",
        "clear_outputs = widgets.Button(\n",
        "    description=\"Clear outputs\",\n",
        "    disabled=False,\n",
        "    button_style=\"info\",\n",
        "    layout=widgets.Layout(height='auto', width='auto'),\n",
        "\n",
        ")\n",
        "\n",
        "def clear_outputs_run(value):\n",
        "  display_imgs.clear_output()\n",
        "  return\n",
        "\n",
        "clear_outputs.on_click(clear_outputs_run)\n",
        "\n",
        "# =====================================\n",
        "# Right ControlNet\n",
        "# =====================================\n",
        "\n",
        "control_model_list = list(CONTROLNET_MODEL_IDS.keys())\n",
        "\n",
        "# Create a Dropdown for selecting options\n",
        "select_task = widgets.Dropdown( #\n",
        "    options=[\n",
        "        control_model_list[13],\n",
        "        control_model_list[12],\n",
        "        control_model_list[0],\n",
        "        control_model_list[1],\n",
        "        control_model_list[2],\n",
        "        control_model_list[3],\n",
        "        control_model_list[4],\n",
        "        control_model_list[5],\n",
        "        control_model_list[6],\n",
        "        control_model_list[7],\n",
        "        control_model_list[8],\n",
        "        control_model_list[10],\n",
        "        control_model_list[11],\n",
        "    ],\n",
        "    description='TASK:',\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "controlnet_output_scaling_in_unet = widgets.FloatText(value=1.0, min=0.0, max=5.0, step=0.1, description='ControlNet Output Scaling in UNet:', style=style)\n",
        "controlnet_start_threshold = widgets.FloatSlider(value=0.0, min=0.00, max=1.0, step=0.01, description='ControlNet Start Threshold (%):', style=style)\n",
        "controlnet_stop_threshold = widgets.FloatSlider(value=1.0, min=0.00, max=1.0, step=0.01, description='ControlNet Stop Threshold (%):', style=style)\n",
        "\n",
        "preprocess_resolution_global = widgets.IntSlider(\n",
        "    value=512,\n",
        "    min=64,\n",
        "    max=2048,\n",
        "    description='Preprocessor resolution',\n",
        "    style=style,\n",
        ")\n",
        "\n",
        "image_resolution_global = widgets.IntSlider(\n",
        "    value=512,\n",
        "    min=64,\n",
        "    step=64,\n",
        "    max=2048,\n",
        "    description='Image resolution',\n",
        "    style=style,\n",
        ")\n",
        "\n",
        "# Create a dictionary to map options to lists of IntText widgets\n",
        "int_inputs = {\n",
        "\n",
        "    control_model_list[13]: [\n",
        "    ],\n",
        "    control_model_list[12]: [\n",
        "        widgets.FloatSlider(value=1.0, min=0.01, max=1.0, step=0.01, description='Inpaint strength:', layout=Layout(visibility='hidden'), style=style),\n",
        "        widgets.Text(value=\"\", placeholder=\"/content/my_mask.png\", rows=1, description='Mask path:', layout=Layout(visibility='hidden'), style=style)\n",
        "    ],\n",
        "    control_model_list[0]: [\n",
        "        widgets.Dropdown(value='Openpose', description='Preprocessor:', options=['None','Openpose'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[1]: [\n",
        "        widgets.BoundedIntText(value=100, min=1, max=255, description='Canny low threshold:', layout=Layout(visibility='hidden'), style=style),\n",
        "        widgets.BoundedIntText(value=200, min=1, max=255, description='Canny high threshold:', layout=Layout(visibility='hidden'), style=style)\n",
        "    ],\n",
        "    control_model_list[2]: [\n",
        "        widgets.BoundedFloatText(value=0.1, min=1, max=2.0, step=0.01, description='Hough value threshold (MLSD):', layout=Layout(visibility='hidden'), style=style),\n",
        "        widgets.BoundedFloatText(value=0.1, min=1, max=20.0, step=0.01, description='Hough distance threshold (MLSD):', layout=Layout(visibility='hidden'), style=style)\n",
        "    ],\n",
        "    control_model_list[3]: [\n",
        "        widgets.Dropdown(value='HED', description='Preprocessor:', options=['HED','PidiNet', 'None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[4]: [\n",
        "        widgets.Dropdown(value='PidiNet', description='Preprocessor:', options=['HED','PidiNet', 'HED safe', 'PidiNet safe','None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[5]: [\n",
        "        widgets.Dropdown(value='UPerNet', description='Preprocessor:', options=['UPerNet','None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[6]: [\n",
        "        widgets.Dropdown(value='DPT', description='Preprocessor:', options=['Midas', 'DPT','None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[7]: [\n",
        "        widgets.Dropdown(value='NormalBae', description='Preprocessor:', options=['NormalBae','None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[8]: [\n",
        "        widgets.Dropdown(value='Lineart', description='Preprocessor:', options=['Lineart','Lineart coarse', 'None', 'Lineart (anime)', 'None (anime)'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[10]: [\n",
        "        widgets.Dropdown(value='ContentShuffle', description='Preprocessor:', options=['ContentShuffle','None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Function to update visibility and enable/disable state of widgets\n",
        "def update_widgets(option):\n",
        "    for opt, int_inputs_list in int_inputs.items():\n",
        "        if opt == option:\n",
        "            for int_input in int_inputs_list:\n",
        "                int_input.layout.visibility = 'visible'\n",
        "        else:\n",
        "            for int_input in int_inputs_list:\n",
        "                int_input.layout.visibility = 'hidden'\n",
        "\n",
        "interactive(update_widgets, option=select_task)\n",
        "\n",
        "# =====================================\n",
        "# Settings\n",
        "# =====================================\n",
        "loop_generator = widgets.IntSlider(\n",
        "    value=1,\n",
        "    min=1,\n",
        "    max=10,\n",
        "    description='Loops ðŸ”',\n",
        "    #layout=auto_layout,\n",
        "    style = style,\n",
        ")\n",
        "\n",
        "xformers_memory_efficient_attention = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Xformers memory efficient attention',\n",
        "    layout=auto_layout,\n",
        "    style = style,\n",
        ")\n",
        "\n",
        "model_precision = widgets.RadioButtons(\n",
        "    options=[(\"float16\", torch.float16),(\"float32\", torch.float32)],\n",
        "    description=\"Model precision (float32 need more memory):\",\n",
        "    layout=auto_layout,\n",
        "    style = style,\n",
        ")\n",
        "\n",
        "init_generator_in_cpu = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Generate initial random noise with the seed on the CPU',\n",
        "    layout=auto_layout,\n",
        "    style = style,\n",
        ")\n",
        "# =====================================\n",
        "# App\n",
        "# =====================================\n",
        "\n",
        "title_tab_one = widgets.HTML(\n",
        "    value=\"<h2>SD Interactive</h2>\",\n",
        "    layout=widgets.Layout(display=\"flex\", justify_content=\"center\")\n",
        ")\n",
        "title_tab_two = widgets.HTML(\n",
        "    value=\"<h2>Settings</h2>\",\n",
        "    layout=widgets.Layout(display=\"flex\", justify_content=\"center\")\n",
        ")\n",
        "\n",
        "buttons_ = TwoByTwoLayout(top_left=generate,\n",
        "               top_right=show_textual_inversion,\n",
        "               #bottom_left=None,\n",
        "               bottom_right=clear_outputs, merge=True)\n",
        "\n",
        "show_textual_inversion.style.button_color = '#97BC62'\n",
        "clear_outputs.style.button_color = '#97BC62'\n",
        "generate.style.button_color = '#2C5F2D'\n",
        "\n",
        "# TAB 1\n",
        "tab_sd = widgets.VBox(\n",
        "    [\n",
        "      widgets.AppLayout(\n",
        "        header=None,\n",
        "        left_sidebar=widgets.VBox(\n",
        "            [\n",
        "                title_tab_one,\n",
        "                num_images,\n",
        "                steps,\n",
        "                CFG,\n",
        "                select_sampler,\n",
        "                img_width,\n",
        "                img_height,\n",
        "                random_seed,\n",
        "                widgets.HBox([select_lora1, lora_weights_scale1],layout=widgets.Layout(width=width)),\n",
        "                widgets.HBox([select_lora2, lora_weights_scale2],layout=widgets.Layout(width=width)),\n",
        "                widgets.HBox([select_lora3, lora_weights_scale3],layout=widgets.Layout(width=width)),\n",
        "                widgets.HBox([select_lora4, lora_weights_scale4],layout=widgets.Layout(width=width)),\n",
        "                widgets.HBox([select_lora5, lora_weights_scale5],layout=widgets.Layout(width=width)),\n",
        "                select_clip_skip,\n",
        "                freeu_check,\n",
        "            ]\n",
        "        ),\n",
        "        center=widgets.VBox(\n",
        "            [\n",
        "                select_task,\n",
        "                select_model,\n",
        "                vae_model_dropdown,\n",
        "                prompt,\n",
        "                neg_prompt,\n",
        "                widgets.HBox([weights_prompt ,active_ti]),\n",
        "                buttons_,\n",
        "            ]\n",
        "        ),\n",
        "        right_sidebar=widgets.VBox(\n",
        "            [\n",
        "                controlnet_output_scaling_in_unet,\n",
        "                controlnet_start_threshold,\n",
        "                controlnet_stop_threshold,\n",
        "            ] +\n",
        "            [preprocess_resolution_global] + [image_resolution_global] + [int_input for int_inputs_list in int_inputs.values() for int_input in int_inputs_list]\n",
        "        ),\n",
        "        footer=None,\n",
        "        pane_widths=[1.1, 1.4, 1.2],\n",
        "        # pane_heights=[\"0px\", 1, '0px'],\n",
        "      ),\n",
        "      display_imgs,\n",
        "    ]\n",
        ")\n",
        "\n",
        "# TAB 2\n",
        "tab_settings = widgets.VBox([\n",
        "    title_tab_two,\n",
        "    loop_generator,\n",
        "    xformers_memory_efficient_attention,\n",
        "    init_generator_in_cpu,\n",
        "    model_precision,\n",
        "    widgets.HTML(\n",
        "        value=\"<p>â²ï¸</p>\",\n",
        "        layout=widgets.Layout(display=\"flex\", justify_content=\"center\")\n",
        "    ),\n",
        "])\n",
        "\n",
        "# APP\n",
        "tab = widgets.Tab()\n",
        "tab.children = [\n",
        "  widgets.VBox(\n",
        "    children = [\n",
        "        tab_sd,\n",
        "    ]),\n",
        "  widgets.VBox(\n",
        "    children = [\n",
        "        tab_settings,\n",
        "    ]),\n",
        "]\n",
        "\n",
        "tab.set_title(0, \"Stable Diffusion\")\n",
        "tab.set_title(1,  \"Settings\")\n",
        "tab.selected_index = 0\n",
        "\n",
        "display(tab)"
      ],
      "metadata": {
        "id": "atmx0PNQ78Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload an image here for use in Inpainting or ControlNet. ðŸ‘ˆâ€â€ ðŸ–¼ï¸ðŸ–¼ï¸ðŸ–¼ï¸\n",
        "#@markdown - To use Controlnet, you need to upload the control image with this cell\n",
        "Create_mask_for_Inpaint = False # @param {type:\"boolean\"}\n",
        "stroke_width = 24 # @param {type:\"integer\"}\n",
        "from google.colab import files\n",
        "from IPython.display import HTML\n",
        "import os\n",
        "import shutil\n",
        "%cd /content\n",
        "uploaded = files.upload()\n",
        "\n",
        "filename = next(iter(uploaded))\n",
        "print(f'Uploaded file: {filename}')\n",
        "\n",
        "upload_folder = 'uploaded_controlnet_image/'\n",
        "if not os.path.exists(upload_folder):\n",
        "    os.makedirs(upload_folder)\n",
        "\n",
        "source_path = filename\n",
        "destination_path_cn_img = os.path.join(upload_folder, filename)\n",
        "shutil.move(source_path, destination_path_cn_img)\n",
        "print(f'Moved file to {destination_path_cn_img}')\n",
        "\n",
        "if select_task.value == 'Inpaint' or Create_mask_for_Inpaint:\n",
        "    init_image = destination_path_cn_img\n",
        "    name_without_extension = os.path.splitext(init_image.split('/')[-1])[0]\n",
        "\n",
        "    image64 = base64.b64encode(open(init_image, 'rb').read())\n",
        "    image64 = image64.decode('utf-8')\n",
        "\n",
        "    print('\\033[34m Draw the mask with the mouse \\033[0m')\n",
        "    img = np.array(plt.imread(f'{init_image}')[:,:,:3])\n",
        "\n",
        "    draw(image64, filename=f\"./{name_without_extension}_draw.png\", w=img.shape[1], h=img.shape[0], line_width=stroke_width)\n",
        "\n",
        "    with_mask = np.array(plt.imread(f\"./{name_without_extension}_draw.png\")[:,:,:3])\n",
        "    mask = (with_mask[:,:,0]==1)*(with_mask[:,:,1]==0)*(with_mask[:,:,2]==0)\n",
        "    plt.imsave(f\"./{name_without_extension}_mask.png\",mask, cmap='gray')\n",
        "    mask_control = f\"./{name_without_extension}_mask.png\"\n",
        "    print(f'\\033[34m Mask saved: {mask_control} \\033[0m')"
      ],
      "metadata": {
        "id": "D-L_EepAsLPE",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upscale and face restoration { form-width: \"20%\", display-mode: \"form\" }\n",
        "from IPython.utils import capture\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "%cd /content\n",
        "directory_codeformer = '/content/CodeFormer/'\n",
        "with capture.capture_output() as cap:\n",
        "  if not os.path.exists(directory_codeformer):\n",
        "      os.makedirs(directory_codeformer)\n",
        "\n",
        "      # Setup\n",
        "      # Clone CodeFormer and enter the CodeFormer folder\n",
        "      %cd /content\n",
        "      !git clone https://github.com/sczhou/CodeFormer.git\n",
        "      %cd CodeFormer\n",
        "\n",
        "\n",
        "      # Set up the environment\n",
        "      # Install python dependencies\n",
        "      !pip install -q -r requirements.txt\n",
        "      !pip -q install ffmpeg\n",
        "      # Install basicsr\n",
        "      !python basicsr/setup.py develop\n",
        "\n",
        "      # Download the pre-trained model\n",
        "      !python scripts/download_pretrained_models.py facelib\n",
        "      !python scripts/download_pretrained_models.py CodeFormer\n",
        "  del cap\n",
        "# Visualization function\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "def display_codeformer(img1, img2):\n",
        "  fig = plt.figure(figsize=(25, 10))\n",
        "  ax1 = fig.add_subplot(1, 2, 1)\n",
        "  plt.title('Input', fontsize=16)\n",
        "  ax1.axis('off')\n",
        "  ax2 = fig.add_subplot(1, 2, 2)\n",
        "  plt.title('CodeFormer', fontsize=16)\n",
        "  ax2.axis('off')\n",
        "  ax1.imshow(img1)\n",
        "  ax2.imshow(img2)\n",
        "def imread(img_path):\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  return img\n",
        "\n",
        "# Copy imgs\n",
        "Select_an_image = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# PROCESS AD\n",
        "if os.path.exists(Select_an_image.strip()):\n",
        "    image_list = [Select_an_image.replace('/content/', '').strip()]\n",
        "\n",
        "destination_directory = '/content/CodeFormer/inputs/user_upload'\n",
        "!rm -rf /content/CodeFormer/inputs/user_upload/*\n",
        "os.makedirs(destination_directory, exist_ok=True)\n",
        "for image_path in image_list:\n",
        "    image_filename = os.path.basename('/content/'+image_path)\n",
        "    destination_path = os.path.join(destination_directory, image_filename)\n",
        "    try:\n",
        "        shutil.copyfile('/content/'+image_path, destination_path)\n",
        "        print(f\"Image '{image_filename}' has been copied to '{destination_path}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to copy '{image_filename}' to '{destination_path}': {e}\")\n",
        "\n",
        "#@markdown `CODEFORMER_FIDELITY`: Balance the quality (lower number) and fidelity (higher number)<br>\n",
        "# you can add '--bg_upsampler realesrgan' to enhance the background\n",
        "CODEFORMER_FIDELITY = 0.7 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown `BACKGROUND_ENHANCE`: Enhance background image with Real-ESRGAN<br>\n",
        "BACKGROUND_ENHANCE = True #@param {type:\"boolean\"}\n",
        "#@markdown `FACE_UPSAMPLE`: Upsample restored faces for high-resolution AI-created images<br>\n",
        "FACE_UPSAMPLE = False #@param {type:\"boolean\"}\n",
        "#markdown `HAS_ALIGNED`: Input are cropped and aligned faces<br>\n",
        "HAS_ALIGNED =  False\n",
        "#@markdown `UPSCALE`: The final upsampling scale of the image. Default: 2<br>\n",
        "UPSCALE = 3 #@param {type:\"slider\", min:2, max:8, step:1}\n",
        "#markdown `DETECTION_MODEL`: Face detector. Default: retinaface_resnet50<br>\n",
        "DETECTION_MODEL = \"retinaface_resnet50\"\n",
        "#markdown `DRAW_BOX`: Draw the bounding box for the detected faces.\n",
        "DRAW_BOX = False\n",
        "\n",
        "BACKGROUND_ENHANCE = '--bg_upsampler realesrgan' if BACKGROUND_ENHANCE else ''\n",
        "FACE_UPSAMPLE = '--face_upsample' if FACE_UPSAMPLE else ''\n",
        "HAS_ALIGNED = '--has_aligned' if HAS_ALIGNED else ''\n",
        "DRAW_BOX = '--draw_box' if DRAW_BOX else ''\n",
        "%cd CodeFormer\n",
        "!python inference_codeformer.py -w $CODEFORMER_FIDELITY --input_path {destination_directory} {BACKGROUND_ENHANCE} {FACE_UPSAMPLE} {HAS_ALIGNED} --upscale {UPSCALE} --detection_model {DETECTION_MODEL} {DRAW_BOX}\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "input_folder = 'inputs/user_upload'\n",
        "result_folder = f'results/user_upload_{CODEFORMER_FIDELITY}/final_results'\n",
        "input_list = sorted(glob.glob(os.path.join(input_folder, '*')))\n",
        "for input_path in input_list:\n",
        "  img_input = imread(input_path)\n",
        "  basename = os.path.splitext(os.path.basename(input_path))[0]\n",
        "  output_path = os.path.join(result_folder, basename+'.png')\n",
        "  img_output = imread(output_path)\n",
        "  display_codeformer(img_input, img_output)\n",
        "\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "__21H6ZLUokz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results in /content/CodeFormer/results\n"
      ],
      "metadata": {
        "id": "OFbbS2mdf7Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Images\n",
        "import os\n",
        "from google.colab import files\n",
        "!rm /content/results.zip\n",
        "!ls /content/images\n",
        "print('Download results')\n",
        "os.system(f'zip -r results.zip /content/images')\n",
        "try:\n",
        "  files.download(\"results.zip\")\n",
        "except:\n",
        "  print(\"Error\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Wlmke1VJPWS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Upscale results\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "%cd /content/CodeFormer\n",
        "!ls results\n",
        "print('Download results')\n",
        "os.system(f'zip -r results.zip results/user_upload_{CODEFORMER_FIDELITY}/final_results')\n",
        "try:\n",
        "  files.download(\"results.zip\")\n",
        "except:\n",
        "  files.download(f'/content/CodeFormer/results/{filename[:-4]}_{CODEFORMER_FIDELITY}/{filename}')\n",
        "%cd /content"
      ],
      "metadata": {
        "cellView": "form",
        "id": "k-FPYmh2Wb4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extras"
      ],
      "metadata": {
        "id": "g83HiPS2mSqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also use this cell to simply reload the model in case you need to.\n",
        "del model"
      ],
      "metadata": {
        "id": "JHxjQLbQ--O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "upscaler_dict = {\n",
        "    \"RealESRGAN_x4plus\" : \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth\",\n",
        "    \"RealESRNet_x4plus\" : \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.1/RealESRNet_x4plus.pth\",\n",
        "    \"RealESRGAN_x4plus_anime_6B\": \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth\",\n",
        "    \"RealESRGAN_x2plus\": \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth\",\n",
        "    \"realesr-animevideov3\": \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-animevideov3.pth\",\n",
        "    \"realesr-general-x4v3\": \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth\",\n",
        "    \"realesr-general-wdn-x4v3\" : \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-wdn-x4v3.pth\",\n",
        "    \"4x-UltraSharp\" : \"https://huggingface.co/Shandypur/ESRGAN-4x-UltraSharp/resolve/main/4x-UltraSharp.pth\",\n",
        "    \"4x_foolhardy_Remacri\" : \"https://huggingface.co/FacehugmanIII/4x_foolhardy_Remacri/resolve/main/4x_foolhardy_Remacri.pth\",\n",
        "    \"Remacri4xExtraSmoother\" : \"https://huggingface.co/hollowstrawberry/upscalers-backup/resolve/main/ESRGAN/Remacri%204x%20ExtraSmoother.pth\",\n",
        "    \"AnimeSharp4x\" : \"https://huggingface.co/hollowstrawberry/upscalers-backup/resolve/main/ESRGAN/AnimeSharp%204x.pth\",\n",
        "    \"lollypop\" : \"https://huggingface.co/hollowstrawberry/upscalers-backup/resolve/main/ESRGAN/lollypop.pth\",\n",
        "    \"RealisticRescaler4x\" : \"https://huggingface.co/hollowstrawberry/upscalers-backup/resolve/main/ESRGAN/RealisticRescaler%204x.pth\",\n",
        "    \"NickelbackFS4x\" : \"https://huggingface.co/hollowstrawberry/upscalers-backup/resolve/main/ESRGAN/NickelbackFS%204x.pth\"\n",
        "}\n",
        "\n",
        "#@markdown # Alternative Upscaler Tool\n",
        "#@markdown You can leave `Select_an_image` blank to process the last generated images.\n",
        "Select_an_image = \"\" # @param {type:\"string\"}\n",
        "MODEL_UPSCALER = \"RealESRGAN_x4plus_anime_6B\" #@param [\"RealESRGAN_x4plus\", \"RealESRNet_x4plus\", \"RealESRGAN_x2plus\", \"RealESRGAN_x4plus_anime_6B\", \"realesr-animevideov3\", \"realesr-general-x4v3\", \"realesr-general-wdn-x4v3\", \"4x-UltraSharp\", \"4x_foolhardy_Remacri\", \"Remacri4xExtraSmoother\", \"AnimeSharp4x\", \"lollypop\", \"RealisticRescaler4x\", \"NickelbackFS4x\"]\n",
        "Scale_of_the_image_x = 1.5 #@param {type:\"slider\", min:1, max:4, step:0.5}\n",
        "show_result = True #@param {type: \"boolean\"}\n",
        "\n",
        "directory_upscalers = 'upscalers'\n",
        "os.makedirs(directory_upscalers, exist_ok=True)\n",
        "\n",
        "url_upscaler = upscaler_dict[MODEL_UPSCALER]\n",
        "\n",
        "if not os.path.exists(f\"./upscalers/{url_upscaler.split('/')[-1]}\"):\n",
        "    download_things(directory_upscalers, url_upscaler, hf_token)\n",
        "\n",
        "scaler_beta = UpscalerESRGAN(\"\")\n",
        "\n",
        "if os.path.exists(Select_an_image.strip()):\n",
        "    image_list = [Select_an_image.replace('/content/', '').strip()]\n",
        "\n",
        "for img_base in image_list:\n",
        "    img_pil = Image.open(img_base)\n",
        "    img_up = scaler_beta.upscale(img_pil, Scale_of_the_image_x , f\"./upscalers/{url_upscaler.split('/')[-1]}\")\n",
        "\n",
        "    if show_result:\n",
        "        display(img_up)\n",
        "\n",
        "    image_path = save_pil_image_with_metadata(img_up, f'{os.getcwd()}/up_images', metadata_list=None)\n",
        "    print(image_path)"
      ],
      "metadata": {
        "id": "BjBOzTfY9FDA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown CONVERT SAFETENSORS TO DIFFUSERS for SD 1.5\n",
        "path_safetensor_model = \"\" # @param {type:\"string\"}\n",
        "path_diffusers_model = \"./converted_model/\" # @param {type:\"string\"}\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "pipe = StableDiffusionPipeline.from_single_file(input_safetensor_model).to(\"cuda\")\n",
        "pipe.save_pretrained(output_diffusers_model) # model path inpaint is ./adetailer_model/\n",
        "del pipe\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HCWT08a6VJtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utility https://github.com/Linaqruf/sdxl-model-converter"
      ],
      "metadata": {
        "id": "F3zWvKUq0WkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "try:\n",
        "  del model\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "except:\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "\n",
        "\n",
        "\n",
        "# OPTIONS #\n",
        "# @markdown # Adetailer ðŸš« beta\n",
        "# @markdown work in progress for implement safetensors format\n",
        "face_detector_ad = True # @param {type:\"boolean\"}\n",
        "person_detector_ad = True # @param {type:\"boolean\"}\n",
        "hand_detector_ad = False # @param {type:\"boolean\"}\n",
        "path_diffusers_model = \"./converted_model/\" # @param {type:\"string\"}\n",
        "common = {\n",
        "    \"prompt\": \"masterpiece, best quality\",\n",
        "    \"num_inference_steps\": 40,\n",
        "    \"height\": img_height.value,\n",
        "    \"width\": img_width.value,\n",
        "}\n",
        "inpaint = {\n",
        "    \"prompt\": \"masterpiece, best quality\",\n",
        "    \"num_inference_steps\": 40,\n",
        "    \"height\": img_height.value,\n",
        "    \"width\": img_width.value,\n",
        "}\n",
        "Select_an_image = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# PROCESS AD\n",
        "if os.path.exists(Select_an_image):\n",
        "    image_list = [Select_an_image]\n",
        "\n",
        "image_list = ad_model_process(\n",
        "    face_detector_ad,\n",
        "    person_detector_ad,\n",
        "    hand_detector_ad,\n",
        "    model_for_inpaint,\n",
        "    common,\n",
        "    inpaint,\n",
        "    image_list,\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CqH-rX3VVCg7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}