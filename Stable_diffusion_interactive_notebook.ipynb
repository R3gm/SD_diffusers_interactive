{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R3gm/SD_diffusers_interactive/blob/main/Stable_diffusion_interactive_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZl9FPC9sbW_"
      },
      "source": [
        "# Interactive Stable Diffusion 0.1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MRWnnDNp4Mh"
      },
      "source": [
        "| Description | Link |\n",
        "| ----------- | ---- |\n",
        "| üéâ Repository | [![GitHub Repository](https://img.shields.io/github/stars/R3gm/SD_diffusers_interactive?style=social)](https://github.com/R3gm/SD_diffusers_interactive) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8lnApcK4vWv"
      },
      "source": [
        "- FreeU for all tasks\n",
        "- More embeddings\n",
        "- For better performance, disable the progress bar in settings.\n",
        "- Adetailer for SD 1.5\n",
        "- Prompt weights: Depending on model and CFG you can weight up to around 1.5 or 1.6 before things start to get weird.\n",
        "- Controlnet 1.1 for SD\n",
        "- SDXL models only support txt2img\n",
        "- More functions, more bugs; less than 10 words, more laughs\n",
        "\n",
        "Previous version of the Interactive Stable Diffusion: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/R3gm/SD_diffusers_interactive/blob/c81decc6defc9528116eac965e3fc9999f276ecf/Stable_diffusion_interactive_notebook.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wILzWiWRfTX8"
      },
      "source": [
        "This Google Colab notebook offers a user-friendly interface for generating AI images from text prompts using Stable Diffusion. It uses [Stablepy](https://github.com/R3gm/stablepy) and Jupyter widgets, providing a simple and lightweight alternative to web-based tools, making it accessible for all to `get started with Stable Diffusion`. With Stablepy, you can smoothly apply stable diffusion in Python, enabling seamless integration with any interface.\n",
        "\n",
        "GUI Based on [redromnon's repository](https://github.com/redromnon/stable-diffusion-interactive-notebook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A04WkRDwGpLY"
      },
      "outputs": [],
      "source": [
        "#@title 1. Installing dependencies\n",
        "\n",
        "!pip install -q git+https://github.com/R3gm/stablepy.git@v0.1.0\n",
        "!apt -y install -qq aria2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da0BxuMj2gBk"
      },
      "outputs": [],
      "source": [
        "#@title 2. Download Models: Please provide a link for the Civitai API, Google Drive, or Hugging Face. { form-width: \"20%\", display-mode: \"form\" }\n",
        "import os\n",
        "%cd /content\n",
        "\n",
        "def download_things(directory, url, hf_token=\"\"):\n",
        "    url = url.strip()\n",
        "\n",
        "    if \"drive.google.com\" in url:\n",
        "        original_dir = os.getcwd()\n",
        "        os.chdir(directory)\n",
        "        !gdown --fuzzy {url}\n",
        "        os.chdir(original_dir)\n",
        "    elif \"huggingface.co\" in url:\n",
        "        if \"/blob/\" in url:\n",
        "            url = url.replace(\"/blob/\", \"/resolve/\")\n",
        "        user_header = f'\"Authorization: Bearer {hf_token}\"'\n",
        "        !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 {url} -d {directory}  -o {url.split('/')[-1]}\n",
        "    else:\n",
        "        !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {directory} {url}\n",
        "\n",
        "def get_model_list(directory_path):\n",
        "    model_list = []\n",
        "    valid_extensions = {'.ckpt' , '.pt', '.pth', '.safetensors', '.bin'}\n",
        "\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if os.path.splitext(filename)[1] in valid_extensions:\n",
        "            name_without_extension = os.path.splitext(filename)[0]\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            model_list.append((name_without_extension, file_path))\n",
        "            print('\\033[34mFILE: ' + file_path + '\\033[0m')\n",
        "    return model_list\n",
        "\n",
        "def process_string(input_string):\n",
        "    parts = input_string.split('/')\n",
        "\n",
        "    if len(parts) == 2:\n",
        "        first_element = parts[1]\n",
        "        complete_string = input_string\n",
        "        result = (first_element, complete_string)\n",
        "        return result\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "directory_models = 'models'\n",
        "os.makedirs(directory_models, exist_ok=True)\n",
        "directory_loras = 'loras'\n",
        "os.makedirs(directory_loras, exist_ok=True)\n",
        "directory_vaes = 'vaes'\n",
        "os.makedirs(directory_vaes, exist_ok=True)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown - **Download a SD Model**\n",
        "download_model = \"https://civitai.com/api/download/models/125771\" # @param {type:\"string\"}\n",
        "#@markdown - For SDXL models, only diffuser format models are supported, and you only need the repository name `user/repo_name`. You can find some compatible XL models [here](https://huggingface.co/models?search=-xl-fp16).\n",
        "load_diffusers_format_model = 'SG161222/RealVisXL_V2.0' # @param {type:\"string\"}\n",
        "#@markdown - **Download a VAE**\n",
        "download_vae = \"https://huggingface.co/fp16-guy/anything_kl-f8-anime2_vae-ft-mse-840000-ema-pruned_blessed_clearvae_fp16_cleaned/resolve/main/anything_fp16.safetensors\" # @param {type:\"string\"}\n",
        "#@markdown - **Download a LoRA**\n",
        "download_lora = \"https://civitai.com/api/download/models/97655\" # @param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **HF TOKEN** - If you need to download your private model from Hugging Face, input your token here.\n",
        "hf_token = \"\"  # @param {type:\"string\"}\n",
        "#@markdown\n",
        "#@markdown ---\n",
        "\n",
        "download_things(directory_models, download_model, hf_token)\n",
        "download_things(directory_vaes, download_vae, hf_token)\n",
        "download_things(directory_loras, download_lora, hf_token)\n",
        "\n",
        "\n",
        "# Embeddings\n",
        "directory_embeds = 'embedings'\n",
        "os.makedirs(directory_embeds, exist_ok=True)\n",
        "download_embeds = [\n",
        "    'https://huggingface.co/datasets/Nerfgun3/bad_prompt/resolve/main/bad_prompt.pt',\n",
        "    'https://huggingface.co/datasets/Nerfgun3/bad_prompt/blob/main/bad_prompt_version2.pt',\n",
        "    'https://huggingface.co/embed/EasyNegative/resolve/main/EasyNegative.safetensors',\n",
        "    'https://huggingface.co/embed/negative/resolve/main/EasyNegativeV2.safetensors',\n",
        "    'https://huggingface.co/embed/negative/resolve/main/bad-hands-5.pt',\n",
        "    'https://huggingface.co/embed/negative/resolve/main/bad-artist.pt',\n",
        "    'https://huggingface.co/embed/negative/resolve/main/ng_deepnegative_v1_75t.pt',\n",
        "    'https://huggingface.co/embed/negative/resolve/main/bad-artist-anime.pt',\n",
        "    'https://huggingface.co/embed/negative/resolve/main/bad-image-v2-39000.pt',\n",
        "    'https://huggingface.co/embed/negative/resolve/main/verybadimagenegative_v1.3.pt',\n",
        "    ]\n",
        "\n",
        "for url_embed in download_embeds:\n",
        "    if not os.path.exists(f\"./embedings/{url_embed.split('/')[-1]}\"):\n",
        "        download_things(directory_embeds, url_embed, hf_token)\n",
        "\n",
        "# Build list models\n",
        "embed_list = get_model_list(directory_embeds)\n",
        "model_list = get_model_list(directory_models)\n",
        "if load_diffusers_format_model.strip() != \"\" and load_diffusers_format_model.count('/') == 1:\n",
        "    model_list.append(process_string(load_diffusers_format_model))\n",
        "lora_model_list = get_model_list(directory_loras)\n",
        "lora_model_list.insert(0, (\"None\",None))\n",
        "vae_model_list = get_model_list(directory_vaes)\n",
        "vae_model_list.insert(0, (\"None\", None))\n",
        "\n",
        "\n",
        "\n",
        "print('\\033[33müèÅ Download finished.\\033[0m')\n",
        "\n",
        "from stablepy import Model_Diffusers, CONTROLNET_MODEL_IDS, UpscalerESRGAN\n",
        "from stablepy.diffusers_vanilla.inpainting_canvas import draw\n",
        "from stablepy.diffusers_vanilla.adetailer import ad_model_process\n",
        "from stablepy.diffusers_vanilla.utils import save_pil_image_with_metadata\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atmx0PNQ78Wa"
      },
      "outputs": [],
      "source": [
        "#@title 3. GUI { form-width: \"20%\", display-mode: \"form\" }\n",
        "#@markdown ---\n",
        "#@markdown - **Prompt** - Tell the model what you want to see.\n",
        "#@markdown - **Negative Prompt** - Tell the model what you don't want to see.\n",
        "#@markdown - **Steps** - How long the model should work on the image.\n",
        "#@markdown - **CFG** - Controls how much the image generation process follows the text prompt.  Guidance scale ranging from 0 to 20. Lower values allow the AI to be more creative and less strict at following the prompt. Default is 7.5\n",
        "#@markdown - **Sampler** - Progressively reduce image noise through denoising steps.\n",
        "#@markdown - **Seed** -  A number that helps the model start generating the image. Set `-1` for using random seed values.\n",
        "#@markdown - **Clip Skip** - It allows to control the level of detail and accuracy in the generated images by skipping certain layers of the CLIP model during the image generation process.\n",
        "#@markdown - **Prompt weights** -  Helps the model focus on different parts of the prompt. Prompt weights can be used to emphasize or de-emphasize certain aspects of the image, such as the object, the scene, or the style. Currently, the [Compel syntax](https://github.com/damian0815/compel/blob/main/doc/syntax.md) is being used. You can also activate the `Convert Prompt weights` to automatically convert syntax from `(word:1.1)` to `(word)1.1` or `(word)` to `(word)+` to make them compatible with Compel weights. Compel scale more with its values so that fewer weights are needed for good results\n",
        "#@markdown - **Embeddings** - Help the model to adapt to a particular style\n",
        "#@markdown - **FreeU** - Is a method that substantially improves diffusion model sample quality at no costs.\n",
        "#@markdown - **ControlNet** - Enhances text-to-image diffusion models by allowing various spatial contexts to serve as additional conditioning, enabling the generation of more controlled and context-aware images. The right column contains all the specific ControlNet options.\n",
        "#@markdown ---\n",
        "%cd /content\n",
        "import ipywidgets as widgets, mediapy, random\n",
        "from PIL import Image\n",
        "import IPython.display\n",
        "import time\n",
        "from IPython.utils import capture\n",
        "import logging\n",
        "from ipywidgets import Button, Layout, jslink, IntText, IntSlider, BoundedFloatText\n",
        "logging.getLogger(\"diffusers\").setLevel(logging.ERROR)\n",
        "import diffusers\n",
        "diffusers.utils.logging.set_verbosity(40)\n",
        "#from IPython.display import display\n",
        "from ipywidgets import TwoByTwoLayout, Layout, Button, Box, FloatText, Textarea, Dropdown, Label, IntSlider, interactive, HBox, VBox, BoundedIntText, BoundedFloatText\n",
        "import gc\n",
        "\n",
        "#PARAMETER WIDGETS\n",
        "width = \"225px\"\n",
        "auto_layout = widgets.Layout(height='auto', width='auto')\n",
        "lora_layout = {'width':'165px'}\n",
        "lora_scale_layout = {'width':'50px'}\n",
        "style = {'description_width': 'initial'} # show full text\n",
        "\n",
        "# =====================================\n",
        "# Left\n",
        "# =====================================\n",
        "\n",
        "num_images = widgets.BoundedIntText(\n",
        "    value = 1,\n",
        "    min = 1,\n",
        "    description=\"Images:\",\n",
        "    layout=widgets.Layout(width=width)\n",
        ")\n",
        "\n",
        "steps = widgets.BoundedIntText(\n",
        "    value = 30,\n",
        "    min = 1,\n",
        "    max = 100,\n",
        "    description=\"Steps:\",\n",
        "    layout=widgets.Layout(width=width)\n",
        ")\n",
        "\n",
        "CFG = widgets.BoundedFloatText(\n",
        "    value = 7.5,\n",
        "    min = 0,\n",
        "    step=0.5,\n",
        "    description=\"CFG:\",\n",
        "    layout=widgets.Layout(width=width)\n",
        ")\n",
        "\n",
        "select_sampler = widgets.Dropdown(\n",
        "    options=[\n",
        "        \"DPM++ 2M\",\n",
        "        \"DPM++ 2M Karras\",\n",
        "        \"DPM++ 2M SDE\",\n",
        "        \"DPM++ 2M SDE Karras\",\n",
        "        \"DPM++ SDE\",\n",
        "        \"DPM++ SDE Karras\",\n",
        "        \"DPM2\",\n",
        "        \"DPM2 Karras\",\n",
        "        \"Euler\",\n",
        "        \"Euler a\",\n",
        "        \"Heun\",\n",
        "        \"LMS\",\n",
        "        \"LMS Karras\",\n",
        "        \"DDIMScheduler\",\n",
        "        \"DEISMultistepScheduler\",\n",
        "        \"UniPCMultistepScheduler\",\n",
        "    ],\n",
        "    description=\"Sampler:\",\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "img_height = widgets.BoundedIntText(\n",
        "    min=64,\n",
        "    max=4096,\n",
        "    step=8,\n",
        "    value=512,\n",
        "    description=\"Height:\",\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "img_width = widgets.BoundedIntText(\n",
        "    min=64,\n",
        "    max=4096,\n",
        "    step=8,\n",
        "    value=512,\n",
        "    description=\"Width:\",\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "random_seed = widgets.IntText(\n",
        "    value=-1,\n",
        "    description=\"Seed:\",\n",
        "    layout=widgets.Layout(width=width),\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "#lora1\n",
        "select_lora1 = widgets.Dropdown(\n",
        "    options=lora_model_list,\n",
        "    description=\"Lora1:\",\n",
        "    layout=lora_layout\n",
        ")\n",
        "\n",
        "lora_weights_scale1 = widgets.FloatText(\n",
        "    min=-2.0,\n",
        "    max=2.0,\n",
        "    step=0.01,\n",
        "    value=1,\n",
        "    #description=\"Lora scale1:\",\n",
        "    layout=lora_scale_layout\n",
        ")\n",
        "#lora2\n",
        "select_lora2 = widgets.Dropdown(\n",
        "    options=lora_model_list,\n",
        "    description=\"Lora2:\",\n",
        "    layout=lora_layout\n",
        ")\n",
        "\n",
        "lora_weights_scale2 = widgets.FloatText(\n",
        "    min=-2.0,\n",
        "    max=2.0,\n",
        "    step=0.01,\n",
        "    value=1,\n",
        "    #description=\"Lora scale2:\",\n",
        "    layout=lora_scale_layout\n",
        ")\n",
        "#lora3\n",
        "select_lora3 = widgets.Dropdown(\n",
        "    options=lora_model_list,\n",
        "    description=\"Lora3:\",\n",
        "    layout=lora_layout\n",
        ")\n",
        "\n",
        "lora_weights_scale3 = widgets.FloatText(\n",
        "    min=-2.0,\n",
        "    max=2.0,\n",
        "    step=0.01,\n",
        "    value=1,\n",
        "    #description=\"Lora scale3:\",\n",
        "    layout=lora_scale_layout\n",
        ")\n",
        "#lora4\n",
        "select_lora4 = widgets.Dropdown(\n",
        "    options=lora_model_list,\n",
        "    description=\"Lora4:\",\n",
        "    layout=lora_layout\n",
        ")\n",
        "\n",
        "lora_weights_scale4 = widgets.FloatText(\n",
        "    min=-2.0,\n",
        "    max=2.0,\n",
        "    step=0.01,\n",
        "    value=1,\n",
        "    #description=\"Lora scale4:\",\n",
        "    layout=lora_scale_layout\n",
        ")\n",
        "#lora5\n",
        "select_lora5 = widgets.Dropdown(\n",
        "    options=lora_model_list,\n",
        "    description=\"Lora5:\",\n",
        "    layout=lora_layout\n",
        ")\n",
        "\n",
        "lora_weights_scale5 = widgets.FloatText(\n",
        "    min=-2.0,\n",
        "    max=2.0,\n",
        "    step=0.01,\n",
        "    value=1,\n",
        "    #description=\"Lora scale5:\",\n",
        "    layout=lora_scale_layout\n",
        ")\n",
        "\n",
        "select_clip_skip = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Layer 2 Clip Skip',\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "freeu_check = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='FreeU',\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "# =====================================\n",
        "# Center\n",
        "# =====================================\n",
        "\n",
        "display_imgs = widgets.Output() # layout={'border': '1px solid black'}\n",
        "\n",
        "select_model = widgets.Dropdown(\n",
        "    options=model_list,\n",
        "    description=\"Model:\",\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "vae_model_dropdown = widgets.Dropdown(\n",
        "    options=vae_model_list,\n",
        "    description=\"VAE:\",\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "prompt = widgets.Textarea(\n",
        "    value=\"\",\n",
        "    placeholder=\"Enter prompt\",\n",
        "    rows=5,\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "neg_prompt = widgets.Textarea(\n",
        "    value=\"\",\n",
        "    placeholder=\"Enter negative prompt\",\n",
        "    rows=5,\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "active_ti = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Active Textual Inversion in prompt',\n",
        "    #style = style,\n",
        "    layout=auto_layout,\n",
        ")\n",
        "# alternative prompt weights\n",
        "weights_prompt = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Convert Prompt weights',\n",
        "    #style = style,\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "generate = widgets.Button(\n",
        "    description=\"Generate\",\n",
        "    disabled=False,\n",
        "    button_style=\"primary\",\n",
        "    layout=Layout(height='auto', width='auto'),\n",
        ")\n",
        "\n",
        "### GENERATE ###\n",
        "count_runs = 0\n",
        "def generate_img(i):\n",
        "  global model\n",
        "  #Clear output\n",
        "  display_imgs.clear_output()\n",
        "  generate.disabled = True\n",
        "\n",
        "  with display_imgs:\n",
        "\n",
        "    print(\"Running...\")\n",
        "\n",
        "\n",
        "    # First load\n",
        "    try:\n",
        "        model\n",
        "    except:\n",
        "        model = Model_Diffusers(base_model_id=select_model.value, task_name=select_task.value, vae_model = vae_model_dropdown.value, type_model_precision = model_precision.value)\n",
        "\n",
        "    model.load_pipe(select_model.value, task_name=select_task.value, vae_model = vae_model_dropdown.value, type_model_precision = model_precision.value)\n",
        "\n",
        "    display_imgs.clear_output()\n",
        "\n",
        "    try:\n",
        "      preprocessor_name_found = int_inputs[select_task.value][0].value\n",
        "    except:\n",
        "      preprocessor_name_found = None\n",
        "\n",
        "    global destination_path_cn_img, mask_control, image_list, count_runs\n",
        "\n",
        "    image_control_base = None\n",
        "    if select_task.value != \"txt2img\":\n",
        "        try:\n",
        "            image_control_base = destination_path_cn_img\n",
        "        except:\n",
        "            print(\"No control image found\")\n",
        "            generate.disabled = False\n",
        "            return\n",
        "\n",
        "    mask_control_base = None\n",
        "    if select_task.value == \"Inpaint\":\n",
        "        if os.path.exists(int_inputs['Inpaint'][1].value):\n",
        "            mask_control_base = int_inputs['Inpaint'][1].value\n",
        "        else:\n",
        "            try:\n",
        "                mask_control_base = mask_control\n",
        "            except:\n",
        "                print(\"No mask image found\")\n",
        "                generate.disabled = False\n",
        "                return\n",
        "\n",
        "    count_runs += 1\n",
        "    if not os.path.exists(\"/content/CodeFormer/\") and count_runs == 40:\n",
        "        print(\"Remember to use other cells, such as 'upscale', to avoid issues with disconnection due to inactivity.\")\n",
        "\n",
        "    adetailer_inpaint_params_A = {\n",
        "        \"prompt\": prompt_ad_A.value,\n",
        "        \"negative_prompt\" : negative_prompt_ad_A.value,\n",
        "        # \"prompt_embeds\" : prompt_embeds,\n",
        "        # \"negative_prompt_embeds\" : negative_prompt_embeds,\n",
        "        \"strength\" : strength_ad_A.value,\n",
        "        \"num_inference_steps\": steps.value,\n",
        "        \"height\": img_height.value,\n",
        "        \"width\": img_width.value,\n",
        "        \"guidance_scale\" : CFG.value,\n",
        "        \"controlnet_conditioning_scale\" : controlnet_output_scaling_in_unet.value,\n",
        "        \"control_guidance_start\" : controlnet_start_threshold.value,\n",
        "        \"control_guidance_end\" : controlnet_stop_threshold.value,\n",
        "    }\n",
        "    adetailer_params_A = {\n",
        "        \"face_detector_ad\" : face_detector_ad_A.value,\n",
        "        \"person_detector_ad\" : person_detector_ad_A.value,\n",
        "        \"hand_detector_ad\" : hand_detector_ad_A.value,\n",
        "        \"inpaint_only\" : adetailer_inpaint_params_A,\n",
        "        # \"image_list_task\" : None,\n",
        "        \"mask_dilation\" : mask_dilation_A.value,\n",
        "        \"mask_blur\" : mask_blur_A.value,\n",
        "        \"mask_padding\" : mask_padding_A.value,\n",
        "    }\n",
        "    pipe_params = {\n",
        "    \"prompt\": prompt.value,\n",
        "    \"negative_prompt\": neg_prompt.value,\n",
        "    \"img_height\": img_height.value,\n",
        "    \"img_width\": img_width.value,\n",
        "    \"num_images\": num_images.value,\n",
        "    \"num_steps\": steps.value,\n",
        "    \"guidance_scale\": CFG.value,\n",
        "    \"clip_skip\": select_clip_skip.value,\n",
        "    \"seed\": random_seed.value,\n",
        "    \"image\": image_control_base,\n",
        "    \"preprocessor_name\": preprocessor_name_found,\n",
        "    \"preprocess_resolution\": preprocess_resolution_global.value,\n",
        "    \"image_resolution\": image_resolution_global.value,\n",
        "    \"additional_prompt\": \"\",\n",
        "    \"image_mask\": mask_control_base, # only for Inpaint\n",
        "    \"strength\": int_inputs['Inpaint'][0].value, # only for Inpaint\n",
        "    \"low_threshold\": int_inputs['Canny'][0].value,\n",
        "    \"high_threshold\": int_inputs['Canny'][1].value,\n",
        "    \"value_threshold\": int_inputs['MLSD'][0].value,\n",
        "    \"distance_threshold\": int_inputs['MLSD'][1].value,\n",
        "    \"lora_A\": select_lora1.value,\n",
        "    \"lora_scale_A\": lora_weights_scale1.value,\n",
        "    \"lora_B\": select_lora2.value,\n",
        "    \"lora_scale_B\": lora_weights_scale2.value,\n",
        "    \"lora_C\": select_lora3.value,\n",
        "    \"lora_scale_C\": lora_weights_scale3.value,\n",
        "    \"lora_D\": select_lora4.value,\n",
        "    \"lora_scale_D\": lora_weights_scale4.value,\n",
        "    \"lora_E\": select_lora5.value,\n",
        "    \"lora_scale_E\": lora_weights_scale5.value,\n",
        "    \"active_textual_inversion\": active_ti.value,\n",
        "    \"textual_inversion\": embed_list,\n",
        "    \"convert_weights_prompt\": weights_prompt.value,\n",
        "    \"sampler\": select_sampler.value,\n",
        "    \"xformers_memory_efficient_attention\": xformers_memory_efficient_attention.value,\n",
        "    \"gui_active\": True,\n",
        "    \"loop_generation\": loop_generator.value,\n",
        "    \"controlnet_conditioning_scale\" : controlnet_output_scaling_in_unet.value,\n",
        "    \"control_guidance_start\" : controlnet_start_threshold.value,\n",
        "    \"control_guidance_end\" : controlnet_stop_threshold.value,\n",
        "    \"generator_in_cpu\" : init_generator_in_cpu.value,\n",
        "    \"FreeU\" : freeu_check.value,\n",
        "    \"adetailer_active\" : adetailer_active_A.value,\n",
        "    \"adetailer_params\" : adetailer_params_A,\n",
        "    \"leave_progress_bar\" : True,\n",
        "    \"disable_progress_bar\" : disable_progress_bar_check.value,\n",
        "    \"image_previews\" : False,\n",
        "    \"upscaler_model_path\" : None,\n",
        "    \"upscaler_increases_size\" : 1.5,\n",
        "    }\n",
        "\n",
        "    images, image_list = model(**pipe_params)\n",
        "\n",
        "    if loop_generator.value == 1:\n",
        "        mediapy.show_images(images)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "  generate.disabled = False\n",
        "  return\n",
        "\n",
        "generate.on_click(generate_img)\n",
        "\n",
        "show_textual_inversion = widgets.Button(\n",
        "    description=\"List available textual inversions\",\n",
        "    disabled=False,\n",
        "    button_style=\"info\",\n",
        "    layout=widgets.Layout(height='auto', width='auto'),\n",
        ")\n",
        "\n",
        "def elemets_textual_inversion(value):\n",
        "  with display_imgs:\n",
        "    print('Write the word in the prompt for use and \"Active Textual Inversion in the prompt\"')\n",
        "    print('The currently supported embeddings are as follows:')\n",
        "    for name, directory_name in embed_list:\n",
        "        print(f'\\033[34m {name} \\033[0m')\n",
        "    return\n",
        "\n",
        "show_textual_inversion.on_click(elemets_textual_inversion)\n",
        "\n",
        "clear_outputs = widgets.Button(\n",
        "    description=\"Clear outputs\",\n",
        "    disabled=False,\n",
        "    button_style=\"info\",\n",
        "    layout=widgets.Layout(height='auto', width='auto'),\n",
        "\n",
        ")\n",
        "\n",
        "def clear_outputs_run(value):\n",
        "  display_imgs.clear_output()\n",
        "  return\n",
        "\n",
        "clear_outputs.on_click(clear_outputs_run)\n",
        "\n",
        "# =====================================\n",
        "# Right ControlNet\n",
        "# =====================================\n",
        "\n",
        "control_model_list = list(CONTROLNET_MODEL_IDS.keys())\n",
        "\n",
        "# Create a Dropdown for selecting options\n",
        "select_task = widgets.Dropdown( #\n",
        "    options=[\n",
        "        control_model_list[13],\n",
        "        control_model_list[12],\n",
        "        control_model_list[0],\n",
        "        control_model_list[1],\n",
        "        control_model_list[2],\n",
        "        control_model_list[3],\n",
        "        control_model_list[4],\n",
        "        control_model_list[5],\n",
        "        control_model_list[6],\n",
        "        control_model_list[7],\n",
        "        control_model_list[8],\n",
        "        control_model_list[10],\n",
        "        control_model_list[11],\n",
        "    ],\n",
        "    description='TASK:',\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "controlnet_output_scaling_in_unet = widgets.FloatText(value=1.0, min=0.0, max=5.0, step=0.1, description='ControlNet Output Scaling in UNet:', style=style)\n",
        "controlnet_start_threshold = widgets.FloatSlider(value=0.0, min=0.00, max=1.0, step=0.01, description='ControlNet Start Threshold (%):', style=style)\n",
        "controlnet_stop_threshold = widgets.FloatSlider(value=1.0, min=0.00, max=1.0, step=0.01, description='ControlNet Stop Threshold (%):', style=style)\n",
        "\n",
        "preprocess_resolution_global = widgets.IntSlider(\n",
        "    value=512,\n",
        "    min=64,\n",
        "    max=2048,\n",
        "    description='Preprocessor resolution',\n",
        "    style=style,\n",
        ")\n",
        "\n",
        "image_resolution_global = widgets.IntSlider(\n",
        "    value=512,\n",
        "    min=64,\n",
        "    step=64,\n",
        "    max=2048,\n",
        "    description='Image resolution',\n",
        "    style=style,\n",
        ")\n",
        "\n",
        "# Create a dictionary to map options to lists of IntText widgets\n",
        "int_inputs = {\n",
        "\n",
        "    control_model_list[13]: [\n",
        "    ],\n",
        "    control_model_list[12]: [\n",
        "        widgets.FloatSlider(value=1.0, min=0.01, max=1.0, step=0.01, description='Inpaint strength:', layout=Layout(visibility='hidden'), style=style),\n",
        "        widgets.Text(value=\"\", placeholder=\"/content/my_mask.png\", rows=1, description='Mask path:', layout=Layout(visibility='hidden'), style=style)\n",
        "    ],\n",
        "    control_model_list[0]: [\n",
        "        widgets.Dropdown(value='Openpose', description='Preprocessor:', options=['None','Openpose'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[1]: [\n",
        "        widgets.BoundedIntText(value=100, min=1, max=255, description='Canny low threshold:', layout=Layout(visibility='hidden'), style=style),\n",
        "        widgets.BoundedIntText(value=200, min=1, max=255, description='Canny high threshold:', layout=Layout(visibility='hidden'), style=style)\n",
        "    ],\n",
        "    control_model_list[2]: [\n",
        "        widgets.BoundedFloatText(value=0.1, min=1, max=2.0, step=0.01, description='Hough value threshold (MLSD):', layout=Layout(visibility='hidden'), style=style),\n",
        "        widgets.BoundedFloatText(value=0.1, min=1, max=20.0, step=0.01, description='Hough distance threshold (MLSD):', layout=Layout(visibility='hidden'), style=style)\n",
        "    ],\n",
        "    control_model_list[3]: [\n",
        "        widgets.Dropdown(value='HED', description='Preprocessor:', options=['HED','PidiNet', 'None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[4]: [\n",
        "        widgets.Dropdown(value='PidiNet', description='Preprocessor:', options=['HED','PidiNet', 'HED safe', 'PidiNet safe','None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[5]: [\n",
        "        widgets.Dropdown(value='UPerNet', description='Preprocessor:', options=['UPerNet','None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[6]: [\n",
        "        widgets.Dropdown(value='DPT', description='Preprocessor:', options=['Midas', 'DPT','None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[7]: [\n",
        "        widgets.Dropdown(value='NormalBae', description='Preprocessor:', options=['NormalBae','None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[8]: [\n",
        "        widgets.Dropdown(value='Lineart', description='Preprocessor:', options=['Lineart','Lineart coarse', 'None', 'Lineart (anime)', 'None (anime)'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[10]: [\n",
        "        widgets.Dropdown(value='ContentShuffle', description='Preprocessor:', options=['ContentShuffle','None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Function to update visibility and enable/disable state of widgets\n",
        "def update_widgets(option):\n",
        "    for opt, int_inputs_list in int_inputs.items():\n",
        "        if opt == option:\n",
        "            for int_input in int_inputs_list:\n",
        "                int_input.layout.visibility = 'visible'\n",
        "        else:\n",
        "            for int_input in int_inputs_list:\n",
        "                int_input.layout.visibility = 'hidden'\n",
        "\n",
        "interactive(update_widgets, option=select_task)\n",
        "\n",
        "# =====================================\n",
        "# Adetailer A\n",
        "# =====================================\n",
        "\n",
        "adetailer_active_A =  widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Enable Adetailer A',\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "prompt_ad_A = widgets.Textarea(\n",
        "    value=\"\",\n",
        "    placeholder=\"Main prompt will be use\",\n",
        "    rows=3,\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "negative_prompt_ad_A = widgets.Textarea(\n",
        "    value=\"\",\n",
        "    placeholder=\"Main negative prompt will be use\",\n",
        "    rows=3,\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "strength_ad_A = widgets.FloatSlider(\n",
        "    value=0.4,\n",
        "    min=0.01,\n",
        "    max=1.0,\n",
        "    step=0.01,\n",
        "    description='Inpaint strength:',\n",
        "    layout=widgets.Layout(width=width),\n",
        "    style=style\n",
        ")\n",
        "\n",
        "face_detector_ad_A = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Face detector',\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "person_detector_ad_A = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Person detector',\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "hand_detector_ad_A = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Hand detector',\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "mask_dilation_A = widgets.BoundedIntText(\n",
        "    value = 4,\n",
        "    min = 1,\n",
        "    description=\"Mask dilation:\",\n",
        "    layout=widgets.Layout(width=width)\n",
        ")\n",
        "\n",
        "mask_blur_A = widgets.BoundedIntText(\n",
        "    value = 4,\n",
        "    min = 1,\n",
        "    description=\"Mask blur:\",\n",
        "    layout=widgets.Layout(width=width)\n",
        ")\n",
        "\n",
        "mask_padding_A = widgets.BoundedIntText(\n",
        "    value = 32,\n",
        "    min = 1,\n",
        "    description=\"Mask padding:\",\n",
        "    layout=widgets.Layout(width=width)\n",
        ")\n",
        "\n",
        "# =====================================\n",
        "# Settings\n",
        "# =====================================\n",
        "loop_generator = widgets.IntSlider(\n",
        "    value=1,\n",
        "    min=1,\n",
        "    max=10,\n",
        "    description='Loops üîÅ',\n",
        "    #layout=auto_layout,\n",
        "    style = style,\n",
        ")\n",
        "\n",
        "disable_progress_bar_check = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Disable progress bar',\n",
        "    layout=auto_layout,\n",
        "    style = style,\n",
        ")\n",
        "\n",
        "xformers_memory_efficient_attention = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Xformers memory efficient attention',\n",
        "    layout=auto_layout,\n",
        "    style = style,\n",
        ")\n",
        "\n",
        "model_precision = widgets.RadioButtons(\n",
        "    options=[(\"float16\", torch.float16),(\"float32\", torch.float32)],\n",
        "    description=\"Model precision (float32 need more memory):\",\n",
        "    layout=auto_layout,\n",
        "    style = style,\n",
        ")\n",
        "\n",
        "init_generator_in_cpu = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Generate initial random noise with the seed on the CPU',\n",
        "    layout=auto_layout,\n",
        "    style = style,\n",
        ")\n",
        "# =====================================\n",
        "# App\n",
        "# =====================================\n",
        "\n",
        "title_tab_one = widgets.HTML(\n",
        "    value=\"<h2>SD Interactive</h2>\",\n",
        "    layout=widgets.Layout(display=\"flex\", justify_content=\"center\")\n",
        ")\n",
        "title_tab_two = widgets.HTML(\n",
        "    value=\"<h2>Settings</h2>\",\n",
        "    layout=widgets.Layout(display=\"flex\", justify_content=\"center\")\n",
        ")\n",
        "\n",
        "buttons_ = TwoByTwoLayout(top_left=generate,\n",
        "               top_right=show_textual_inversion,\n",
        "               #bottom_left=None,\n",
        "               bottom_right=clear_outputs, merge=True)\n",
        "\n",
        "show_textual_inversion.style.button_color = '#97BC62'\n",
        "clear_outputs.style.button_color = '#97BC62'\n",
        "generate.style.button_color = '#2C5F2D'\n",
        "\n",
        "# TAB 1\n",
        "tab_sd = widgets.VBox(\n",
        "    [\n",
        "      widgets.AppLayout(\n",
        "        header=None,\n",
        "        left_sidebar=widgets.VBox(\n",
        "            [\n",
        "                title_tab_one,\n",
        "                num_images,\n",
        "                steps,\n",
        "                CFG,\n",
        "                select_sampler,\n",
        "                img_width,\n",
        "                img_height,\n",
        "                random_seed,\n",
        "                widgets.HBox([select_lora1, lora_weights_scale1],layout=widgets.Layout(width=width)),\n",
        "                widgets.HBox([select_lora2, lora_weights_scale2],layout=widgets.Layout(width=width)),\n",
        "                widgets.HBox([select_lora3, lora_weights_scale3],layout=widgets.Layout(width=width)),\n",
        "                widgets.HBox([select_lora4, lora_weights_scale4],layout=widgets.Layout(width=width)),\n",
        "                widgets.HBox([select_lora5, lora_weights_scale5],layout=widgets.Layout(width=width)),\n",
        "                select_clip_skip,\n",
        "                freeu_check,\n",
        "            ]\n",
        "        ),\n",
        "        center=widgets.VBox(\n",
        "            [\n",
        "                select_task,\n",
        "                select_model,\n",
        "                vae_model_dropdown,\n",
        "                prompt,\n",
        "                neg_prompt,\n",
        "                widgets.HBox([weights_prompt ,active_ti]),\n",
        "                buttons_,\n",
        "            ]\n",
        "        ),\n",
        "        right_sidebar=widgets.VBox(\n",
        "            [\n",
        "                controlnet_output_scaling_in_unet,\n",
        "                controlnet_start_threshold,\n",
        "                controlnet_stop_threshold,\n",
        "            ] +\n",
        "            [preprocess_resolution_global] + [image_resolution_global] + [int_input for int_inputs_list in int_inputs.values() for int_input in int_inputs_list]\n",
        "        ),\n",
        "        footer=None,\n",
        "        pane_widths=[1.1, 1.4, 1.2],\n",
        "        # pane_heights=[\"0px\", 1, '0px'],\n",
        "      ),\n",
        "      display_imgs,\n",
        "    ]\n",
        ")\n",
        "\n",
        "# TAB 2\n",
        "tab_adetailer = widgets.VBox([\n",
        "    widgets.Label(value=\"For better performance, use 'Disable progress bar' in settings.\"),\n",
        "    adetailer_active_A,\n",
        "    prompt_ad_A,\n",
        "    negative_prompt_ad_A,\n",
        "    strength_ad_A,\n",
        "    face_detector_ad_A,\n",
        "    person_detector_ad_A,\n",
        "    hand_detector_ad_A,\n",
        "    mask_dilation_A,\n",
        "    mask_blur_A,\n",
        "    mask_padding_A,\n",
        "])\n",
        "\n",
        "# TAB 3\n",
        "tab_settings = widgets.VBox([\n",
        "    title_tab_two,\n",
        "    loop_generator,\n",
        "    disable_progress_bar_check,\n",
        "    xformers_memory_efficient_attention,\n",
        "    init_generator_in_cpu,\n",
        "    model_precision,\n",
        "    widgets.HTML(\n",
        "        value=\"<p>‚è≤Ô∏è</p>\",\n",
        "        layout=widgets.Layout(display=\"flex\", justify_content=\"center\")\n",
        "    ),\n",
        "])\n",
        "\n",
        "\n",
        "# APP\n",
        "tab = widgets.Tab()\n",
        "tab.children = [\n",
        "  widgets.VBox(\n",
        "    children = [\n",
        "        tab_sd,\n",
        "    ]),\n",
        "  widgets.VBox(\n",
        "    children = [\n",
        "        tab_adetailer,\n",
        "    ]),\n",
        "  widgets.VBox(\n",
        "    children = [\n",
        "        tab_settings,\n",
        "    ]),\n",
        "]\n",
        "\n",
        "tab.set_title(0, \"Stable Diffusion\")\n",
        "tab.set_title(1,  \"Adetailer\")\n",
        "tab.set_title(2,  \"Settings\")\n",
        "tab.selected_index = 0\n",
        "\n",
        "display(tab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "D-L_EepAsLPE"
      },
      "outputs": [],
      "source": [
        "#@title Upload an image here for use in Inpainting or ControlNet. üëà‚Äç‚Äç üñºÔ∏èüñºÔ∏èüñºÔ∏è\n",
        "#@markdown - To use Controlnet, you need to upload the control image with this cell\n",
        "Create_mask_for_Inpaint = False # @param {type:\"boolean\"}\n",
        "stroke_width = 24 # @param {type:\"integer\"}\n",
        "from google.colab import files\n",
        "from IPython.display import HTML\n",
        "import os, shutil, base64\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%cd /content\n",
        "uploaded = files.upload()\n",
        "\n",
        "filename = next(iter(uploaded))\n",
        "print(f'Uploaded file: {filename}')\n",
        "\n",
        "upload_folder = 'uploaded_controlnet_image/'\n",
        "if not os.path.exists(upload_folder):\n",
        "    os.makedirs(upload_folder)\n",
        "\n",
        "source_path = filename\n",
        "destination_path_cn_img = os.path.join(upload_folder, filename)\n",
        "shutil.move(source_path, destination_path_cn_img)\n",
        "print(f'Moved file to {destination_path_cn_img}')\n",
        "\n",
        "if Create_mask_for_Inpaint:\n",
        "    init_image = destination_path_cn_img\n",
        "    name_without_extension = os.path.splitext(init_image.split('/')[-1])[0]\n",
        "\n",
        "    image64 = base64.b64encode(open(init_image, 'rb').read())\n",
        "    image64 = image64.decode('utf-8')\n",
        "\n",
        "    print('\\033[34m Draw the mask with the mouse \\033[0m')\n",
        "    img = np.array(plt.imread(f'{init_image}')[:,:,:3])\n",
        "\n",
        "    draw(image64, filename=f\"./{name_without_extension}_draw.png\", w=img.shape[1], h=img.shape[0], line_width=stroke_width)\n",
        "\n",
        "    with_mask = np.array(plt.imread(f\"./{name_without_extension}_draw.png\")[:,:,:3])\n",
        "    mask = (with_mask[:,:,0]==1)*(with_mask[:,:,1]==0)*(with_mask[:,:,2]==0)\n",
        "    plt.imsave(f\"./{name_without_extension}_mask.png\",mask, cmap='gray')\n",
        "    mask_control = f\"./{name_without_extension}_mask.png\"\n",
        "    print(f'\\033[34m Mask saved: {mask_control} \\033[0m')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Show the last generated image\n",
        "for i in image_list:\n",
        "    mediapy.show_images([Image.open(i)])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wQShTIUGVfLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__21H6ZLUokz"
      },
      "outputs": [],
      "source": [
        "#@title Upscale and face restoration { form-width: \"20%\", display-mode: \"form\" }\n",
        "from IPython.utils import capture\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "%cd /content\n",
        "directory_codeformer = '/content/CodeFormer/'\n",
        "with capture.capture_output() as cap:\n",
        "  if not os.path.exists(directory_codeformer):\n",
        "      os.makedirs(directory_codeformer)\n",
        "\n",
        "      # Setup\n",
        "      # Clone CodeFormer and enter the CodeFormer folder\n",
        "      %cd /content\n",
        "      !git clone https://github.com/sczhou/CodeFormer.git\n",
        "      %cd CodeFormer\n",
        "\n",
        "\n",
        "      # Set up the environment\n",
        "      # Install python dependencies\n",
        "      !pip install -q -r requirements.txt\n",
        "      !pip -q install ffmpeg\n",
        "      # Install basicsr\n",
        "      !python basicsr/setup.py develop\n",
        "\n",
        "      # Download the pre-trained model\n",
        "      !python scripts/download_pretrained_models.py facelib\n",
        "      !python scripts/download_pretrained_models.py CodeFormer\n",
        "  del cap\n",
        "# Visualization function\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "def display_codeformer(img1, img2):\n",
        "  fig = plt.figure(figsize=(25, 10))\n",
        "  ax1 = fig.add_subplot(1, 2, 1)\n",
        "  plt.title('Input', fontsize=16)\n",
        "  ax1.axis('off')\n",
        "  ax2 = fig.add_subplot(1, 2, 2)\n",
        "  plt.title('CodeFormer', fontsize=16)\n",
        "  ax2.axis('off')\n",
        "  ax1.imshow(img1)\n",
        "  ax2.imshow(img2)\n",
        "def imread(img_path):\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  return img\n",
        "\n",
        "# Copy imgs\n",
        "Select_an_image = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# PROCESS AD\n",
        "if os.path.exists(Select_an_image.strip()):\n",
        "    image_list = [Select_an_image.replace('/content/', '').strip()]\n",
        "\n",
        "destination_directory = '/content/CodeFormer/inputs/user_upload'\n",
        "!rm -rf /content/CodeFormer/inputs/user_upload/*\n",
        "os.makedirs(destination_directory, exist_ok=True)\n",
        "for image_path in image_list:\n",
        "    image_filename = os.path.basename('/content/'+image_path)\n",
        "    destination_path = os.path.join(destination_directory, image_filename)\n",
        "    try:\n",
        "        shutil.copyfile('/content/'+image_path, destination_path)\n",
        "        print(f\"Image '{image_filename}' has been copied to '{destination_path}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to copy '{image_filename}' to '{destination_path}': {e}\")\n",
        "\n",
        "#@markdown `CODEFORMER_FIDELITY`: Balance the quality (lower number) and fidelity (higher number)<br>\n",
        "# you can add '--bg_upsampler realesrgan' to enhance the background\n",
        "CODEFORMER_FIDELITY = 0.7 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown `BACKGROUND_ENHANCE`: Enhance background image with Real-ESRGAN<br>\n",
        "BACKGROUND_ENHANCE = True #@param {type:\"boolean\"}\n",
        "#@markdown `FACE_UPSAMPLE`: Upsample restored faces for high-resolution AI-created images<br>\n",
        "FACE_UPSAMPLE = False #@param {type:\"boolean\"}\n",
        "#markdown `HAS_ALIGNED`: Input are cropped and aligned faces<br>\n",
        "HAS_ALIGNED =  False\n",
        "#@markdown `UPSCALE`: The final upsampling scale of the image. Default: 2<br>\n",
        "UPSCALE = 3 #@param {type:\"slider\", min:2, max:8, step:1}\n",
        "#markdown `DETECTION_MODEL`: Face detector. Default: retinaface_resnet50<br>\n",
        "DETECTION_MODEL = \"retinaface_resnet50\"\n",
        "#markdown `DRAW_BOX`: Draw the bounding box for the detected faces.\n",
        "DRAW_BOX = False\n",
        "\n",
        "BACKGROUND_ENHANCE = '--bg_upsampler realesrgan' if BACKGROUND_ENHANCE else ''\n",
        "FACE_UPSAMPLE = '--face_upsample' if FACE_UPSAMPLE else ''\n",
        "HAS_ALIGNED = '--has_aligned' if HAS_ALIGNED else ''\n",
        "DRAW_BOX = '--draw_box' if DRAW_BOX else ''\n",
        "%cd CodeFormer\n",
        "!python inference_codeformer.py -w $CODEFORMER_FIDELITY --input_path {destination_directory} {BACKGROUND_ENHANCE} {FACE_UPSAMPLE} {HAS_ALIGNED} --upscale {UPSCALE} --detection_model {DETECTION_MODEL} {DRAW_BOX}\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "input_folder = 'inputs/user_upload'\n",
        "result_folder = f'results/user_upload_{CODEFORMER_FIDELITY}/final_results'\n",
        "input_list = sorted(glob.glob(os.path.join(input_folder, '*')))\n",
        "for input_path in input_list:\n",
        "  img_input = imread(input_path)\n",
        "  basename = os.path.splitext(os.path.basename(input_path))[0]\n",
        "  output_path = os.path.join(result_folder, basename+'.png')\n",
        "  img_output = imread(output_path)\n",
        "  display_codeformer(img_input, img_output)\n",
        "\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFbbS2mdf7Al"
      },
      "source": [
        "Upscale results in /content/CodeFormer/results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Wlmke1VJPWS3"
      },
      "outputs": [],
      "source": [
        "#@title Download Images\n",
        "import os\n",
        "from google.colab import files\n",
        "!rm /content/results.zip\n",
        "!ls /content/images\n",
        "print('Download results')\n",
        "os.system(f'zip -r results.zip /content/images')\n",
        "try:\n",
        "  files.download(\"results.zip\")\n",
        "except:\n",
        "  print(\"Error\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "k-FPYmh2Wb4v"
      },
      "outputs": [],
      "source": [
        "#@title Download Upscale results\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "%cd /content/CodeFormer\n",
        "!ls results\n",
        "print('Download results')\n",
        "os.system(f'zip -r results.zip results/user_upload_{CODEFORMER_FIDELITY}/final_results')\n",
        "try:\n",
        "  files.download(\"results.zip\")\n",
        "except:\n",
        "  files.download(f'/content/CodeFormer/results/{filename[:-4]}_{CODEFORMER_FIDELITY}/{filename}')\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g83HiPS2mSqJ"
      },
      "source": [
        "# Extras for advanced users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHxjQLbQ--O1"
      },
      "outputs": [],
      "source": [
        "# You can also use this cell to simply reload the model in case you need to.\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If the generate button is disabled\n",
        "generate.disabled = False"
      ],
      "metadata": {
        "id": "mCTsZIewscvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht5-ztYhXrkm"
      },
      "outputs": [],
      "source": [
        "# If you encounter a problem with \"Out Of Memory,\" you can click on this cell and rerun cells 2 and 3.\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BjBOzTfY9FDA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "upscaler_dict = {\n",
        "    \"RealESRGAN_x4plus\" : \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth\",\n",
        "    \"RealESRNet_x4plus\" : \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.1/RealESRNet_x4plus.pth\",\n",
        "    \"RealESRGAN_x4plus_anime_6B\": \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth\",\n",
        "    \"RealESRGAN_x2plus\": \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth\",\n",
        "    \"realesr-animevideov3\": \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-animevideov3.pth\",\n",
        "    \"realesr-general-x4v3\": \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth\",\n",
        "    \"realesr-general-wdn-x4v3\" : \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-wdn-x4v3.pth\",\n",
        "    \"4x-UltraSharp\" : \"https://huggingface.co/Shandypur/ESRGAN-4x-UltraSharp/resolve/main/4x-UltraSharp.pth\",\n",
        "    \"4x_foolhardy_Remacri\" : \"https://huggingface.co/FacehugmanIII/4x_foolhardy_Remacri/resolve/main/4x_foolhardy_Remacri.pth\",\n",
        "    \"Remacri4xExtraSmoother\" : \"https://huggingface.co/hollowstrawberry/upscalers-backup/resolve/main/ESRGAN/Remacri%204x%20ExtraSmoother.pth\",\n",
        "    \"AnimeSharp4x\" : \"https://huggingface.co/hollowstrawberry/upscalers-backup/resolve/main/ESRGAN/AnimeSharp%204x.pth\",\n",
        "    \"lollypop\" : \"https://huggingface.co/hollowstrawberry/upscalers-backup/resolve/main/ESRGAN/lollypop.pth\",\n",
        "    \"RealisticRescaler4x\" : \"https://huggingface.co/hollowstrawberry/upscalers-backup/resolve/main/ESRGAN/RealisticRescaler%204x.pth\",\n",
        "    \"NickelbackFS4x\" : \"https://huggingface.co/hollowstrawberry/upscalers-backup/resolve/main/ESRGAN/NickelbackFS%204x.pth\"\n",
        "}\n",
        "\n",
        "#@markdown # Alternative Upscaler Tool\n",
        "#@markdown You can leave `Select_an_image` blank to process the last generated images.\n",
        "Select_an_image = \"\" # @param {type:\"string\"}\n",
        "MODEL_UPSCALER = \"RealESRGAN_x4plus_anime_6B\" #@param [\"RealESRGAN_x4plus\", \"RealESRNet_x4plus\", \"RealESRGAN_x2plus\", \"RealESRGAN_x4plus_anime_6B\", \"realesr-animevideov3\", \"realesr-general-x4v3\", \"realesr-general-wdn-x4v3\", \"4x-UltraSharp\", \"4x_foolhardy_Remacri\", \"Remacri4xExtraSmoother\", \"AnimeSharp4x\", \"lollypop\", \"RealisticRescaler4x\", \"NickelbackFS4x\"]\n",
        "Scale_of_the_image_x = 1.5 #@param {type:\"slider\", min:1, max:4, step:0.5}\n",
        "show_result = True #@param {type: \"boolean\"}\n",
        "\n",
        "directory_upscalers = 'upscalers'\n",
        "os.makedirs(directory_upscalers, exist_ok=True)\n",
        "\n",
        "url_upscaler = upscaler_dict[MODEL_UPSCALER]\n",
        "\n",
        "if not os.path.exists(f\"./upscalers/{url_upscaler.split('/')[-1]}\"):\n",
        "    download_things(directory_upscalers, url_upscaler, hf_token)\n",
        "\n",
        "scaler_beta = UpscalerESRGAN()\n",
        "\n",
        "if os.path.exists(Select_an_image.strip()):\n",
        "    image_list = [Select_an_image.replace('/content/', '').strip()]\n",
        "\n",
        "for img_base in image_list:\n",
        "    img_pil = Image.open(img_base)\n",
        "    img_up = scaler_beta.upscale(img_pil, Scale_of_the_image_x , f\"./upscalers/{url_upscaler.split('/')[-1]}\")\n",
        "\n",
        "    if show_result:\n",
        "        display(img_up)\n",
        "\n",
        "    image_path = save_pil_image_with_metadata(img_up, f'{os.getcwd()}/up_images', metadata_list=None)\n",
        "    print(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CqH-rX3VVCg7"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "import gc\n",
        "from asdff.sd import AdCnPreloadPipe\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "adetailer = AdCnPreloadPipe(model.pipe) # use the loaded sampler\n",
        "\n",
        "# try:\n",
        "#     adetailer.inpaint_pipeline.scheduler = DPMSolverSinglestepScheduler.from_config(adetailer.inpaint_pipeline.scheduler.config, use_karras_sigmas=True)\n",
        "# except:\n",
        "#     adetailer.inpaint_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(adetailer.inpaint_pipeline.scheduler.config)\n",
        "\n",
        "# OPTIONS #\n",
        "# @markdown # Adetailer\n",
        "# @markdown This will use the parameters and the loaded model of the GUI.\n",
        "Select_an_image = \"\" # @param {type:\"string\"}\n",
        "prompt_ad = \"masterpiece, best quality, extremely detailed wallpaper\" # @param {type:\"string\"}\n",
        "negative_prompt_ad = \"worst quality, normal quality, low quality, low res, blurry, text, watermark, logo, banner, extra digits, cropped, jpeg artifacts, signature, username, error, sketch ,duplicate, ugly, monochrome, horror, geometry, mutation, disgusting\" # @param {type:\"string\"}\n",
        "strength_ad = 0.4 # @param {type:\"number\"}\n",
        "face_detector_ad = True # @param {type:\"boolean\"}\n",
        "person_detector_ad = True # @param {type:\"boolean\"}\n",
        "hand_detector_ad = False # @param {type:\"boolean\"}\n",
        "mask_dilation = 4 # @param {type:\"integer\"}\n",
        "mask_blur = 4 # @param {type:\"integer\"}\n",
        "mask_padding = 32 # @param {type:\"integer\"}\n",
        "\n",
        "adetailer_inpaint_params = {\n",
        "    \"prompt\": prompt_ad,\n",
        "    \"negative_prompt\" : negative_prompt_ad,\n",
        "    # \"prompt_embeds\" : prompt_embeds,\n",
        "    # \"negative_prompt_embeds\" : negative_prompt_embeds,\n",
        "    \"strength\" : strength_ad,\n",
        "    \"num_inference_steps\": steps.value,\n",
        "    \"height\": img_height.value,\n",
        "    \"width\": img_width.value,\n",
        "    \"guidance_scale\" : CFG.value,\n",
        "    \"controlnet_conditioning_scale\" : controlnet_output_scaling_in_unet.value,\n",
        "    \"control_guidance_start\" : controlnet_start_threshold.value,\n",
        "    \"control_guidance_end\" : controlnet_stop_threshold.value,\n",
        "}\n",
        "adetailer_params = {\n",
        "    \"face_detector_ad\" : face_detector_ad,\n",
        "    \"person_detector_ad\" : person_detector_ad,\n",
        "    \"hand_detector_ad\" : hand_detector_ad,\n",
        "    \"inpaint_only\" : adetailer_inpaint_params,\n",
        "    # \"image_list_task\" : None,\n",
        "    \"mask_dilation\" : mask_dilation,\n",
        "    \"mask_blur\" : mask_blur,\n",
        "    \"mask_padding\" : mask_padding,\n",
        "}\n",
        "\n",
        "\n",
        "# PROCESS AD\n",
        "if os.path.exists(Select_an_image):\n",
        "    image_list = [Select_an_image]\n",
        "\n",
        "image_pil_list = []\n",
        "for path in image_list:\n",
        "    if os.path.exists(path):\n",
        "        # Open the image using PIL and convert it to PIL.Image.Image\n",
        "        with Image.open(path) as img:\n",
        "            image_ad = img.convert(\"RGB\")\n",
        "            image_pil_list.append(image_ad)\n",
        "\n",
        "ad_pil_list = ad_model_process(\n",
        "    adetailer = adetailer,\n",
        "    image_list_task = image_pil_list,\n",
        "    **adetailer_params,\n",
        ")\n",
        "\n",
        "for img_ad in ad_pil_list:\n",
        "    display(img_ad)\n",
        "    image_path = save_pil_image_with_metadata(img_ad, f'{os.getcwd()}/images', metadata_list=None)\n",
        "    print(image_path)\n",
        "\n",
        "del adetailer\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HCWT08a6VJtH"
      },
      "outputs": [],
      "source": [
        "# @markdown CONVERT SAFETENSORS TO DIFFUSERS for SD 1.5\n",
        "path_safetensor_model = \"\" # @param {type:\"string\"}\n",
        "path_diffusers_model = \"./converted_model/\" # @param {type:\"string\"}\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "pipe = StableDiffusionPipeline.from_single_file(path_safetensor_model).to(\"cuda\")\n",
        "pipe.save_pretrained(path_diffusers_model) # model path inpaint is ./adetailer_model/\n",
        "del pipe\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3zWvKUq0WkD"
      },
      "source": [
        "Utility:\n",
        "\n",
        "Convert [SDXL TO DIFFUSERS](https://github.com/Linaqruf/sdxl-model-converter)\n",
        "\n",
        "If you are a mobile user, you can use this [space](https://huggingface.co/spaces/r3gm/inpaint-mask-maker) to create the mask for the inpaint."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "g83HiPS2mSqJ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}