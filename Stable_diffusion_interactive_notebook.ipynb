{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R3gm/SD_diffusers_interactive/blob/main/Stable_diffusion_interactive_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stable Diffusion Interactive Notebook üìì ü§ñ"
      ],
      "metadata": {
        "id": "kZl9FPC9sbW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Description | Link |\n",
        "| ----------- | ---- |\n",
        "| üéâ Repository | [![GitHub Repository](https://img.shields.io/github/stars/R3gm/SD_diffusers_interactive?style=social)](https://github.com/R3gm/SD_diffusers_interactive) |\n"
      ],
      "metadata": {
        "id": "8MRWnnDNp4Mh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Compel added for SD 1.5 (prompt weights)\n",
        "- Controlnet 1.1 for SD\n",
        "- SDXL models only support txt2img\n",
        "- Lora usually doesn't work correctly alongside Controlnet\n",
        "- More functions, more bugs; less than 10 words, more laughs\n"
      ],
      "metadata": {
        "id": "e8lnApcK4vWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "A widgets-based interactive notebook for Google Colab that lets users generate AI images from prompts (Text2Image) using [Stable Diffusion (by Stability AI, Runway & CompVis)](https://en.wikipedia.org/wiki/Stable_Diffusion).\n",
        "\n",
        "This notebook aims to be an alternative to WebUIs while offering a simple and lightweight GUI for `anyone to get started with Stable Diffusion`.\n",
        "\n",
        "Uses Stable Diffusion, [HuggingFace](https://huggingface.co/) Diffusers and [Jupyter widgets](https://github.com/jupyter-widgets/ipywidgets).\n",
        "\n",
        "<br/>\n",
        "\n",
        "Based on redromnon's repository\n",
        "\n",
        "[Original GitHub](https://github.com/redromnon/stable-diffusion-interactive-notebook)"
      ],
      "metadata": {
        "id": "wILzWiWRfTX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üëá Installing dependencies { display-mode: \"form\" }\n",
        "#@markdown ---\n",
        "#@markdown Make sure to select **GPU** as the runtime type:<br/>\n",
        "#@markdown *Runtime->Change Runtime Type->Under Hardware accelerator, select GPU*\n",
        "#@markdown\n",
        "#@markdown ---\n",
        "\n",
        "!pip install -q omegaconf==2.3.0 torch git+https://github.com/huggingface/diffusers.git git+https://github.com/damian0815/compel.git invisible_watermark  transformers accelerate scipy safetensors==0.3.3 xformers safetensors mediapy ipywidgets==7.7.1 controlnet_aux==0.0.6 mediapipe==0.10.1 pytorch-lightning asdff\n",
        "\n",
        "!apt install git-lfs\n",
        "!git lfs install\n",
        "!apt -y install -qq aria2"
      ],
      "metadata": {
        "id": "A04WkRDwGpLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`RESTART THE RUNTIME` before executing the next cell."
      ],
      "metadata": {
        "id": "XbCVq0rCrGPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üëá Download Model: Please provide a link for the Civitai API, Google Drive, or Hugging Face. { form-width: \"20%\", display-mode: \"form\" }\n",
        "import os\n",
        "%cd /content\n",
        "\n",
        "def download_things(directory, url, hf_token=\"\"):\n",
        "    url = url.strip()\n",
        "\n",
        "    if \"drive.google.com\" in url:\n",
        "        original_dir = os.getcwd()\n",
        "        os.chdir(directory)\n",
        "        !gdown --fuzzy {url}\n",
        "        os.chdir(original_dir)\n",
        "    elif \"huggingface.co\" in url:\n",
        "        if \"/blob/\" in url:\n",
        "            url = url.replace(\"/blob/\", \"/resolve/\")\n",
        "        user_header = f'\"Authorization: Bearer {hf_token}\"'\n",
        "        !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 {url} -d {directory}  -o {url.split('/')[-1]}\n",
        "    else:\n",
        "        !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {directory} {url}\n",
        "\n",
        "def get_model_list(directory_path):\n",
        "    model_list = []\n",
        "    valid_extensions = {'.ckpt' , '.pt', '.pth', '.safetensors', '.bin'}\n",
        "\n",
        "    for filename in os.listdir(directory_path):\n",
        "        if os.path.splitext(filename)[1] in valid_extensions:\n",
        "            name_without_extension = os.path.splitext(filename)[0]\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            model_list.append((name_without_extension, file_path))\n",
        "            print('\\033[34mFILE: ' + name_without_extension + file_path + '\\033[0m')\n",
        "    return model_list\n",
        "\n",
        "def process_string(input_string):\n",
        "    parts = input_string.split('/')\n",
        "\n",
        "    if len(parts) == 2:\n",
        "        first_element = parts[1]\n",
        "        complete_string = input_string\n",
        "        result = (first_element, complete_string)\n",
        "        return result\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "directory_models = 'models'\n",
        "os.makedirs(directory_models, exist_ok=True)\n",
        "directory_loras = 'loras'\n",
        "os.makedirs(directory_loras, exist_ok=True)\n",
        "directory_vaes = 'vaes'\n",
        "os.makedirs(directory_vaes, exist_ok=True)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown - **Download a Model**\n",
        "download_model = \"https://civitai.com/api/download/models/125771\" # @param {type:\"string\"}\n",
        "#@markdown - For SDXL models, only diffuser format models are supported, and you only need the repository name.\n",
        "load_diffusers_format_model = 'Linaqruf/animagine-xl' # @param {type:\"string\"}\n",
        "#@markdown - **Download a VAE**\n",
        "download_vae = \"https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/VAEs/orangemix.vae.pt\" # @param {type:\"string\"}\n",
        "#@markdown - **Download a LoRA**\n",
        "download_lora = \"https://civitai.com/api/download/models/97655\" # @param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **HF TOKEN** - If you need to download your private model from Hugging Face, input your token here.\n",
        "hf_token = \"\"  # @param {type:\"string\"}\n",
        "#@markdown\n",
        "#@markdown ---\n",
        "\n",
        "download_things(directory_models, download_model, hf_token)\n",
        "download_things(directory_vaes, download_vae, hf_token)\n",
        "download_things(directory_loras, download_lora, hf_token)\n",
        "\n",
        "\n",
        "# TI more combatible in safetensor format; maybe convert to safetensor can help\n",
        "directory_embeds = 'embedings'\n",
        "os.makedirs(directory_embeds, exist_ok=True)\n",
        "download_embeds = [\n",
        "    'https://huggingface.co/datasets/Nerfgun3/bad_prompt/resolve/main/bad_prompt.pt',\n",
        "    'https://huggingface.co/datasets/Nerfgun3/bad_prompt/blob/main/bad_prompt_version2.pt',\n",
        "    'https://huggingface.co/sayakpaul/EasyNegative-test/blob/main/EasyNegative.safetensors',\n",
        "    ]\n",
        "for url_embed in download_embeds:\n",
        "    download_things(directory_embeds, url_embed, hf_token)\n",
        "embed_list = get_model_list(directory_embeds)\n",
        "\n",
        "model_list = get_model_list(directory_models)\n",
        "if load_diffusers_format_model.strip() != \"\" and load_diffusers_format_model.count('/') == 1:\n",
        "    model_list.append(process_string(load_diffusers_format_model))\n",
        "lora_model_list = get_model_list(directory_loras)\n",
        "lora_model_list.insert(0, (\"None\",None))\n",
        "vae_model_list = get_model_list(directory_vaes)\n",
        "vae_model_list.insert(0, (\"None\",\"None\"))\n",
        "\n",
        "\n",
        "\n",
        "print('\\033[33müèÅ Download finished.\\033[0m')\n",
        "\n",
        "### SECOND PART ###\n",
        "import gc\n",
        "import time\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from diffusers import (\n",
        "    ControlNetModel,\n",
        "    DiffusionPipeline,\n",
        "    StableDiffusionControlNetPipeline,\n",
        "    StableDiffusionControlNetInpaintPipeline,\n",
        "    StableDiffusionPipeline,\n",
        "    AutoencoderKL\n",
        ")\n",
        "import torch\n",
        "import random\n",
        "from controlnet_aux import (CannyDetector, ContentShuffleDetector, HEDdetector,\n",
        "                            LineartAnimeDetector, LineartDetector,\n",
        "                            MidasDetector, MLSDdetector, NormalBaeDetector,\n",
        "                            OpenposeDetector, PidiNetDetector)\n",
        "from transformers import pipeline\n",
        "from controlnet_aux.util import HWC3, ade_palette\n",
        "from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\n",
        "import cv2\n",
        "from diffusers import (\n",
        "    DPMSolverMultistepScheduler,\n",
        "    DPMSolverSinglestepScheduler,\n",
        "    KDPM2DiscreteScheduler,\n",
        "    KDPM2AncestralDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    HeunDiscreteScheduler,\n",
        "    LMSDiscreteScheduler,\n",
        "    DDIMScheduler,\n",
        "    DEISMultistepScheduler,\n",
        "    UniPCMultistepScheduler,\n",
        ")\n",
        "\n",
        "# =====================================\n",
        "# Utils preprocessor\n",
        "# =====================================\n",
        "def resize_image(input_image, resolution, interpolation=None):\n",
        "    H, W, C = input_image.shape\n",
        "    H = float(H)\n",
        "    W = float(W)\n",
        "    k = float(resolution) / max(H, W)\n",
        "    H *= k\n",
        "    W *= k\n",
        "    H = int(np.round(H / 64.0)) * 64\n",
        "    W = int(np.round(W / 64.0)) * 64\n",
        "    if interpolation is None:\n",
        "        interpolation = cv2.INTER_LANCZOS4 if k > 1 else cv2.INTER_AREA\n",
        "    img = cv2.resize(input_image, (W, H), interpolation=interpolation)\n",
        "    return img\n",
        "\n",
        "class DepthEstimator:\n",
        "    def __init__(self):\n",
        "        self.model = pipeline('depth-estimation')\n",
        "\n",
        "    def __call__(self, image: np.ndarray, **kwargs) -> PIL.Image.Image:\n",
        "        detect_resolution = kwargs.pop('detect_resolution', 512)\n",
        "        image_resolution = kwargs.pop('image_resolution', 512)\n",
        "        image = np.array(image)\n",
        "        image = HWC3(image)\n",
        "        image = resize_image(image, resolution=detect_resolution)\n",
        "        image = PIL.Image.fromarray(image)\n",
        "        image = self.model(image)\n",
        "        image = image['depth']\n",
        "        image = np.array(image)\n",
        "        image = HWC3(image)\n",
        "        image = resize_image(image, resolution=image_resolution)\n",
        "        return PIL.Image.fromarray(image)\n",
        "\n",
        "class ImageSegmentor:\n",
        "    def __init__(self):\n",
        "        self.image_processor = AutoImageProcessor.from_pretrained(\n",
        "            'openmmlab/upernet-convnext-small')\n",
        "        self.image_segmentor = UperNetForSemanticSegmentation.from_pretrained(\n",
        "            'openmmlab/upernet-convnext-small')\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def __call__(self, image: np.ndarray, **kwargs) -> PIL.Image.Image:\n",
        "        detect_resolution = kwargs.pop('detect_resolution', 512)\n",
        "        image_resolution = kwargs.pop('image_resolution', 512)\n",
        "        image = HWC3(image)\n",
        "        image = resize_image(image, resolution=detect_resolution)\n",
        "        image = PIL.Image.fromarray(image)\n",
        "\n",
        "        pixel_values = self.image_processor(image,\n",
        "                                            return_tensors='pt').pixel_values\n",
        "        outputs = self.image_segmentor(pixel_values)\n",
        "        seg = self.image_processor.post_process_semantic_segmentation(\n",
        "            outputs, target_sizes=[image.size[::-1]])[0]\n",
        "        color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8)\n",
        "        for label, color in enumerate(ade_palette()):\n",
        "            color_seg[seg == label, :] = color\n",
        "        color_seg = color_seg.astype(np.uint8)\n",
        "\n",
        "        color_seg = resize_image(color_seg,\n",
        "                                 resolution=image_resolution,\n",
        "                                 interpolation=cv2.INTER_NEAREST)\n",
        "        return PIL.Image.fromarray(color_seg)\n",
        "\n",
        "class Preprocessor:\n",
        "    MODEL_ID = 'lllyasviel/Annotators'\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.name = ''\n",
        "\n",
        "    def load(self, name: str) -> None:\n",
        "        if name == self.name:\n",
        "            return\n",
        "        if name == 'HED':\n",
        "            self.model = HEDdetector.from_pretrained(self.MODEL_ID)\n",
        "        elif name == 'Midas':\n",
        "            self.model = MidasDetector.from_pretrained(self.MODEL_ID)\n",
        "        elif name == 'MLSD':\n",
        "            self.model = MLSDdetector.from_pretrained(self.MODEL_ID)\n",
        "        elif name == 'Openpose':\n",
        "            self.model = OpenposeDetector.from_pretrained(self.MODEL_ID)\n",
        "        elif name == 'PidiNet':\n",
        "            self.model = PidiNetDetector.from_pretrained(self.MODEL_ID)\n",
        "        elif name == 'NormalBae':\n",
        "            self.model = NormalBaeDetector.from_pretrained(self.MODEL_ID)\n",
        "        elif name == 'Lineart':\n",
        "            self.model = LineartDetector.from_pretrained(self.MODEL_ID)\n",
        "        elif name == 'LineartAnime':\n",
        "            self.model = LineartAnimeDetector.from_pretrained(self.MODEL_ID)\n",
        "        elif name == 'Canny':\n",
        "            self.model = CannyDetector()\n",
        "        elif name == 'ContentShuffle':\n",
        "            self.model = ContentShuffleDetector()\n",
        "        elif name == 'DPT':\n",
        "            self.model = DepthEstimator()\n",
        "        elif name == 'UPerNet':\n",
        "            self.model = ImageSegmentor()\n",
        "        else:\n",
        "            raise ValueError\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        self.name = name\n",
        "\n",
        "    def __call__(self, image: PIL.Image.Image, **kwargs) -> PIL.Image.Image:\n",
        "        if self.name == 'Canny':\n",
        "            if 'detect_resolution' in kwargs:\n",
        "                detect_resolution = kwargs.pop('detect_resolution')\n",
        "                image = np.array(image)\n",
        "                image = HWC3(image)\n",
        "                image = resize_image(image, resolution=detect_resolution)\n",
        "            image = self.model(image, **kwargs)\n",
        "            return PIL.Image.fromarray(image)\n",
        "        elif self.name == 'Midas':\n",
        "            detect_resolution = kwargs.pop('detect_resolution', 512)\n",
        "            image_resolution = kwargs.pop('image_resolution', 512)\n",
        "            image = np.array(image)\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=detect_resolution)\n",
        "            image = self.model(image, **kwargs)\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            return PIL.Image.fromarray(image)\n",
        "        else:\n",
        "            return self.model(image, **kwargs)\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# Base Model\n",
        "# =====================================\n",
        "\n",
        "CONTROLNET_MODEL_IDS = {\n",
        "    'Openpose': 'lllyasviel/control_v11p_sd15_openpose',\n",
        "    'Canny': 'lllyasviel/control_v11p_sd15_canny',\n",
        "    'MLSD': 'lllyasviel/control_v11p_sd15_mlsd',\n",
        "    'scribble': 'lllyasviel/control_v11p_sd15_scribble',\n",
        "    'softedge': 'lllyasviel/control_v11p_sd15_softedge',\n",
        "    'segmentation': 'lllyasviel/control_v11p_sd15_seg',\n",
        "    'depth': 'lllyasviel/control_v11f1p_sd15_depth',\n",
        "    'NormalBae': 'lllyasviel/control_v11p_sd15_normalbae',\n",
        "    'lineart': 'lllyasviel/control_v11p_sd15_lineart',\n",
        "    'lineart_anime': 'lllyasviel/control_v11p_sd15s2_lineart_anime',\n",
        "    'shuffle': 'lllyasviel/control_v11e_sd15_shuffle',\n",
        "    'ip2p': 'lllyasviel/control_v11e_sd15_ip2p',\n",
        "    'Inpaint': 'lllyasviel/control_v11p_sd15_inpaint',\n",
        "    'txt2img': 'NotControlnet',\n",
        "}\n",
        "\n",
        "def download_all_controlnet_weights() -> None:\n",
        "    for model_id in CONTROLNET_MODEL_IDS.values():\n",
        "        ControlNetModel.from_pretrained(model_id)\n",
        "\n",
        "class Model:\n",
        "    def __init__(self,\n",
        "                 base_model_id: str = 'runwayml/stable-diffusion-v1-5',\n",
        "                 task_name: str = 'Canny', vae_model=None, type_model_precision = torch.float16):\n",
        "        self.device = torch.device(\n",
        "            'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.base_model_id = ''\n",
        "        self.task_name = ''\n",
        "        self.vae_model = None\n",
        "        self.type_model_precision = type_model_precision if torch.cuda.is_available() else torch.float32 # For SD 1.5\n",
        "\n",
        "        self.pipe = self.load_pipe(base_model_id, task_name, vae_model, type_model_precision)\n",
        "        self.preprocessor = Preprocessor()\n",
        "\n",
        "    def load_pipe(self, base_model_id: str, task_name, vae_model=None, type_model_precision = torch.float16, reload=False) -> DiffusionPipeline:\n",
        "        if base_model_id == self.base_model_id and task_name == self.task_name and hasattr(\n",
        "                self, 'pipe') and self.vae_model==vae_model and self.pipe is not None and reload==False and type_model_precision == self.type_model_precision:\n",
        "            print('Previous loaded')\n",
        "            return self.pipe\n",
        "        if base_model_id == self.base_model_id and task_name == self.task_name and hasattr(\n",
        "                self, 'pipe') and self.vae_model==vae_model and self.pipe is not None and reload==False and self.device == \"cpu\":\n",
        "            print('Pipe in CPU')\n",
        "            return self.pipe\n",
        "\n",
        "\n",
        "        self.type_model_precision = type_model_precision if torch.cuda.is_available() else torch.float32\n",
        "        self.pipe = None\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        model_id = CONTROLNET_MODEL_IDS[task_name]\n",
        "\n",
        "        if task_name == 'txt2img':\n",
        "            if os.path.exists(base_model_id):\n",
        "                if self.type_model_precision == torch.float32:\n",
        "                    print(\"Working with full precision\")\n",
        "                pipe = StableDiffusionPipeline.from_single_file(\n",
        "                  base_model_id,\n",
        "                  vae = None if vae_model == 'None' else AutoencoderKL.from_single_file(vae_model), # , torch_dtype=self.type_model_precision\n",
        "                  torch_dtype=self.type_model_precision,\n",
        "                )\n",
        "                pipe.safety_checker = None\n",
        "            else:\n",
        "                print(\"Default VAE: madebyollin/sdxl-vae-fp16-fix\")\n",
        "                pipe = DiffusionPipeline.from_pretrained(\n",
        "                    base_model_id,\n",
        "                    vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16),\n",
        "                    torch_dtype=torch.float16,\n",
        "                    use_safetensors=True,\n",
        "                    variant=\"fp16\",\n",
        "                    )\n",
        "                pipe.safety_checker = None\n",
        "            print('Loaded txt2img pipeline')\n",
        "        elif task_name == 'Inpaint':\n",
        "            if self.type_model_precision == torch.float32:\n",
        "                print(\"Working with full precision\")\n",
        "            controlnet = ControlNetModel.from_pretrained(model_id,\n",
        "                                                        torch_dtype=self.type_model_precision)\n",
        "            if os.path.exists(base_model_id):\n",
        "                pipe = StableDiffusionControlNetInpaintPipeline.from_single_file(\n",
        "                    base_model_id,\n",
        "                    vae = None if vae_model == 'None' else AutoencoderKL.from_single_file(vae_model),\n",
        "                    safety_checker=None,\n",
        "                    controlnet=controlnet,\n",
        "                    torch_dtype=self.type_model_precision)\n",
        "            print('Loaded ControlNet Inpaint pipeline')\n",
        "        else:\n",
        "            if self.type_model_precision == torch.float32:\n",
        "                print(\"Working with full precision\")\n",
        "            controlnet = ControlNetModel.from_pretrained(model_id,\n",
        "                                                        torch_dtype=self.type_model_precision) # for all\n",
        "            if os.path.exists(base_model_id):\n",
        "                pipe = StableDiffusionControlNetPipeline.from_single_file(\n",
        "                    base_model_id,\n",
        "                    vae = None if vae_model == 'None' else AutoencoderKL.from_single_file(vae_model),\n",
        "                    safety_checker=None,\n",
        "                    controlnet=controlnet,\n",
        "                    torch_dtype=self.type_model_precision)\n",
        "            else:\n",
        "                raise ZeroDivisionError(\"Not implemented\")\n",
        "            #     pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "            #         base_model_id,\n",
        "            #         vae = AutoencoderKL.from_pretrained(base_model_id, subfolder='vae') if vae_model == 'None' else AutoencoderKL.from_single_file(vae_model),\n",
        "            #         safety_checker=None,\n",
        "            #         controlnet=controlnet,\n",
        "            #         torch_dtype=torch.float16)\n",
        "            # print('Loaded ControlNet pipeline')\n",
        "\n",
        "            pipe.scheduler = UniPCMultistepScheduler.from_config(\n",
        "                pipe.scheduler.config)\n",
        "\n",
        "        if self.device.type == 'cuda':\n",
        "            pipe.enable_xformers_memory_efficient_attention()\n",
        "\n",
        "        pipe.to(self.device)\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        self.pipe = pipe\n",
        "        self.base_model_id = base_model_id\n",
        "        self.task_name = task_name\n",
        "        self.vae_model = vae_model\n",
        "        return pipe\n",
        "\n",
        "    def set_base_model(self, base_model_id: str) -> str:\n",
        "        if not base_model_id or base_model_id == self.base_model_id:\n",
        "            return self.base_model_id\n",
        "        del self.pipe\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        try:\n",
        "            self.pipe = self.load_pipe(base_model_id, self.task_name, self.vae_model)\n",
        "        except Exception:\n",
        "            self.pipe = self.load_pipe(self.base_model_id, self.task_name, self.vae_model)\n",
        "        return self.base_model_id\n",
        "\n",
        "    def load_controlnet_weight(self, task_name: str) -> None:\n",
        "        if task_name == self.task_name:\n",
        "            return\n",
        "        if self.pipe is not None and hasattr(self.pipe, 'controlnet'):\n",
        "            del self.pipe.controlnet\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        model_id = CONTROLNET_MODEL_IDS[task_name]\n",
        "        controlnet = ControlNetModel.from_pretrained(model_id,\n",
        "                                                     torch_dtype=self.type_model_precision)\n",
        "        controlnet.to(self.device)\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        self.pipe.controlnet = controlnet\n",
        "        self.task_name = task_name\n",
        "\n",
        "    def get_prompt(self, prompt: str, additional_prompt: str) -> str:\n",
        "        if not prompt:\n",
        "            prompt = additional_prompt\n",
        "        else:\n",
        "            prompt = f'{prompt}, {additional_prompt}'\n",
        "        return prompt\n",
        "\n",
        "    @torch.autocast('cuda')\n",
        "    def run_pipe(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        negative_prompt: str,\n",
        "        prompt_embeds,\n",
        "        negative_prompt_embeds,\n",
        "        control_image: PIL.Image.Image,\n",
        "        num_images: int,\n",
        "        num_steps: int,\n",
        "        guidance_scale: float,\n",
        "        clip_skip: int,\n",
        "        generator,\n",
        "        controlnet_conditioning_scale,\n",
        "        control_guidance_start,\n",
        "        control_guidance_end,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        # Return PIL images\n",
        "        #generator = torch.Generator().manual_seed(seed)\n",
        "        return self.pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "            guidance_scale=guidance_scale,\n",
        "            clip_skip = clip_skip,\n",
        "            num_images_per_prompt=num_images,\n",
        "            num_inference_steps=num_steps,\n",
        "            generator=generator,\n",
        "            controlnet_conditioning_scale = controlnet_conditioning_scale,\n",
        "            control_guidance_start = control_guidance_start,\n",
        "            control_guidance_end = control_guidance_end,\n",
        "            image=control_image\n",
        "            ).images\n",
        "\n",
        "    @torch.autocast('cuda')\n",
        "    def run_pipe_SD(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        negative_prompt: str,\n",
        "        prompt_embeds,\n",
        "        negative_prompt_embeds,\n",
        "        num_images: int,\n",
        "        num_steps: int,\n",
        "        guidance_scale: float,\n",
        "        clip_skip: int,\n",
        "        height : int,\n",
        "        width : int,\n",
        "        generator,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        # Return PIL images\n",
        "        #generator = torch.Generator().manual_seed(seed)\n",
        "        return self.pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "            guidance_scale=guidance_scale,\n",
        "            clip_skip = clip_skip,\n",
        "            num_images_per_prompt=num_images,\n",
        "            num_inference_steps=num_steps,\n",
        "            generator=generator,\n",
        "            height = height,\n",
        "            width = width,\n",
        "            ).images\n",
        "\n",
        "\n",
        "    # @torch.autocast('cuda')\n",
        "    # def run_pipe_SDXL(\n",
        "    #     self,\n",
        "    #     prompt: str,\n",
        "    #     negative_prompt: str,\n",
        "    #     prompt_embeds,\n",
        "    #     negative_prompt_embeds,\n",
        "    #     num_images: int,\n",
        "    #     num_steps: int,\n",
        "    #     guidance_scale: float,\n",
        "    #     clip_skip: int,\n",
        "    #     height : int,\n",
        "    #     width : int,\n",
        "    #     generator,\n",
        "    #     seddd,\n",
        "    #     conditioning,\n",
        "    #     pooled,\n",
        "    # ) -> list[PIL.Image.Image]:\n",
        "    #     # Return PIL images\n",
        "    #     #generator = torch.Generator(\"cuda\").manual_seed(seddd) # generator = torch.Generator(\"cuda\").manual_seed(seed),\n",
        "    #     return self.pipe(\n",
        "    #         prompt = None,\n",
        "    #         negative_prompt = None,\n",
        "    #         prompt_embeds=conditioning[0:1],\n",
        "    #         pooled_prompt_embeds=pooled[0:1],\n",
        "    #         negative_prompt_embeds=conditioning[1:2],\n",
        "    #         negative_pooled_prompt_embeds=pooled[1:2],\n",
        "    #         height = height,\n",
        "    #         width = width,\n",
        "    #         num_inference_steps = num_steps,\n",
        "    #         guidance_scale = guidance_scale,\n",
        "    #         clip_skip = clip_skip,\n",
        "    #         num_images_per_prompt = num_images,\n",
        "    #         generator = generator,\n",
        "    #         ).images\n",
        "\n",
        "    @torch.autocast('cuda')\n",
        "    def run_pipe_inpaint(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        negative_prompt: str,\n",
        "        prompt_embeds,\n",
        "        negative_prompt_embeds,\n",
        "        control_image: PIL.Image.Image,\n",
        "        num_images: int,\n",
        "        num_steps: int,\n",
        "        guidance_scale: float,\n",
        "        clip_skip: int,\n",
        "        strength: float,\n",
        "        init_image,\n",
        "        control_mask,\n",
        "        controlnet_conditioning_scale,\n",
        "        control_guidance_start,\n",
        "        control_guidance_end,\n",
        "        generator,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        # Return PIL images\n",
        "        #generator = torch.Generator().manual_seed(seed)\n",
        "        return self.pipe(\n",
        "            prompt= None,\n",
        "            negative_prompt= None,\n",
        "            prompt_embeds=prompt_embeds,\n",
        "            negative_prompt_embeds=negative_prompt_embeds,\n",
        "            eta=1.0,\n",
        "            strength = strength,\n",
        "            image = init_image, # original image\n",
        "            mask_image = control_mask, # mask, values of 0 to 255\n",
        "            control_image = control_image, # tensor control image\n",
        "            num_images_per_prompt  = num_images,\n",
        "            num_inference_steps = num_steps,\n",
        "            guidance_scale = guidance_scale,\n",
        "            clip_skip = clip_skip,\n",
        "            generator = generator,\n",
        "            controlnet_conditioning_scale = controlnet_conditioning_scale,\n",
        "            control_guidance_start = control_guidance_start,\n",
        "            control_guidance_end = control_guidance_end,\n",
        "            ).images\n",
        "\n",
        "    ### self.x_process return image_preprocessor###\n",
        "    @torch.inference_mode()\n",
        "    def process_canny(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int = 512,\n",
        "        low_threshold: int = 100,\n",
        "        high_threshold: int = 200,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        self.preprocessor.load('Canny')\n",
        "        control_image = self.preprocessor(image=image,\n",
        "                                          low_threshold=low_threshold,\n",
        "                                          high_threshold=high_threshold,\n",
        "                                          detect_resolution=image_resolution)\n",
        "\n",
        "        self.load_controlnet_weight('Canny')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_mlsd(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        value_threshold: float,\n",
        "        distance_threshold: float,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        self.preprocessor.load('MLSD')\n",
        "        control_image = self.preprocessor(\n",
        "            image=image,\n",
        "            image_resolution=image_resolution,\n",
        "            detect_resolution=preprocess_resolution,\n",
        "            thr_v=value_threshold,\n",
        "            thr_d=distance_threshold,\n",
        "        )\n",
        "        self.load_controlnet_weight('MLSD')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_scribble(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        preprocessor_name: str,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        if preprocessor_name == 'None':\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            control_image = PIL.Image.fromarray(image)\n",
        "        elif preprocessor_name == 'HED':\n",
        "            self.preprocessor.load(preprocessor_name)\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "                scribble=False,\n",
        "            )\n",
        "        elif preprocessor_name == 'PidiNet':\n",
        "            self.preprocessor.load(preprocessor_name)\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "                safe=False,\n",
        "            )\n",
        "        self.load_controlnet_weight('scribble')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_scribble_interactive(\n",
        "        self,\n",
        "        image_and_mask: dict[str, np.ndarray],\n",
        "        image_resolution: int,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image_and_mask is None:\n",
        "            raise ValueError\n",
        "\n",
        "        image = image_and_mask['mask']\n",
        "        image = HWC3(image)\n",
        "        image = resize_image(image, resolution=image_resolution)\n",
        "        control_image = PIL.Image.fromarray(image)\n",
        "\n",
        "        self.load_controlnet_weight('scribble')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_softedge(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        preprocessor_name: str,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        if preprocessor_name == 'None':\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            control_image = PIL.Image.fromarray(image)\n",
        "        elif preprocessor_name in ['HED', 'HED safe']:\n",
        "            safe = 'safe' in preprocessor_name\n",
        "            self.preprocessor.load('HED')\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "                scribble=safe,\n",
        "            )\n",
        "        elif preprocessor_name in ['PidiNet', 'PidiNet safe']:\n",
        "            safe = 'safe' in preprocessor_name\n",
        "            self.preprocessor.load('PidiNet')\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "                safe=safe,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError\n",
        "        self.load_controlnet_weight('softedge')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_openpose(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        preprocessor_name: str,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        if preprocessor_name == 'None':\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            control_image = PIL.Image.fromarray(image)\n",
        "        else:\n",
        "            self.preprocessor.load('Openpose')\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "                hand_and_face=True,\n",
        "            )\n",
        "        self.load_controlnet_weight('Openpose')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_segmentation(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        preprocessor_name: str,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        if preprocessor_name == 'None':\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            control_image = PIL.Image.fromarray(image)\n",
        "        else:\n",
        "            self.preprocessor.load(preprocessor_name)\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "            )\n",
        "        self.load_controlnet_weight('segmentation')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_depth(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        preprocessor_name: str,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        if preprocessor_name == 'None':\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            control_image = PIL.Image.fromarray(image)\n",
        "        else:\n",
        "            self.preprocessor.load(preprocessor_name)\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "            )\n",
        "        self.load_controlnet_weight('depth')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_normal(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        preprocessor_name: str,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        if preprocessor_name == 'None':\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            control_image = PIL.Image.fromarray(image)\n",
        "        else:\n",
        "            self.preprocessor.load('NormalBae')\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "            )\n",
        "        self.load_controlnet_weight('NormalBae')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_lineart(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        preprocessor_name: str,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        if preprocessor_name in ['None', 'None (anime)']:\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            control_image = PIL.Image.fromarray(image)\n",
        "        elif preprocessor_name in ['Lineart', 'Lineart coarse']:\n",
        "            coarse = 'coarse' in preprocessor_name\n",
        "            self.preprocessor.load('Lineart')\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "                coarse=coarse,\n",
        "            )\n",
        "        elif preprocessor_name == 'Lineart (anime)':\n",
        "            self.preprocessor.load('LineartAnime')\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "                detect_resolution=preprocess_resolution,\n",
        "            )\n",
        "        if 'anime' in preprocessor_name:\n",
        "            self.load_controlnet_weight('lineart_anime')\n",
        "        else:\n",
        "            self.load_controlnet_weight('lineart')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_shuffle(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocessor_name: str,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        if preprocessor_name == 'None':\n",
        "            image = HWC3(image)\n",
        "            image = resize_image(image, resolution=image_resolution)\n",
        "            control_image = PIL.Image.fromarray(image)\n",
        "        else:\n",
        "            self.preprocessor.load(preprocessor_name)\n",
        "            control_image = self.preprocessor(\n",
        "                image=image,\n",
        "                image_resolution=image_resolution,\n",
        "            )\n",
        "        self.load_controlnet_weight('shuffle')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_ip2p(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        image = HWC3(image)\n",
        "        image = resize_image(image, resolution=image_resolution)\n",
        "        control_image = PIL.Image.fromarray(image)\n",
        "        self.load_controlnet_weight('ip2p')\n",
        "\n",
        "        return control_image\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def process_inpaint(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        image_resolution: int,\n",
        "        preprocess_resolution: int,\n",
        "        image_mask: str,###\n",
        "    ) -> list[PIL.Image.Image]:\n",
        "        if image is None:\n",
        "            raise ValueError\n",
        "\n",
        "        image = HWC3(image)\n",
        "        image = resize_image(image, resolution=preprocess_resolution)\n",
        "        init_image = PIL.Image.fromarray(image)\n",
        "\n",
        "        image_mask = HWC3(image_mask)\n",
        "        image_mask = resize_image(image_mask, resolution=preprocess_resolution)\n",
        "        control_mask = PIL.Image.fromarray(image_mask)\n",
        "\n",
        "        control_image = make_inpaint_condition(init_image, control_mask)\n",
        "\n",
        "        self.load_controlnet_weight('Inpaint')\n",
        "\n",
        "        return init_image, control_mask, control_image\n",
        "\n",
        "    def get_scheduler(self, name):\n",
        "      #Get scheduler\n",
        "      match name:\n",
        "\n",
        "        case \"DPM++ 2M\":\n",
        "          return DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config)\n",
        "\n",
        "        case \"DPM++ 2M Karras\":\n",
        "          return DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config, use_karras_sigmas=True)\n",
        "\n",
        "        case \"DPM++ 2M SDE\":\n",
        "          return DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config, algorithm_type=\"sde-dpmsolver++\")\n",
        "\n",
        "        case \"DPM++ 2M SDE Karras\":\n",
        "          return DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config, use_karras_sigmas=True, algorithm_type=\"sde-dpmsolver++\")\n",
        "\n",
        "        case \"DPM++ SDE\":\n",
        "          return DPMSolverSinglestepScheduler.from_config(self.pipe.scheduler.config, )\n",
        "\n",
        "        case \"DPM++ SDE Karras\":\n",
        "          return DPMSolverSinglestepScheduler.from_config(self.pipe.scheduler.config, use_karras_sigmas=True)\n",
        "\n",
        "        case \"DPM2\":\n",
        "          return KDPM2DiscreteScheduler.from_config(self.pipe.scheduler.config, )\n",
        "\n",
        "        case \"DPM2 Karras\":\n",
        "          return KDPM2DiscreteScheduler.from_config(self.pipe.scheduler.config, use_karras_sigmas=True)\n",
        "\n",
        "        case \"Euler\":\n",
        "          return EulerDiscreteScheduler.from_config(self.pipe.scheduler.config, )\n",
        "\n",
        "        case \"Euler a\":\n",
        "          return EulerAncestralDiscreteScheduler.from_config(self.pipe.scheduler.config, )\n",
        "\n",
        "        case \"Heun\":\n",
        "          return HeunDiscreteScheduler.from_config(self.pipe.scheduler.config, )\n",
        "\n",
        "        case \"LMS\":\n",
        "          return LMSDiscreteScheduler.from_config(self.pipe.scheduler.config, )\n",
        "\n",
        "        case \"LMS Karras\":\n",
        "          return LMSDiscreteScheduler.from_config(self.pipe.scheduler.config, use_karras_sigmas=True)\n",
        "\n",
        "        case \"DDIMScheduler\":\n",
        "          return DDIMScheduler.from_config(self.pipe.scheduler.config)\n",
        "\n",
        "        case \"DEISMultistepScheduler\":\n",
        "          return DEISMultistepScheduler.from_config(self.pipe.scheduler.config)\n",
        "\n",
        "        case \"UniPCMultistepScheduler\":\n",
        "          return UniPCMultistepScheduler.from_config(self.pipe.scheduler.config)\n",
        "\n",
        "    def process_lora(self, select_lora, lora_weights_scale, unload=False):\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        if not unload:\n",
        "            if select_lora != None:\n",
        "                  try:\n",
        "                      self.pipe = lora_mix_load(self.pipe, select_lora, lora_weights_scale, device=device, dtype=self.type_model_precision)\n",
        "                      print(select_lora)\n",
        "                  except:\n",
        "                      print(f\"ERROR: LoRA not compatible: {select_lora}\")\n",
        "            return self.pipe\n",
        "        else:\n",
        "            # Unload numerically unstable but fast\n",
        "            if select_lora != None:\n",
        "                try:\n",
        "                    self.pipe = lora_mix_load(self.pipe, select_lora, -lora_weights_scale, device=device, dtype=self.type_model_precision)\n",
        "                    #print(select_lora, 'unload')\n",
        "                except:\n",
        "                    pass\n",
        "            return self.pipe\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt = None,\n",
        "        negative_prompt = None,\n",
        "        # prompt_embeds = None,\n",
        "        # negative_prompt_embeds = None,\n",
        "        img_height = None,\n",
        "        img_width = None,\n",
        "        num_images = None,\n",
        "        num_steps = None,\n",
        "        guidance_scale = None,\n",
        "        clip_skip = True,\n",
        "        seed = None,\n",
        "        image = None, # path, np.array, or PIL image\n",
        "        preprocessor_name = None,\n",
        "        preprocess_resolution = None,\n",
        "        image_resolution = None,\n",
        "        additional_prompt = \"\",\n",
        "        image_mask = None,\n",
        "        strength = None,\n",
        "        low_threshold=None, # Canny\n",
        "        high_threshold=None, # Canny\n",
        "        value_threshold=None, # MLSD\n",
        "        distance_threshold=None, # MLSD\n",
        "        lora_A = None,\n",
        "        lora_scale_A = 1.0,\n",
        "        lora_B = None,\n",
        "        lora_scale_B = 1.0,\n",
        "        lora_C = None,\n",
        "        lora_scale_C = 1.0,\n",
        "        lora_D = None,\n",
        "        lora_scale_D = 1.0,\n",
        "        lora_E = None,\n",
        "        lora_scale_E = 1.0,\n",
        "        active_textual_inversion = False,\n",
        "        textual_inversion = [], # List of tuples [(activation_token, path_embedding),...]\n",
        "        convert_weights_prompt = False,\n",
        "        sampler = None,\n",
        "        xformers_memory_efficient_attention = True,\n",
        "        gui_active = False,\n",
        "        loop_generation = 1,\n",
        "        controlnet_conditioning_scale = 1.0,\n",
        "        control_guidance_start = 0.0,\n",
        "        control_guidance_end  = 0.0,\n",
        "        generator_in_cpu = False, # Initial noise not in CPU\n",
        "    ):\n",
        "        if self.task_name != \"txt2img\" and image == None:\n",
        "            raise ValueError\n",
        "\n",
        "        if self.pipe == None:\n",
        "            self.load_pipe(self.base_model_id, task_name = self.task_name, vae_model = self.vae_model)\n",
        "\n",
        "        self.pipe.to(self.device)\n",
        "        # self.pipe.unfuse_lora()\n",
        "        # self.pipe.unload_lora_weights()\n",
        "        self.pipe = self.process_lora(lora_A, lora_scale_A)\n",
        "        self.pipe = self.process_lora(lora_B, lora_scale_B)\n",
        "        self.pipe = self.process_lora(lora_C, lora_scale_C)\n",
        "        self.pipe = self.process_lora(lora_D, lora_scale_D)\n",
        "        self.pipe = self.process_lora(lora_E, lora_scale_E)\n",
        "        self.pipe.to(self.device)\n",
        "\n",
        "        # Prompt Optimizations for 1.5\n",
        "        if os.path.exists(self.base_model_id):\n",
        "            if  active_textual_inversion:\n",
        "              # Textual Inversion\n",
        "              for name, directory_name in textual_inversion:\n",
        "                  try:\n",
        "                          #self.pipe.text_encoder.resize_token_embeddings(len(self.pipe.tokenizer),pad_to_multiple_of=128)\n",
        "                          #self.pipe.load_textual_inversion(\"./bad_prompt.pt\", token=\"baddd\")\n",
        "                          self.pipe.load_textual_inversion(directory_name, token=name)\n",
        "                  except ValueError:\n",
        "                      # previous loaded ti\n",
        "                      pass\n",
        "                  except:\n",
        "                      print(f\"Can't apply {name}\")\n",
        "\n",
        "            #Clip skip\n",
        "            if clip_skip:\n",
        "                #clip_skip_diffusers = None #clip_skip - 1 # future update\n",
        "                compel = Compel(\n",
        "                    tokenizer=self.pipe.tokenizer,\n",
        "                    text_encoder=self.pipe.text_encoder,\n",
        "                    truncate_long_prompts=False,\n",
        "                    returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NORMALIZED\n",
        "                )\n",
        "            else:\n",
        "                #clip_skip_diffusers = None # clip_skip = None # future update\n",
        "                compel = Compel(\n",
        "                    tokenizer=self.pipe.tokenizer,\n",
        "                    text_encoder=self.pipe.text_encoder,\n",
        "                    truncate_long_prompts=False\n",
        "                )\n",
        "\n",
        "            # Prompt weights for textual inversion\n",
        "            prompt_ti = self.pipe.maybe_convert_prompt(prompt, self.pipe.tokenizer)\n",
        "            negative_prompt_ti = self.pipe.maybe_convert_prompt(negative_prompt, self.pipe.tokenizer)\n",
        "\n",
        "            # prompt syntax style a1...\n",
        "            if convert_weights_prompt:\n",
        "                prompt_ti = prompt_weight_conversor(prompt_ti)\n",
        "                negative_prompt_ti = prompt_weight_conversor(negative_prompt_ti)\n",
        "\n",
        "            # prompt embed chunks style a1...\n",
        "            prompt_emb = merge_embeds(tokenize_line(prompt_ti, self.pipe.tokenizer), compel)\n",
        "            negative_prompt_emb = merge_embeds(tokenize_line(negative_prompt_ti, self.pipe.tokenizer), compel)\n",
        "\n",
        "            # fix error shape\n",
        "            if prompt_emb.shape != negative_prompt_emb.shape:\n",
        "                prompt_emb, negative_prompt_emb = compel.pad_conditioning_tensors_to_same_length([prompt_emb, negative_prompt_emb])\n",
        "\n",
        "            compel = None\n",
        "            del compel\n",
        "\n",
        "        # Prompt Optimizations for SDXL\n",
        "        else:\n",
        "            if  active_textual_inversion:\n",
        "              # Textual Inversion\n",
        "              # for name, directory_name in textual_inversion:\n",
        "              #     try:\n",
        "              #             #self.pipe.text_encoder.resize_token_embeddings(len(self.pipe.tokenizer),pad_to_multiple_of=128)\n",
        "              #             #self.pipe.load_textual_inversion(\"./bad_prompt.pt\", token=\"baddd\")\n",
        "              #             self.pipe.load_textual_inversion(directory_name, token=name)\n",
        "              #     except ValueError:\n",
        "              #         # previous loaded ti\n",
        "              #         pass\n",
        "              #     except:\n",
        "              #         print(f\"Can't apply {name}\")\n",
        "              print(\"SDXL textual inversion not available\")\n",
        "\n",
        "            #Clip skip\n",
        "            if clip_skip:\n",
        "                #clip_skip_diffusers = None #clip_skip - 1 # future update\n",
        "                compel = Compel(\n",
        "                    tokenizer=[self.pipe.tokenizer, self.pipe.tokenizer_2] ,\n",
        "                    text_encoder=[self.pipe.text_encoder, self.pipe.text_encoder_2],\n",
        "                    returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED,\n",
        "                    requires_pooled=[False, True],\n",
        "                    truncate_long_prompts=False)\n",
        "            else:\n",
        "                #clip_skip_diffusers = None # clip_skip = None # future update\n",
        "                compel = Compel(\n",
        "                    tokenizer=[self.pipe.tokenizer, self.pipe.tokenizer_2] ,\n",
        "                    text_encoder=[self.pipe.text_encoder, self.pipe.text_encoder_2],\n",
        "                    requires_pooled=[False, True],\n",
        "                    truncate_long_prompts=False)\n",
        "\n",
        "            # Prompt weights for textual inversion\n",
        "            # prompt_ti = self.pipe.maybe_convert_prompt(prompt, self.pipe.tokenizer)\n",
        "            # negative_prompt_ti = self.pipe.maybe_convert_prompt(negative_prompt, self.pipe.tokenizer)\n",
        "\n",
        "            # prompt syntax style a1...\n",
        "            if convert_weights_prompt:\n",
        "                prompt_ti = prompt_weight_conversor(prompt)\n",
        "                negative_prompt_ti = prompt_weight_conversor(negative_prompt)\n",
        "            else:\n",
        "                prompt_ti = prompt\n",
        "                negative_prompt_ti = negative_prompt\n",
        "\n",
        "            # prompt embed chunks style a1...\n",
        "            # prompt_emb = merge_embeds(tokenize_line(prompt_ti, self.pipe.tokenizer), compel)\n",
        "            # negative_prompt_emb = merge_embeds(tokenize_line(negative_prompt_ti, self.pipe.tokenizer), compel)\n",
        "\n",
        "            # fix error shape\n",
        "            # if prompt_emb.shape != negative_prompt_emb.shape:\n",
        "            #     prompt_emb, negative_prompt_emb = compel.pad_conditioning_tensors_to_same_length([prompt_emb, negative_prompt_emb])\n",
        "\n",
        "            conditioning, pooled = compel([prompt_ti, negative_prompt_ti])\n",
        "            prompt_emb = None\n",
        "            negative_prompt_emb = None\n",
        "\n",
        "            compel = None\n",
        "            del compel\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            if xformers_memory_efficient_attention:\n",
        "                self.pipe.enable_xformers_memory_efficient_attention()\n",
        "            else:\n",
        "                self.pipe.disable_xformers_memory_efficient_attention()\n",
        "\n",
        "        try:\n",
        "            self.pipe.scheduler = self.get_scheduler(sampler)\n",
        "        except:\n",
        "                print(\"Error in sampler, please try again\")\n",
        "                self.pipe = None\n",
        "                self.base_model_id = None\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "                return\n",
        "\n",
        "        self.pipe.safety_checker = None\n",
        "\n",
        "        # Get Control image\n",
        "        if self.task_name != 'txt2img':\n",
        "            if isinstance(image, str):\n",
        "                # If the input is a string (file path), open it as an image\n",
        "                image_pil = Image.open(image)\n",
        "                numpy_array = np.array(image_pil, dtype=np.uint8)\n",
        "            elif isinstance(image, Image.Image):\n",
        "                # If the input is already a PIL Image, convert it to a NumPy array\n",
        "                numpy_array = np.array(image, dtype=np.uint8)\n",
        "            elif isinstance(image, np.ndarray):\n",
        "                # If the input is a NumPy array, np.uint8\n",
        "                numpy_array = image.astype(np.uint8)\n",
        "            else:\n",
        "                self.pipe = None\n",
        "                self.base_model_id = None\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "                if gui_active:\n",
        "                    print(\"To use this function, you have to upload an image in the cell below first üëá\")\n",
        "                    return\n",
        "                else:\n",
        "                    raise ValueError(\"Unsupported image type or not control image found\")\n",
        "\n",
        "            # Extract the RGB channels\n",
        "            try:\n",
        "                array_rgb = numpy_array[:, :, :3]\n",
        "            except:\n",
        "                print(\"Unsupported image type\")\n",
        "                self.pipe = None\n",
        "                self.base_model_id = None\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "                raise ValueError(\"Unsupported image type\") # return\n",
        "\n",
        "        # Get params preprocess\n",
        "        preprocess_params_config = {}\n",
        "        if self.task_name != 'txt2img' and self.task_name != 'Inpaint':\n",
        "            print(\"resolution is control_image(min(W, H))\")\n",
        "            preprocess_params_config[\"image\"] = array_rgb\n",
        "            preprocess_params_config[\"image_resolution\"] = image_resolution # min(img_height.value, img_width.value)\n",
        "            #preprocess_params_config[\"additional_prompt\"] = additional_prompt # \"\"\n",
        "\n",
        "            if self.task_name != \"ip2p\" and self.task_name != \"Canny\":\n",
        "                if  self.task_name != \"shuffle\":\n",
        "                    preprocess_params_config[\"preprocess_resolution\"] = preprocess_resolution\n",
        "                if self.task_name != \"MLSD\":\n",
        "                    preprocess_params_config[\"preprocessor_name\"] = preprocessor_name\n",
        "\n",
        "        # RUN Preprocess\n",
        "        if self.task_name == 'Inpaint':\n",
        "\n",
        "            # Get mask for Inpaint\n",
        "            if gui_active or os.path.exists(image_mask):\n",
        "                # Read image mask from gui\n",
        "                mask_control_img = Image.open(image_mask)\n",
        "                numpy_array_mask = np.array(mask_control_img, dtype=np.uint8)\n",
        "                array_rgb_mask = numpy_array_mask[:, :, :3]\n",
        "            elif not gui_active:\n",
        "                # Convert control image for draw\n",
        "                name_without_extension = os.path.splitext(image.split('/')[-1])[0]\n",
        "                image64 = base64.b64encode(open(image, 'rb').read())\n",
        "                image64 = image64.decode('utf-8')\n",
        "                img = np.array(plt.imread(f'{image}')[:,:,:3])\n",
        "\n",
        "                # Create mask interactive\n",
        "                draw(image64, filename=f\"./{name_without_extension}_draw.png\", w=img.shape[1], h=img.shape[0], line_width=0.04*img.shape[1])\n",
        "\n",
        "                # Create mask and save\n",
        "                with_mask = np.array(plt.imread(f\"./{name_without_extension}_draw.png\")[:,:,:3])\n",
        "                mask = (with_mask[:,:,0]==1)*(with_mask[:,:,1]==0)*(with_mask[:,:,2]==0)\n",
        "                plt.imsave(f\"./{name_without_extension}_mask.png\",mask, cmap='gray')\n",
        "                mask_control = f\"./{name_without_extension}_mask.png\"\n",
        "                print(f'Mask saved: {mask_control}')\n",
        "\n",
        "                # Read image mask\n",
        "                mask_control_img = Image.open(mask_control)\n",
        "                numpy_array_mask = np.array(mask_control_img, dtype=np.uint8)\n",
        "                array_rgb_mask = numpy_array_mask[:, :, :3]\n",
        "            else:\n",
        "                raise ValueError(\"No images found\")\n",
        "\n",
        "            init_image, control_mask, control_image = self.process_inpaint(\n",
        "                image=array_rgb,\n",
        "                image_resolution=image_resolution, ### edit\n",
        "                preprocess_resolution=preprocess_resolution, # edit size in Inpaint\n",
        "                image_mask=array_rgb_mask,\n",
        "            )\n",
        "\n",
        "        elif self.task_name == 'Openpose':\n",
        "            print('Openpose')\n",
        "            control_image = self.process_openpose(**preprocess_params_config)\n",
        "\n",
        "        elif self.task_name == 'Canny':\n",
        "            print('Canny')\n",
        "            control_image = self.process_canny(\n",
        "                **preprocess_params_config,\n",
        "                low_threshold=low_threshold,\n",
        "                high_threshold=high_threshold,\n",
        "            )\n",
        "\n",
        "        elif self.task_name == 'MLSD':\n",
        "            print('MLSD')\n",
        "            control_image = self.process_mlsd(\n",
        "                **preprocess_params_config,\n",
        "                value_threshold=value_threshold,\n",
        "                distance_threshold=distance_threshold,\n",
        "            )\n",
        "\n",
        "        elif self.task_name == 'scribble':\n",
        "            print('Scribble')\n",
        "            control_image = self.process_scribble(**preprocess_params_config)\n",
        "\n",
        "        elif self.task_name == 'softedge':\n",
        "            print('Softedge')\n",
        "            control_image = self.process_softedge(**preprocess_params_config)\n",
        "\n",
        "        elif self.task_name == 'segmentation':\n",
        "            print('Segmentation')\n",
        "            control_image = self.process_segmentation(**preprocess_params_config)\n",
        "\n",
        "        elif self.task_name == 'depth':\n",
        "            print('Depth')\n",
        "            control_image = self.process_depth(**preprocess_params_config)\n",
        "\n",
        "        elif self.task_name == 'NormalBae':\n",
        "            print('NormalBae')\n",
        "            control_image = self.process_normal(**preprocess_params_config)\n",
        "\n",
        "        elif 'lineart' in self.task_name:\n",
        "            print('Lineart')\n",
        "            control_image = self.process_lineart(**preprocess_params_config)\n",
        "\n",
        "        elif self.task_name == 'shuffle':\n",
        "            print('Shuffle')\n",
        "            control_image = self.process_shuffle(**preprocess_params_config)\n",
        "\n",
        "        elif self.task_name == 'ip2p':\n",
        "            print('Ip2p')\n",
        "            control_image = self.process_ip2p(**preprocess_params_config)\n",
        "\n",
        "\n",
        "        # Get params for TASK\n",
        "        pipe_params_config = {\n",
        "            \"prompt\": None, #prompt, #self.get_prompt(prompt, additional_prompt),\n",
        "            \"negative_prompt\": None, #negative_prompt,\n",
        "            \"prompt_embeds\": prompt_emb,\n",
        "            \"negative_prompt_embeds\": negative_prompt_emb,\n",
        "            \"num_images\": num_images,\n",
        "            \"num_steps\": num_steps,\n",
        "            \"guidance_scale\": guidance_scale,\n",
        "            \"clip_skip\": None, #clip_skip, because we use clip skip of compel\n",
        "        }\n",
        "\n",
        "        if not os.path.exists(self.base_model_id):\n",
        "            # pipe_params_config[\"prompt_embeds\"]                 = conditioning[0:1]\n",
        "            # pipe_params_config[\"pooled_prompt_embeds\"]          = pooled[0:1],\n",
        "            # pipe_params_config[\"negative_prompt_embeds\"]        = conditioning[1:2]\n",
        "            # pipe_params_config[\"negative_pooled_prompt_embeds\"] = pooled[1:2],\n",
        "            # pipe_params_config[\"conditioning\"]                  = conditioning,\n",
        "            # pipe_params_config[\"pooled\"]                        = pooled,\n",
        "            # pipe_params_config[\"height\"]                        = img_height\n",
        "            # pipe_params_config[\"width\"]                         = img_width\n",
        "            pass\n",
        "        elif self.task_name == 'txt2img':\n",
        "            pipe_params_config[\"height\"]                        = img_height\n",
        "            pipe_params_config[\"width\"]                         = img_width\n",
        "        elif self.task_name == \"Inpaint\":\n",
        "            pipe_params_config[\"strength\"]                      = strength\n",
        "            pipe_params_config[\"init_image\"]                    = init_image\n",
        "            pipe_params_config[\"control_mask\"]                  = control_mask\n",
        "            pipe_params_config[\"control_image\"]                 = control_image\n",
        "            pipe_params_config[\"controlnet_conditioning_scale\"] = controlnet_conditioning_scale\n",
        "            pipe_params_config[\"control_guidance_start\"]        = control_guidance_start\n",
        "            pipe_params_config[\"control_guidance_end\"]          = control_guidance_end\n",
        "        elif self.task_name != 'txt2img' and self.task_name != 'Inpaint':\n",
        "            pipe_params_config[\"control_image\"]                 = control_image\n",
        "            pipe_params_config[\"controlnet_conditioning_scale\"] = controlnet_conditioning_scale\n",
        "            pipe_params_config[\"control_guidance_start\"]        = control_guidance_start\n",
        "            pipe_params_config[\"control_guidance_end\"]          = control_guidance_end\n",
        "\n",
        "        ### RUN PIPE ###\n",
        "        for i in range (loop_generation):\n",
        "\n",
        "            calculate_seed = random.randint(0, 2147483647) if seed == -1 else seed\n",
        "            if generator_in_cpu:\n",
        "                pipe_params_config[\"generator\"] = torch.Generator().manual_seed(calculate_seed)\n",
        "            else:\n",
        "                try:\n",
        "                    pipe_params_config[\"generator\"] = torch.Generator(\"cuda\").manual_seed(calculate_seed)\n",
        "                except:\n",
        "                    print(\"Generator in CPU\")\n",
        "                    pipe_params_config[\"generator\"] = torch.Generator().manual_seed(calculate_seed)\n",
        "\n",
        "            if not os.path.exists(self.base_model_id):\n",
        "                #images = self.run_pipe_SDXL(**pipe_params_config)\n",
        "                images = self.pipe(\n",
        "                    prompt = None,\n",
        "                    negative_prompt = None,\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    height = img_height,\n",
        "                    width = img_width,\n",
        "                    num_inference_steps = num_steps,\n",
        "                    guidance_scale = guidance_scale,\n",
        "                    clip_skip = None,\n",
        "                    num_images_per_prompt = num_images,\n",
        "                    generator = pipe_params_config[\"generator\"],\n",
        "                ).images\n",
        "            elif self.task_name == 'txt2img':\n",
        "                images = self.run_pipe_SD(**pipe_params_config)\n",
        "            elif self.task_name == \"Inpaint\":\n",
        "                images = self.run_pipe_inpaint(**pipe_params_config)\n",
        "            elif self.task_name != 'txt2img' and self.task_name != 'Inpaint':\n",
        "                results = self.run_pipe(**pipe_params_config) ## pipe ControlNet add condition_weights\n",
        "                images =  [control_image] + results\n",
        "                del results\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            if loop_generation > 1:\n",
        "\n",
        "                mediapy.show_images(images)\n",
        "                #print(image_list)\n",
        "                #del images\n",
        "                time.sleep(1)\n",
        "\n",
        "            # save image\n",
        "            image_list = []\n",
        "            metadata = [\n",
        "                    prompt,\n",
        "                    negative_prompt,\n",
        "                    self.base_model_id,\n",
        "                    self.vae_model,\n",
        "                    num_steps,\n",
        "                    guidance_scale,\n",
        "                    sampler,\n",
        "                    calculate_seed\n",
        "            ]\n",
        "\n",
        "            for image_ in images:\n",
        "                image_path = save_pil_image_with_metadata(image_, \"./images\", metadata)\n",
        "                image_list.append(image_path)\n",
        "\n",
        "            if loop_generation > 1:\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "                print(image_list)\n",
        "            print(f\"Seed:\\n{calculate_seed}\")\n",
        "\n",
        "\n",
        "\n",
        "        # if select_lora1.value != \"None\":\n",
        "        #     model.pipe.unfuse_lora()\n",
        "        #     model.pipe.unload_lora_weights()\n",
        "        # if select_lora2.value != \"None\" or select_lora3.value != \"None\":\n",
        "        #     print('BETA: reload weights for lora')\n",
        "        #     model.load_pipe(select_model.value, task_name=options_controlnet.value, vae_model = vae_model_dropdown.value, reload=True)\n",
        "        if xformers_memory_efficient_attention and torch.cuda.is_available():\n",
        "            self.pipe.disable_xformers_memory_efficient_attention()\n",
        "        self.pipe.to(self.device)\n",
        "        self.pipe = self.process_lora(lora_A, lora_scale_A, unload=True)\n",
        "        self.pipe = self.process_lora(lora_B, lora_scale_B, unload=True)\n",
        "        self.pipe = self.process_lora(lora_C, lora_scale_C, unload=True)\n",
        "        self.pipe = self.process_lora(lora_D, lora_scale_D, unload=True)\n",
        "        self.pipe = self.process_lora(lora_E, lora_scale_E, unload=True)\n",
        "        # model.pipe.unfuse_lora()\n",
        "        # model.pipe.unload_lora_weights()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return images, image_list\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# Prompt weights\n",
        "# =====================================\n",
        "from compel import Compel\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "import re\n",
        "from compel import EmbeddingsProvider, ReturnedEmbeddingsType\n",
        "\n",
        "def concat_tensor(t):\n",
        "    t_list = torch.split(t, 1, dim=0)\n",
        "    t = torch.cat(t_list, dim=1)\n",
        "    return t\n",
        "\n",
        "def merge_embeds(prompt_chanks, compel):\n",
        "    num_chanks = len(prompt_chanks)\n",
        "    power_prompt = 1/(num_chanks*(num_chanks+1)//2)\n",
        "    prompt_embs = compel(prompt_chanks)\n",
        "    t_list = list(torch.split(prompt_embs, 1, dim=0))\n",
        "    for i in range(num_chanks):\n",
        "        t_list[-(i+1)] = t_list[-(i+1)] * ((i+1)*power_prompt)\n",
        "    prompt_emb = torch.stack(t_list, dim=0).sum(dim=0)\n",
        "    return prompt_emb\n",
        "\n",
        "def detokenize(chunk, actual_prompt):\n",
        "    chunk[-1] = chunk[-1].replace('</w>', '')\n",
        "    chanked_prompt = ''.join(chunk).strip()\n",
        "    while '</w>' in chanked_prompt:\n",
        "        if actual_prompt[chanked_prompt.find('</w>')] == ' ':\n",
        "            chanked_prompt = chanked_prompt.replace('</w>', ' ', 1)\n",
        "        else:\n",
        "            chanked_prompt = chanked_prompt.replace('</w>', '', 1)\n",
        "    actual_prompt = actual_prompt.replace(chanked_prompt,'')\n",
        "    return chanked_prompt.strip(), actual_prompt.strip()\n",
        "\n",
        "def tokenize_line(line, tokenizer): # split into chunks\n",
        "    actual_prompt = line.lower().strip()\n",
        "    if actual_prompt == \"\":\n",
        "      actual_prompt = 'worst quality'\n",
        "    actual_tokens = tokenizer.tokenize(actual_prompt)\n",
        "    max_tokens = tokenizer.model_max_length - 2\n",
        "    comma_token = tokenizer.tokenize(',')[0]\n",
        "\n",
        "    chunks = []\n",
        "    chunk = []\n",
        "    for item in actual_tokens:\n",
        "        chunk.append(item)\n",
        "        if len(chunk) == max_tokens:\n",
        "            if chunk[-1] != comma_token:\n",
        "                for i in range(max_tokens-1, -1, -1):\n",
        "                    if chunk[i] == comma_token:\n",
        "                        actual_chunk, actual_prompt = detokenize(chunk[:i+1], actual_prompt)\n",
        "                        chunks.append(actual_chunk)\n",
        "                        chunk = chunk[i+1:]\n",
        "                        break\n",
        "                else:\n",
        "                    actual_chunk, actual_prompt = detokenize(chunk, actual_prompt)\n",
        "                    chunks.append(actual_chunk)\n",
        "                    chunk = []\n",
        "            else:\n",
        "                actual_chunk, actual_prompt = detokenize(chunk, actual_prompt)\n",
        "                chunks.append(actual_chunk)\n",
        "                chunk = []\n",
        "    if chunk:\n",
        "        actual_chunk, _ = detokenize(chunk, actual_prompt)\n",
        "        chunks.append(actual_chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def prompt_weight_conversor(input_string):\n",
        "    # Convert prompt weights from a1... to comel\n",
        "\n",
        "    # Find and replace instances of the colon format with the desired format\n",
        "    converted_string = re.sub(r'\\(([^:]+):([\\d.]+)\\)', r'(\\1)\\2', input_string)\n",
        "\n",
        "    # Find and replace square brackets with round brackets and assign weight\n",
        "    converted_string = re.sub(r'\\[([^:\\]]+)\\]', r'(\\1)0.909090909', converted_string)\n",
        "\n",
        "    # Handle the general case of [x:number] and convert it to (x)0.9\n",
        "    converted_string = re.sub(r'\\[([^:]+):[\\d.]+\\]', r'(\\1)0.9', converted_string)\n",
        "\n",
        "    # Add a '+' sign after the closing parenthesis if no weight is specified\n",
        "    modified_string = re.sub(r'\\(([^)]+)\\)(?![\\d.])', r'(\\1)+', converted_string)\n",
        "\n",
        "    # double (())\n",
        "    #modified_string = re.sub(r'\\(\\(([^)]+)\\)\\+\\)', r'(\\1)++', modified_string)\n",
        "\n",
        "    # triple ((()))\n",
        "    #modified_string = re.sub(r'\\(\\(([^)]+)\\)\\+\\+\\)', r'(\\1)+++', modified_string)\n",
        "\n",
        "    #print(modified_string)\n",
        "    return modified_string\n",
        "\n",
        "# =====================================\n",
        "# IMAGE: METADATA AND SAVE\n",
        "# =====================================\n",
        "import os\n",
        "from PIL import Image\n",
        "from PIL.PngImagePlugin import PngInfo\n",
        "\n",
        "def save_pil_image_with_metadata(image, folder_path, metadata_list):\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "\n",
        "    existing_files = os.listdir(folder_path)\n",
        "\n",
        "    # Determine the next available image name\n",
        "    image_name = f\"image{str(len(existing_files) + 1).zfill(3)}.png\"\n",
        "    image_path = os.path.join(folder_path, image_name)\n",
        "\n",
        "    try:\n",
        "        # metadata\n",
        "        metadata = PngInfo()\n",
        "        metadata.add_text(\"Prompt\", str(metadata_list[0]))\n",
        "        metadata.add_text(\"Negative prompt\", str(metadata_list[1]))\n",
        "        metadata.add_text(\"Model\", str(metadata_list[2]))\n",
        "        metadata.add_text(\"VAE\", str(metadata_list[3]))\n",
        "        metadata.add_text(\"Steps\", str(metadata_list[4]))\n",
        "        metadata.add_text(\"CFG\", str(metadata_list[5]))\n",
        "        metadata.add_text(\"Scheduler\", str(metadata_list[6]))\n",
        "        metadata.add_text(\"Seed\", str(metadata_list[7]))\n",
        "\n",
        "        image.save(image_path, pnginfo=metadata)\n",
        "    except:\n",
        "        print('Saving image without metadata')\n",
        "        image.save(image_path)\n",
        "\n",
        "    return image_path\n",
        "\n",
        "# =====================================\n",
        "# LoRA Loaders\n",
        "# =====================================\n",
        "import torch\n",
        "from safetensors.torch import load_file\n",
        "from collections import defaultdict\n",
        "def load_lora_weights(pipeline, checkpoint_path, multiplier, device, dtype):\n",
        "    LORA_PREFIX_UNET = \"lora_unet\"\n",
        "    LORA_PREFIX_TEXT_ENCODER = \"lora_te\"\n",
        "    # load LoRA weight from .safetensors\n",
        "    if isinstance(checkpoint_path, str):\n",
        "\n",
        "        state_dict = load_file(checkpoint_path, device=device)\n",
        "\n",
        "        updates = defaultdict(dict)\n",
        "        for key, value in state_dict.items():\n",
        "            # it is suggested to print out the key, it usually will be something like below\n",
        "            # \"lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\"\n",
        "\n",
        "            layer, elem = key.split('.', 1)\n",
        "            updates[layer][elem] = value\n",
        "\n",
        "        # directly update weight in diffusers model\n",
        "        for layer, elems in updates.items():\n",
        "\n",
        "            if \"text\" in layer:\n",
        "                layer_infos = layer.split(LORA_PREFIX_TEXT_ENCODER + \"_\")[-1].split(\"_\")\n",
        "                curr_layer = pipeline.text_encoder\n",
        "            else:\n",
        "                layer_infos = layer.split(LORA_PREFIX_UNET + \"_\")[-1].split(\"_\")\n",
        "                curr_layer = pipeline.unet\n",
        "\n",
        "            # find the target layer\n",
        "            temp_name = layer_infos.pop(0)\n",
        "            while len(layer_infos) > -1:\n",
        "                try:\n",
        "                    curr_layer = curr_layer.__getattr__(temp_name)\n",
        "                    if len(layer_infos) > 0:\n",
        "                        temp_name = layer_infos.pop(0)\n",
        "                    elif len(layer_infos) == 0:\n",
        "                        break\n",
        "                except Exception:\n",
        "                    if len(temp_name) > 0:\n",
        "                        temp_name += \"_\" + layer_infos.pop(0)\n",
        "                    else:\n",
        "                        temp_name = layer_infos.pop(0)\n",
        "\n",
        "            # get elements for this layer\n",
        "            weight_up = elems['lora_up.weight'].to(dtype)\n",
        "            weight_down = elems['lora_down.weight'].to(dtype)\n",
        "            alpha = elems['alpha']\n",
        "            if alpha:\n",
        "                alpha = alpha.item() / weight_up.shape[1]\n",
        "            else:\n",
        "                alpha = 1.0\n",
        "\n",
        "            # update weight\n",
        "            if len(weight_up.shape) == 4:\n",
        "                curr_layer.weight.data += multiplier * alpha * torch.mm(weight_up.squeeze(3).squeeze(2), weight_down.squeeze(3).squeeze(2)).unsqueeze(2).unsqueeze(3)\n",
        "            else:\n",
        "                curr_layer.weight.data += multiplier * alpha * torch.mm(weight_up, weight_down)\n",
        "    else:\n",
        "        for ckptpath in checkpoint_path:\n",
        "            state_dict = load_file(ckptpath, device=device)\n",
        "\n",
        "            updates = defaultdict(dict)\n",
        "            for key, value in state_dict.items():\n",
        "                # it is suggested to print out the key, it usually will be something like below\n",
        "                # \"lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\"\n",
        "\n",
        "                layer, elem = key.split('.', 1)\n",
        "                updates[layer][elem] = value\n",
        "\n",
        "            # directly update weight in diffusers model\n",
        "            for layer, elems in updates.items():\n",
        "\n",
        "                if \"text\" in layer:\n",
        "                    layer_infos = layer.split(LORA_PREFIX_TEXT_ENCODER + \"_\")[-1].split(\"_\")\n",
        "                    curr_layer = pipeline.text_encoder\n",
        "                else:\n",
        "                    layer_infos = layer.split(LORA_PREFIX_UNET + \"_\")[-1].split(\"_\")\n",
        "                    curr_layer = pipeline.unet\n",
        "\n",
        "                # find the target layer\n",
        "                temp_name = layer_infos.pop(0)\n",
        "                while len(layer_infos) > -1:\n",
        "                    try:\n",
        "                        curr_layer = curr_layer.__getattr__(temp_name)\n",
        "                        if len(layer_infos) > 0:\n",
        "                            temp_name = layer_infos.pop(0)\n",
        "                        elif len(layer_infos) == 0:\n",
        "                            break\n",
        "                    except Exception:\n",
        "                        if len(temp_name) > 0:\n",
        "                            temp_name += \"_\" + layer_infos.pop(0)\n",
        "                        else:\n",
        "                            temp_name = layer_infos.pop(0)\n",
        "\n",
        "                # get elements for this layer\n",
        "                weight_up = elems['lora_up.weight'].to(dtype)\n",
        "                weight_down = elems['lora_down.weight'].to(dtype)\n",
        "                alpha = elems['alpha']\n",
        "                if alpha:\n",
        "                    alpha = alpha.item() / weight_up.shape[1]\n",
        "                else:\n",
        "                    alpha = 1.0\n",
        "\n",
        "                # update weight\n",
        "                if len(weight_up.shape) == 4:\n",
        "                    curr_layer.weight.data += multiplier * alpha * torch.mm(weight_up.squeeze(3).squeeze(2), weight_down.squeeze(3).squeeze(2)).unsqueeze(2).unsqueeze(3)\n",
        "                else:\n",
        "                    curr_layer.weight.data += multiplier * alpha * torch.mm(weight_up, weight_down)\n",
        "    return pipeline\n",
        "\n",
        "def lora_mix_load(pipe, lora_path, alpha_scale=1.0, device='cuda', dtype=torch.float16):\n",
        "    try:\n",
        "        pipe=load_lora_weights(pipe, [lora_path], alpha_scale, device=device, dtype=torch.float16)\n",
        "    except:\n",
        "        pipe.load_lora_weights(lora_path)\n",
        "        pipe.fuse_lora(lora_scale=alpha_scale)\n",
        "\n",
        "    return pipe\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# Inpainting canvas\n",
        "# =====================================\n",
        "canvas_html = \"\"\"\n",
        "<style>\n",
        ".button {\n",
        "  background-color: #4CAF50;\n",
        "  border: none;\n",
        "  color: white;\n",
        "  padding: 15px 32px;\n",
        "  text-align: center;\n",
        "  text-decoration: none;\n",
        "  display: inline-block;\n",
        "  font-size: 16px;\n",
        "  margin: 4px 2px;\n",
        "  cursor: pointer;\n",
        "}\n",
        "</style>\n",
        "<canvas1 width=%d height=%d>\n",
        "</canvas1>\n",
        "<canvas width=%d height=%d>\n",
        "</canvas>\n",
        "\n",
        "<button>Finish</button>\n",
        "<script>\n",
        "var canvas = document.querySelector('canvas')\n",
        "var ctx = canvas.getContext('2d')\n",
        "\n",
        "var canvas1 = document.querySelector('canvas1')\n",
        "var ctx1 = canvas.getContext('2d')\n",
        "\n",
        "\n",
        "ctx.strokeStyle = 'red';\n",
        "\n",
        "var img = new Image();\n",
        "img.src = \"data:image/%s;charset=utf-8;base64,%s\";\n",
        "console.log(img)\n",
        "img.onload = function() {\n",
        "  ctx1.drawImage(img, 0, 0);\n",
        "};\n",
        "img.crossOrigin = 'Anonymous';\n",
        "\n",
        "ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
        "\n",
        "ctx.lineWidth = %d\n",
        "var button = document.querySelector('button')\n",
        "var mouse = {x: 0, y: 0}\n",
        "\n",
        "canvas.addEventListener('mousemove', function(e) {\n",
        "  mouse.x = e.pageX - this.offsetLeft\n",
        "  mouse.y = e.pageY - this.offsetTop\n",
        "})\n",
        "canvas.onmousedown = ()=>{\n",
        "  ctx.beginPath()\n",
        "  ctx.moveTo(mouse.x, mouse.y)\n",
        "  canvas.addEventListener('mousemove', onPaint)\n",
        "}\n",
        "canvas.onmouseup = ()=>{\n",
        "  canvas.removeEventListener('mousemove', onPaint)\n",
        "}\n",
        "var onPaint = ()=>{\n",
        "  ctx.lineTo(mouse.x, mouse.y)\n",
        "  ctx.stroke()\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "  button.onclick = ()=>{\n",
        "    resolve(canvas.toDataURL('image/png'))\n",
        "  }\n",
        "})\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "import base64, os\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from shutil import copyfile\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def draw(imgm, filename='drawing.png', w=400, h=200, line_width=1):\n",
        "  display(HTML(canvas_html % (w, h, w,h, filename.split('.')[-1], imgm, line_width)))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "\n",
        "# the control image of init_image and mask_image\n",
        "def make_inpaint_condition(image, image_mask):\n",
        "    image = np.array(image.convert(\"RGB\")).astype(np.float32) / 255.0\n",
        "    image_mask = np.array(image_mask.convert(\"L\")).astype(np.float32) / 255.0\n",
        "\n",
        "    assert image.shape[0:1] == image_mask.shape[0:1], \"image and image_mask must have the same image size\"\n",
        "    image[image_mask > 0.5] = -1.0  # set as masked pixel\n",
        "    image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return image\n",
        "\n",
        "# =====================================\n",
        "# Adetailer\n",
        "# =====================================\n",
        "from functools import partial\n",
        "from diffusers import DPMSolverMultistepScheduler, DPMSolverSinglestepScheduler\n",
        "from asdff import AdCnPipeline, AdPipeline, yolo_detector\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "def ad_model_process(\n",
        "    face_detector_ad,\n",
        "    person_detector_ad,\n",
        "    hand_detector_ad,\n",
        "    model_repo_id,\n",
        "    common,\n",
        "    inpaint,\n",
        "    image_list_task,\n",
        "    ):\n",
        "\n",
        "    pipe = AdPipeline.from_pretrained(model_repo_id, torch_dtype=torch.float16)\n",
        "\n",
        "    try:\n",
        "        pipe.scheduler = DPMSolverSinglestepScheduler.from_config(pipe.scheduler.config, use_karras_sigmas=True)\n",
        "    except:\n",
        "        pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "    pipe.safety_checker = None\n",
        "    pipe.to(\"cuda\")\n",
        "\n",
        "    image_list_ad = []\n",
        "\n",
        "    for path in image_list_task:\n",
        "        if os.path.exists(path):\n",
        "            # Open the image using PIL and convert it to PIL.Image.Image\n",
        "            with Image.open(path) as img:\n",
        "                images_ad = img.convert(\"RGB\")\n",
        "\n",
        "        detectors = []\n",
        "        if face_detector_ad:\n",
        "            face_model_path = hf_hub_download(\"Bingsu/adetailer\", \"face_yolov8n.pt\")\n",
        "            face_detector = partial(yolo_detector, model_path=face_model_path)\n",
        "            detectors.append(face_detector)\n",
        "        if person_detector_ad:\n",
        "            person_model_path = hf_hub_download(\"Bingsu/adetailer\", \"person_yolov8s-seg.pt\")\n",
        "            person_detector = partial(yolo_detector, model_path=person_model_path)\n",
        "            detectors.append(person_detector)\n",
        "        if hand_detector_ad:\n",
        "            hand_model_path = hf_hub_download(\"Bingsu/adetailer\", \"hand_yolov8n.pt\")\n",
        "            hand_detector = partial(yolo_detector, model_path=hand_model_path)\n",
        "            detectors.append(hand_detector)\n",
        "\n",
        "        result_ad = pipe(images=[images_ad], common=common, inpaint_only=inpaint, detectors=detectors)\n",
        "\n",
        "        try:\n",
        "            mediapy.show_images([result_ad[0][0], result_ad[1][0]])\n",
        "        except:\n",
        "            del pipe\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            return image_list_task\n",
        "\n",
        "        image_path = save_pil_image_with_metadata(result_ad[0][0], f'{os.getcwd()}/images', metadata_list=None)\n",
        "        image_list_ad.append(image_path)\n",
        "\n",
        "    del pipe\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return image_list_ad"
      ],
      "metadata": {
        "id": "da0BxuMj2gBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üëá Generating Images { form-width: \"20%\", display-mode: \"form\" }\n",
        "#@markdown ---\n",
        "#@markdown - **Prompt** - Description of the image\n",
        "#@markdown - **Negative Prompt** - Things you don't want to see or ignore in the image\n",
        "#@markdown - **Steps** - Number of denoising steps. Higher steps may lead to better results but takes longer time to generate the image. Default is `30`.\n",
        "#@markdown - **CFG** - Guidance scale ranging from `0` to `20`. Lower values allow the AI to be more creative and less strict at following the prompt. Default is `7.5`.\n",
        "#@markdown - **Sampler** - The scheduler is responsible for controlling the learning rate of the diffusion model. Different schedulers can produce different results, so it is important to choose a scheduler that is appropriate for the desired task.\n",
        "#@markdown - **Seed** - A random value that controls image generation. The same seed and prompt produce the same images. Set `-1` for using random seed values.\n",
        "#@markdown - **Prompt weights** -  Are a way to control the influence of different text prompts on the image generation process. Prompt weights can be used to emphasize or de-emphasize certain aspects of the image, such as the object, the scene, or the style. Currently, the [Compel syntax](https://github.com/damian0815/compel/blob/main/doc/syntax.md) is being used. You can also activate the `Convert Prompt weights` to automatically convert syntax from `(word:1.1)` to `(word)1.1` or `(word)` to `(word)+` to make them compatible with Compel weights. Compel scale more with its values so that fewer weights are needed for good results\n",
        "#@markdown - **ControlNet** - Is a neural network model for controlling diffusion models. It allows users to input additional information, such as edge maps, segmentation maps, and key points, into diffusion models to guide the image generation process.\n",
        "#@markdown ---\n",
        "%cd /content\n",
        "import ipywidgets as widgets, mediapy, random\n",
        "from diffusers.models.attention_processor import AttnProcessor2_0\n",
        "from PIL import Image\n",
        "import IPython.display\n",
        "import time\n",
        "from IPython.utils import capture\n",
        "import logging\n",
        "from ipywidgets import Button, Layout, jslink, IntText, IntSlider, BoundedFloatText\n",
        "logging.getLogger(\"diffusers\").setLevel(logging.ERROR)\n",
        "\n",
        "#from IPython.display import display\n",
        "from ipywidgets import TwoByTwoLayout, Layout, Button, Box, FloatText, Textarea, Dropdown, Label, IntSlider, interactive, HBox, VBox, BoundedIntText, BoundedFloatText\n",
        "\n",
        "#PARAMETER WIDGETS\n",
        "width = \"225px\"\n",
        "auto_layout = widgets.Layout(height='auto', width='auto')\n",
        "lora_layout = {'width':'165px'}\n",
        "lora_scale_layout = {'width':'50px'}\n",
        "style = {'description_width': 'initial'} # show full text\n",
        "\n",
        "# =====================================\n",
        "# Left\n",
        "# =====================================\n",
        "\n",
        "num_images = widgets.BoundedIntText(\n",
        "    value = 1,\n",
        "    min = 1,\n",
        "    description=\"Images:\",\n",
        "    layout=widgets.Layout(width=width)\n",
        ")\n",
        "\n",
        "steps = widgets.BoundedIntText(\n",
        "    value = 30,\n",
        "    min = 1,\n",
        "    max = 100,\n",
        "    description=\"Steps:\",\n",
        "    layout=widgets.Layout(width=width)\n",
        ")\n",
        "\n",
        "CFG = widgets.BoundedFloatText(\n",
        "    value = 7.5,\n",
        "    min = 0,\n",
        "    step=0.5,\n",
        "    description=\"CFG:\",\n",
        "    layout=widgets.Layout(width=width)\n",
        ")\n",
        "\n",
        "select_sampler = widgets.Dropdown(\n",
        "    options=[\n",
        "        \"DPM++ 2M\",\n",
        "        \"DPM++ 2M Karras\",\n",
        "        \"DPM++ 2M SDE\",\n",
        "        \"DPM++ 2M SDE Karras\",\n",
        "        \"DPM++ SDE\",\n",
        "        \"DPM++ SDE Karras\",\n",
        "        \"DPM2\",\n",
        "        \"DPM2 Karras\",\n",
        "        \"Euler\",\n",
        "        \"Euler a\",\n",
        "        \"Heun\",\n",
        "        \"LMS\",\n",
        "        \"LMS Karras\",\n",
        "        \"DDIMScheduler\",\n",
        "        \"DEISMultistepScheduler\",\n",
        "        \"UniPCMultistepScheduler\",\n",
        "    ],\n",
        "    description=\"Sampler:\",\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "img_height = widgets.BoundedIntText(\n",
        "    min=64,\n",
        "    max=4096,\n",
        "    value=512,\n",
        "    description=\"Height:\",\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "img_width = widgets.BoundedIntText(\n",
        "    min=64,\n",
        "    max=4096,\n",
        "    value=512,\n",
        "    description=\"Width:\",\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "random_seed = widgets.IntText(\n",
        "    value=-1,\n",
        "    description=\"Seed:\",\n",
        "    layout=widgets.Layout(width=width),\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "#lora1\n",
        "select_lora1 = widgets.Dropdown(\n",
        "    options=lora_model_list,\n",
        "    description=\"Lora1:\",\n",
        "    layout=lora_layout\n",
        ")\n",
        "\n",
        "lora_weights_scale1 = widgets.FloatText(\n",
        "    min=-2.0,\n",
        "    max=2.0,\n",
        "    step=0.01,\n",
        "    value=1,\n",
        "    #description=\"Lora scale1:\",\n",
        "    layout=lora_scale_layout\n",
        ")\n",
        "#lora2\n",
        "select_lora2 = widgets.Dropdown(\n",
        "    options=lora_model_list,\n",
        "    description=\"Lora2:\",\n",
        "    layout=lora_layout\n",
        ")\n",
        "\n",
        "lora_weights_scale2 = widgets.FloatText(\n",
        "    min=-2.0,\n",
        "    max=2.0,\n",
        "    step=0.01,\n",
        "    value=1,\n",
        "    #description=\"Lora scale2:\",\n",
        "    layout=lora_scale_layout\n",
        ")\n",
        "#lora3\n",
        "select_lora3 = widgets.Dropdown(\n",
        "    options=lora_model_list,\n",
        "    description=\"Lora3:\",\n",
        "    layout=lora_layout\n",
        ")\n",
        "\n",
        "lora_weights_scale3 = widgets.FloatText(\n",
        "    min=-2.0,\n",
        "    max=2.0,\n",
        "    step=0.01,\n",
        "    value=1,\n",
        "    #description=\"Lora scale3:\",\n",
        "    layout=lora_scale_layout\n",
        ")\n",
        "#lora4\n",
        "select_lora4 = widgets.Dropdown(\n",
        "    options=lora_model_list,\n",
        "    description=\"Lora4:\",\n",
        "    layout=lora_layout\n",
        ")\n",
        "\n",
        "lora_weights_scale4 = widgets.FloatText(\n",
        "    min=-2.0,\n",
        "    max=2.0,\n",
        "    step=0.01,\n",
        "    value=1,\n",
        "    #description=\"Lora scale4:\",\n",
        "    layout=lora_scale_layout\n",
        ")\n",
        "#lora5\n",
        "select_lora5 = widgets.Dropdown(\n",
        "    options=lora_model_list,\n",
        "    description=\"Lora5:\",\n",
        "    layout=lora_layout\n",
        ")\n",
        "\n",
        "lora_weights_scale5 = widgets.FloatText(\n",
        "    min=-2.0,\n",
        "    max=2.0,\n",
        "    step=0.01,\n",
        "    value=1,\n",
        "    #description=\"Lora scale5:\",\n",
        "    layout=lora_scale_layout\n",
        ")\n",
        "\n",
        "select_clip_skip = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Layer 2 Clip Skip',\n",
        "    layout=widgets.Layout(width=width),\n",
        ")\n",
        "\n",
        "# =====================================\n",
        "# Center\n",
        "# =====================================\n",
        "\n",
        "display_imgs = widgets.Output() # layout={'border': '1px solid black'}\n",
        "\n",
        "select_model = widgets.Dropdown(\n",
        "    options=model_list,\n",
        "    description=\"Model:\",\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "vae_model_dropdown = widgets.Dropdown(\n",
        "    options=vae_model_list,\n",
        "    description=\"VAE:\",\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "prompt = widgets.Textarea(\n",
        "    value=\"\",\n",
        "    placeholder=\"Enter prompt\",\n",
        "    rows=5,\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "neg_prompt = widgets.Textarea(\n",
        "    value=\"\",\n",
        "    placeholder=\"Enter negative prompt\",\n",
        "    rows=5,\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "active_ti = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Active Textual Inversion in prompt (Experimental)',\n",
        "    #style = style,\n",
        "    layout=auto_layout,\n",
        ")\n",
        "# alternative prompt weights\n",
        "weights_prompt = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Convert Prompt weights',\n",
        "    #style = style,\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "generate = widgets.Button(\n",
        "    description=\"Generate\",\n",
        "    disabled=False,\n",
        "    button_style=\"primary\",\n",
        "    layout=Layout(height='auto', width='auto'),\n",
        ")\n",
        "\n",
        "### GENERATE ###\n",
        "\n",
        "def generate_img(i):\n",
        "  global model\n",
        "  #Clear output\n",
        "  display_imgs.clear_output()\n",
        "  generate.disabled = True\n",
        "\n",
        "  with display_imgs:\n",
        "\n",
        "    print(\"Running...\")\n",
        "\n",
        "\n",
        "    # First load\n",
        "    try:\n",
        "        model\n",
        "    except:\n",
        "        model = Model(base_model_id=select_model.value, task_name=select_task.value, vae_model = vae_model_dropdown.value, type_model_precision = model_precision.value)\n",
        "\n",
        "    model.load_pipe(select_model.value, task_name=select_task.value, vae_model = vae_model_dropdown.value, type_model_precision = model_precision.value)\n",
        "\n",
        "    display_imgs.clear_output()\n",
        "\n",
        "    try:\n",
        "      preprocessor_name_found = int_inputs[select_task.value][0].value\n",
        "    except:\n",
        "      preprocessor_name_found = None\n",
        "\n",
        "    global destination_path_cn_img, mask_control, image_list\n",
        "\n",
        "    image_control_base = None\n",
        "    if select_task.value != \"txt2img\":\n",
        "        try:\n",
        "            image_control_base = destination_path_cn_img\n",
        "        except:\n",
        "            print(\"No control image found\")\n",
        "            generate.disabled = False\n",
        "            return\n",
        "\n",
        "    mask_control_base = None\n",
        "    if select_task.value == \"Inpaint\":\n",
        "        if os.path.exists(int_inputs['Inpaint'][1].value):\n",
        "            mask_control_base = int_inputs['Inpaint'][1].value\n",
        "        else:\n",
        "            try:\n",
        "                mask_control_base = mask_control\n",
        "            except:\n",
        "                print(\"No mask image found\")\n",
        "                generate.disabled = False\n",
        "                return\n",
        "\n",
        "    pipe_params = {\n",
        "    \"prompt\": prompt.value,\n",
        "    \"negative_prompt\": neg_prompt.value,\n",
        "    \"img_height\": img_height.value,\n",
        "    \"img_width\": img_width.value,\n",
        "    \"num_images\": num_images.value,\n",
        "    \"num_steps\": steps.value,\n",
        "    \"guidance_scale\": CFG.value,\n",
        "    \"clip_skip\": select_clip_skip.value,\n",
        "    \"seed\": random_seed.value,\n",
        "    \"image\": image_control_base,\n",
        "    \"preprocessor_name\": preprocessor_name_found,\n",
        "    \"preprocess_resolution\": preprocess_resolution_global.value,\n",
        "    \"image_resolution\": min(img_height.value, img_width.value), # min\n",
        "    \"additional_prompt\": \"\",\n",
        "    \"image_mask\": mask_control_base,\n",
        "    \"strength\": int_inputs['Inpaint'][0].value, # only for Inpaint\n",
        "    \"low_threshold\": int_inputs['Canny'][0].value,\n",
        "    \"high_threshold\": int_inputs['Canny'][1].value,\n",
        "    \"value_threshold\": int_inputs['MLSD'][0].value,\n",
        "    \"distance_threshold\": int_inputs['MLSD'][1].value,\n",
        "    \"lora_A\": select_lora1.value,\n",
        "    \"lora_scale_A\": lora_weights_scale1.value,\n",
        "    \"lora_B\": select_lora2.value,\n",
        "    \"lora_scale_B\": lora_weights_scale2.value,\n",
        "    \"lora_C\": select_lora3.value,\n",
        "    \"lora_scale_C\": lora_weights_scale3.value,\n",
        "    \"lora_D\": select_lora4.value,\n",
        "    \"lora_scale_D\": lora_weights_scale4.value,\n",
        "    \"lora_E\": select_lora5.value,\n",
        "    \"lora_scale_E\": lora_weights_scale5.value,\n",
        "    \"active_textual_inversion\": active_ti.value,\n",
        "    \"textual_inversion\": embed_list,\n",
        "    \"convert_weights_prompt\": weights_prompt.value,\n",
        "    \"sampler\": select_sampler.value,\n",
        "    \"xformers_memory_efficient_attention\": xformers_memory_efficient_attention.value,\n",
        "    \"gui_active\": True,\n",
        "    \"loop_generation\": loop_generator.value,\n",
        "    \"controlnet_conditioning_scale\" : controlnet_output_scaling_in_unet.value,\n",
        "    \"control_guidance_start\" : controlnet_start_threshold.value,\n",
        "    \"control_guidance_end\" : controlnet_stop_threshold.value,\n",
        "    \"generator_in_cpu\" : init_generator_in_cpu.value,\n",
        "    }\n",
        "\n",
        "    images, image_list = model(**pipe_params)\n",
        "\n",
        "    if loop_generator.value == 1:\n",
        "        mediapy.show_images(images)\n",
        "        del images\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "  generate.disabled = False\n",
        "  return\n",
        "\n",
        "generate.on_click(generate_img)\n",
        "\n",
        "show_textual_inversion = widgets.Button(\n",
        "    description=\"List available textual inversions\",\n",
        "    disabled=False,\n",
        "    button_style=\"info\",\n",
        "    layout=widgets.Layout(height='auto', width='auto'),\n",
        ")\n",
        "\n",
        "def elemets_textual_inversion(value):\n",
        "  with display_imgs:\n",
        "    print('The embeddings currently supported. Write in the prompt the word for use')\n",
        "    for name, directory_name in embed_list:\n",
        "        print(name)\n",
        "    return\n",
        "\n",
        "show_textual_inversion.on_click(elemets_textual_inversion)\n",
        "\n",
        "clear_outputs = widgets.Button(\n",
        "    description=\"Clear outputs\",\n",
        "    disabled=False,\n",
        "    button_style=\"info\",\n",
        "    layout=widgets.Layout(height='auto', width='auto'),\n",
        "\n",
        ")\n",
        "\n",
        "def clear_outputs_run(value):\n",
        "  display_imgs.clear_output()\n",
        "  return\n",
        "\n",
        "clear_outputs.on_click(clear_outputs_run)\n",
        "\n",
        "# =====================================\n",
        "# Right ControlNet\n",
        "# =====================================\n",
        "\n",
        "control_model_list = list(CONTROLNET_MODEL_IDS.keys())\n",
        "\n",
        "# Create a Dropdown for selecting options\n",
        "select_task = widgets.Dropdown( #\n",
        "    options=[\n",
        "        control_model_list[13],\n",
        "        control_model_list[12],\n",
        "        control_model_list[0],\n",
        "        control_model_list[1],\n",
        "        control_model_list[2],\n",
        "        control_model_list[3],\n",
        "        control_model_list[4],\n",
        "        control_model_list[5],\n",
        "        control_model_list[6],\n",
        "        control_model_list[7],\n",
        "        control_model_list[8],\n",
        "        control_model_list[10],\n",
        "        control_model_list[11],\n",
        "    ],\n",
        "    description='TASK:',\n",
        "    layout=auto_layout,\n",
        ")\n",
        "\n",
        "controlnet_output_scaling_in_unet = widgets.FloatText(value=1.0, min=0.0, max=5.0, step=0.1, description='ControlNet Output Scaling in UNet:', style=style)\n",
        "controlnet_start_threshold = widgets.FloatSlider(value=0.0, min=0.00, max=1.0, step=0.01, description='ControlNet Start Threshold (%):', style=style)\n",
        "controlnet_stop_threshold = widgets.FloatSlider(value=1.0, min=0.00, max=1.0, step=0.01, description='ControlNet Stop Threshold (%):', style=style)\n",
        "\n",
        "preprocess_resolution_global = widgets.IntSlider(\n",
        "    value=512,\n",
        "    min=256,\n",
        "    max=2048,\n",
        "    description='Preprocess resolution',\n",
        "    style=style,\n",
        ")\n",
        "\n",
        "# Create a dictionary to map options to lists of IntText widgets\n",
        "int_inputs = {\n",
        "\n",
        "    control_model_list[13]: [\n",
        "    ],\n",
        "    control_model_list[12]: [\n",
        "        widgets.FloatSlider(value=1.0, min=0.01, max=1.0, step=0.01, description='Inpaint strength:', layout=Layout(visibility='hidden'), style=style),\n",
        "        widgets.Text(value=\"\", placeholder=\"/content/my_mask.png\", rows=1, description='Mask path:', layout=Layout(visibility='hidden'), style=style)\n",
        "    ],\n",
        "    control_model_list[0]: [\n",
        "        widgets.Dropdown(value='Openpose', description='Preprocessor:', options=['None','Openpose'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[1]: [\n",
        "        widgets.BoundedIntText(value=100, min=1, max=255, description='Canny low threshold:', layout=Layout(visibility='hidden'), style=style),\n",
        "        widgets.BoundedIntText(value=200, min=1, max=255, description='Canny high threshold:', layout=Layout(visibility='hidden'), style=style)\n",
        "    ],\n",
        "    control_model_list[2]: [\n",
        "        widgets.BoundedFloatText(value=0.1, min=1, max=2.0, step=0.01, description='Hough value threshold (MLSD):', layout=Layout(visibility='hidden'), style=style),\n",
        "        widgets.BoundedFloatText(value=0.1, min=1, max=20.0, step=0.01, description='Hough distance threshold (MLSD):', layout=Layout(visibility='hidden'), style=style)\n",
        "    ],\n",
        "    control_model_list[3]: [\n",
        "        widgets.Dropdown(value='HED', description='Preprocessor:', options=['HED','PidiNet', 'None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[4]: [\n",
        "        widgets.Dropdown(value='PidiNet', description='Preprocessor:', options=['HED','PidiNet', 'HED safe', 'PidiNet safe','None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[5]: [\n",
        "        widgets.Dropdown(value='UPerNet', description='Preprocessor:', options=['UPerNet','None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[6]: [\n",
        "        widgets.Dropdown(value='DPT', description='Preprocessor:', options=['Midas', 'DPT','None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[7]: [\n",
        "        widgets.Dropdown(value='NormalBae', description='Preprocessor:', options=['NormalBae','None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[8]: [\n",
        "        widgets.Dropdown(value='Lineart', description='Preprocessor:', options=['Lineart','Lineart coarse', 'None', 'Lineart (anime)', 'None (anime)'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "    control_model_list[10]: [\n",
        "        widgets.Dropdown(value='ContentShuffle', description='Preprocessor:', options=['ContentShuffle','None'], layout=Layout(visibility='hidden'), style=style),\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Function to update visibility and enable/disable state of widgets\n",
        "def update_widgets(option):\n",
        "    for opt, int_inputs_list in int_inputs.items():\n",
        "        if opt == option:\n",
        "            for int_input in int_inputs_list:\n",
        "                int_input.layout.visibility = 'visible'\n",
        "        else:\n",
        "            for int_input in int_inputs_list:\n",
        "                int_input.layout.visibility = 'hidden'\n",
        "\n",
        "interactive(update_widgets, option=select_task)\n",
        "\n",
        "# =====================================\n",
        "# Settings\n",
        "# =====================================\n",
        "loop_generator = widgets.IntSlider(\n",
        "    value=1,\n",
        "    min=1,\n",
        "    max=10,\n",
        "    description='Loops üîÅ',\n",
        "    #layout=auto_layout,\n",
        "    style = style,\n",
        ")\n",
        "\n",
        "xformers_memory_efficient_attention = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description='Xformers memory efficient attention',\n",
        "    layout=auto_layout,\n",
        "    style = style,\n",
        ")\n",
        "\n",
        "model_precision = widgets.RadioButtons(\n",
        "    options=[(\"float16\", torch.float16),(\"float32\", torch.float32)],\n",
        "    description=\"Model precision (float32 need more memory):\",\n",
        "    layout=auto_layout,\n",
        "    style = style,\n",
        ")\n",
        "\n",
        "init_generator_in_cpu = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Generate initial random noise with the seed on the CPU',\n",
        "    layout=auto_layout,\n",
        "    style = style,\n",
        ")\n",
        "# =====================================\n",
        "# App\n",
        "# =====================================\n",
        "\n",
        "title_tab_one = widgets.HTML(\n",
        "    value=\"<h2>SD Interactive</h2>\",\n",
        "    layout=widgets.Layout(display=\"flex\", justify_content=\"center\")\n",
        ")\n",
        "title_tab_two = widgets.HTML(\n",
        "    value=\"<h2>Settings</h2>\",\n",
        "    layout=widgets.Layout(display=\"flex\", justify_content=\"center\")\n",
        ")\n",
        "\n",
        "buttons_ = TwoByTwoLayout(top_left=generate,\n",
        "               top_right=show_textual_inversion,\n",
        "               #bottom_left=None,\n",
        "               bottom_right=clear_outputs, merge=True)\n",
        "\n",
        "show_textual_inversion.style.button_color = '#97BC62'\n",
        "clear_outputs.style.button_color = '#97BC62'\n",
        "generate.style.button_color = '#2C5F2D'\n",
        "\n",
        "# TAB 1\n",
        "tab_sd = widgets.VBox(\n",
        "    [\n",
        "      widgets.AppLayout(\n",
        "        header=None,\n",
        "        left_sidebar=widgets.VBox(\n",
        "            [\n",
        "                title_tab_one,\n",
        "                num_images,\n",
        "                steps,\n",
        "                CFG,\n",
        "                select_sampler,\n",
        "                img_width,\n",
        "                img_height,\n",
        "                random_seed,\n",
        "                widgets.HBox([select_lora1, lora_weights_scale1],layout=widgets.Layout(width=width)),\n",
        "                widgets.HBox([select_lora2, lora_weights_scale2],layout=widgets.Layout(width=width)),\n",
        "                widgets.HBox([select_lora3, lora_weights_scale3],layout=widgets.Layout(width=width)),\n",
        "                widgets.HBox([select_lora4, lora_weights_scale4],layout=widgets.Layout(width=width)),\n",
        "                widgets.HBox([select_lora5, lora_weights_scale5],layout=widgets.Layout(width=width)),\n",
        "                select_clip_skip,\n",
        "            ]\n",
        "        ),\n",
        "        center=widgets.VBox(\n",
        "            [\n",
        "                select_task,\n",
        "                select_model,\n",
        "                vae_model_dropdown,\n",
        "                prompt,\n",
        "                neg_prompt,\n",
        "                widgets.HBox([weights_prompt ,active_ti]),\n",
        "                buttons_,\n",
        "            ]\n",
        "        ),\n",
        "        right_sidebar=widgets.VBox(\n",
        "            [\n",
        "                controlnet_output_scaling_in_unet,\n",
        "                controlnet_start_threshold,\n",
        "                controlnet_stop_threshold,\n",
        "            ] +\n",
        "            [preprocess_resolution_global] + [int_input for int_inputs_list in int_inputs.values() for int_input in int_inputs_list]\n",
        "        ),\n",
        "        footer=None,\n",
        "        pane_widths=[1.1, 1.4, 1.2],\n",
        "        # pane_heights=[\"0px\", 1, '0px'],\n",
        "      ),\n",
        "      display_imgs,\n",
        "    ]\n",
        ")\n",
        "\n",
        "# TAB 2\n",
        "tab_settings = widgets.VBox([\n",
        "    title_tab_two,\n",
        "    loop_generator,\n",
        "    xformers_memory_efficient_attention,\n",
        "    init_generator_in_cpu,\n",
        "    model_precision,\n",
        "    widgets.HTML(\n",
        "        value=\"<p>‚è≤Ô∏è</p>\",\n",
        "        layout=widgets.Layout(display=\"flex\", justify_content=\"center\")\n",
        "    ),\n",
        "])\n",
        "\n",
        "# APP\n",
        "tab = widgets.Tab()\n",
        "tab.children = [\n",
        "  widgets.VBox(\n",
        "    children = [\n",
        "        tab_sd,\n",
        "    ]),\n",
        "  widgets.VBox(\n",
        "    children = [\n",
        "        tab_settings,\n",
        "    ]),\n",
        "]\n",
        "\n",
        "tab.set_title(0, \"Stable Diffusion\")\n",
        "tab.set_title(1,  \"Settings\")\n",
        "tab.selected_index = 0\n",
        "\n",
        "display(tab)"
      ],
      "metadata": {
        "id": "atmx0PNQ78Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload an image here for use in Inpainting or ControlNet. üëà‚Äç‚Äç üñºÔ∏èüñºÔ∏èüñºÔ∏è\n",
        "#@markdown - To use Controlnet, you need to upload the control image with this cell\n",
        "Create_mask_for_Inpaint = True # @param {type:\"boolean\"}\n",
        "stroke_width = 24 # @param {type:\"integer\"}\n",
        "from google.colab import files\n",
        "from IPython.display import HTML\n",
        "import os\n",
        "import shutil\n",
        "%cd /content\n",
        "uploaded = files.upload()\n",
        "\n",
        "filename = next(iter(uploaded))\n",
        "print(f'Uploaded file: {filename}')\n",
        "\n",
        "upload_folder = 'uploaded_controlnet_image/'\n",
        "if not os.path.exists(upload_folder):\n",
        "    os.makedirs(upload_folder)\n",
        "\n",
        "source_path = filename\n",
        "destination_path_cn_img = os.path.join(upload_folder, filename)\n",
        "shutil.move(source_path, destination_path_cn_img)\n",
        "print(f'Moved file to {destination_path_cn_img}')\n",
        "\n",
        "if select_task.value == 'Inpaint' or Create_mask_for_Inpaint:\n",
        "    init_image = destination_path_cn_img\n",
        "    name_without_extension = os.path.splitext(init_image.split('/')[-1])[0]\n",
        "\n",
        "    image64 = base64.b64encode(open(init_image, 'rb').read())\n",
        "    image64 = image64.decode('utf-8')\n",
        "\n",
        "    print('\\033[34m Draw the mask with the mouse \\033[0m')\n",
        "    img = np.array(plt.imread(f'{init_image}')[:,:,:3])\n",
        "\n",
        "    draw(image64, filename=f\"./{name_without_extension}_draw.png\", w=img.shape[1], h=img.shape[0], line_width=stroke_width)\n",
        "\n",
        "    with_mask = np.array(plt.imread(f\"./{name_without_extension}_draw.png\")[:,:,:3])\n",
        "    mask = (with_mask[:,:,0]==1)*(with_mask[:,:,1]==0)*(with_mask[:,:,2]==0)\n",
        "    plt.imsave(f\"./{name_without_extension}_mask.png\",mask, cmap='gray')\n",
        "    mask_control = f\"./{name_without_extension}_mask.png\"\n",
        "    print(f'\\033[34m Mask saved: {mask_control} \\033[0m')"
      ],
      "metadata": {
        "id": "D-L_EepAsLPE",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üëá Upscale and face restoration { form-width: \"20%\", display-mode: \"form\" }\n",
        "from IPython.utils import capture\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "%cd /content\n",
        "directory_codeformer = '/content/CodeFormer/'\n",
        "with capture.capture_output() as cap:\n",
        "  if not os.path.exists(directory_codeformer):\n",
        "      os.makedirs(directory_codeformer)\n",
        "\n",
        "      # Setup\n",
        "      # Clone CodeFormer and enter the CodeFormer folder\n",
        "      %cd /content\n",
        "      !git clone https://github.com/sczhou/CodeFormer.git\n",
        "      %cd CodeFormer\n",
        "\n",
        "\n",
        "      # Set up the environment\n",
        "      # Install python dependencies\n",
        "      !pip install -q -r requirements.txt\n",
        "      !pip -q install ffmpeg\n",
        "      # Install basicsr\n",
        "      !python basicsr/setup.py develop\n",
        "\n",
        "      # Download the pre-trained model\n",
        "      !python scripts/download_pretrained_models.py facelib\n",
        "      !python scripts/download_pretrained_models.py CodeFormer\n",
        "  del cap\n",
        "# Visualization function\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "def display(img1, img2):\n",
        "  fig = plt.figure(figsize=(25, 10))\n",
        "  ax1 = fig.add_subplot(1, 2, 1)\n",
        "  plt.title('Input', fontsize=16)\n",
        "  ax1.axis('off')\n",
        "  ax2 = fig.add_subplot(1, 2, 2)\n",
        "  plt.title('CodeFormer', fontsize=16)\n",
        "  ax2.axis('off')\n",
        "  ax1.imshow(img1)\n",
        "  ax2.imshow(img2)\n",
        "def imread(img_path):\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  return img\n",
        "\n",
        "# Copy imgs\n",
        "Select_an_image = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# PROCESS AD\n",
        "if os.path.exists(Select_an_image.strip()):\n",
        "    image_list = [Select_an_image.replace('/content/', '').strip()]\n",
        "\n",
        "destination_directory = '/content/CodeFormer/inputs/user_upload'\n",
        "!rm -rf /content/CodeFormer/inputs/user_upload/*\n",
        "os.makedirs(destination_directory, exist_ok=True)\n",
        "for image_path in image_list:\n",
        "    image_filename = os.path.basename('/content/'+image_path)\n",
        "    destination_path = os.path.join(destination_directory, image_filename)\n",
        "    try:\n",
        "        shutil.copyfile('/content/'+image_path, destination_path)\n",
        "        print(f\"Image '{image_filename}' has been copied to '{destination_path}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to copy '{image_filename}' to '{destination_path}': {e}\")\n",
        "\n",
        "#@markdown `CODEFORMER_FIDELITY`: Balance the quality (lower number) and fidelity (higher number)<br>\n",
        "# you can add '--bg_upsampler realesrgan' to enhance the background\n",
        "CODEFORMER_FIDELITY = 0.7 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown `BACKGROUND_ENHANCE`: Enhance background image with Real-ESRGAN<br>\n",
        "BACKGROUND_ENHANCE = False #@param {type:\"boolean\"}\n",
        "#@markdown `FACE_UPSAMPLE`: Upsample restored faces for high-resolution AI-created images<br>\n",
        "FACE_UPSAMPLE = False #@param {type:\"boolean\"}\n",
        "#markdown `HAS_ALIGNED`: Input are cropped and aligned faces<br>\n",
        "HAS_ALIGNED =  False\n",
        "#@markdown `UPSCALE`: The final upsampling scale of the image. Default: 2<br>\n",
        "UPSCALE = 2 #@param {type:\"slider\", min:2, max:8, step:1}\n",
        "#markdown `DETECTION_MODEL`: Face detector. Default: retinaface_resnet50<br>\n",
        "DETECTION_MODEL = \"retinaface_resnet50\"\n",
        "#markdown `DRAW_BOX`: Draw the bounding box for the detected faces.\n",
        "DRAW_BOX = False\n",
        "\n",
        "BACKGROUND_ENHANCE = '--bg_upsampler realesrgan' if BACKGROUND_ENHANCE else ''\n",
        "FACE_UPSAMPLE = '--face_upsample' if FACE_UPSAMPLE else ''\n",
        "HAS_ALIGNED = '--has_aligned' if HAS_ALIGNED else ''\n",
        "DRAW_BOX = '--draw_box' if DRAW_BOX else ''\n",
        "%cd CodeFormer\n",
        "!python inference_codeformer.py -w $CODEFORMER_FIDELITY --input_path {destination_directory} {BACKGROUND_ENHANCE} {FACE_UPSAMPLE} {HAS_ALIGNED} --upscale {UPSCALE} --detection_model {DETECTION_MODEL} {DRAW_BOX}\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "input_folder = 'inputs/user_upload'\n",
        "result_folder = f'results/user_upload_{CODEFORMER_FIDELITY}/final_results'\n",
        "input_list = sorted(glob.glob(os.path.join(input_folder, '*')))\n",
        "for input_path in input_list:\n",
        "  img_input = imread(input_path)\n",
        "  basename = os.path.splitext(os.path.basename(input_path))[0]\n",
        "  output_path = os.path.join(result_folder, basename+'.png')\n",
        "  img_output = imread(output_path)\n",
        "  display(img_input, img_output)\n",
        "\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "__21H6ZLUokz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results in /content/CodeFormer/results\n"
      ],
      "metadata": {
        "id": "OFbbS2mdf7Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Images\n",
        "import os\n",
        "from google.colab import files\n",
        "!rm /content/results.zip\n",
        "!ls /content/images\n",
        "print('Download results')\n",
        "os.system(f'zip -r results.zip /content/images')\n",
        "try:\n",
        "  files.download(\"results.zip\")\n",
        "except:\n",
        "  print(\"Error\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Wlmke1VJPWS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Upscale results\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "%cd /content/CodeFormer\n",
        "!ls results\n",
        "print('Download results')\n",
        "os.system(f'zip -r results.zip results/user_upload_{CODEFORMER_FIDELITY}/final_results')\n",
        "try:\n",
        "  files.download(\"results.zip\")\n",
        "except:\n",
        "  files.download(f'/content/CodeFormer/results/{filename[:-4]}_{CODEFORMER_FIDELITY}/{filename}')\n",
        "%cd /content"
      ],
      "metadata": {
        "cellView": "form",
        "id": "k-FPYmh2Wb4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extras"
      ],
      "metadata": {
        "id": "g83HiPS2mSqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also use this cell to simply reload the model in case you need to.\n",
        "del model"
      ],
      "metadata": {
        "id": "JHxjQLbQ--O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "try:\n",
        "  del model\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "except:\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "\n",
        "# CONVERT SAFETENSORS TO DIFFUSERS FOR USE HERE\n",
        "# from diffusers import StableDiffusionPipeline\n",
        "# pipe = StableDiffusionPipeline.from_single_file(select_model.value).to(\"cuda\")\n",
        "# pipe.save_pretrained(\"./adetailer_model/\") # model path inpaint is ./adetailer_model/\n",
        "# del pipe\n",
        "# torch.cuda.empty_cache()\n",
        "# gc.collect()\n",
        "\n",
        "# OPTIONS #\n",
        "# @markdown # Adetailer üö´ beta\n",
        "face_detector_ad = True # @param {type:\"boolean\"}\n",
        "person_detector_ad = True # @param {type:\"boolean\"}\n",
        "hand_detector_ad = False # @param {type:\"boolean\"}\n",
        "model_for_inpaint = \"frankjoshua/toonyou_beta6\" # @param {type:\"string\"}\n",
        "common = {\n",
        "    \"prompt\": \"masterpiece, best quality\",\n",
        "    \"num_inference_steps\": 40,\n",
        "    \"height\": img_height.value,\n",
        "    \"width\": img_width.value,\n",
        "}\n",
        "inpaint = {\n",
        "    \"prompt\": \"masterpiece, best quality\",\n",
        "    \"num_inference_steps\": 40,\n",
        "    \"height\": img_height.value,\n",
        "    \"width\": img_width.value,\n",
        "}\n",
        "Select_an_image = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# PROCESS AD\n",
        "if os.path.exists(Select_an_image):\n",
        "    image_list = [Select_an_image]\n",
        "\n",
        "image_list = ad_model_process(\n",
        "    face_detector_ad,\n",
        "    person_detector_ad,\n",
        "    hand_detector_ad,\n",
        "    model_for_inpaint,\n",
        "    common,\n",
        "    inpaint,\n",
        "    image_list,\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CqH-rX3VVCg7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}